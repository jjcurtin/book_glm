--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 7: Dealing with Messy Data I - Case Analysis

```{r}
#| echo: false

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```

## Anscombe's Quartet

```{r}
#| echo: false
#| message: false

library(car)
library(tidyverse)
library(patchwork)

theme_set(theme_classic()) 

plot_y1 <- Quartet |> 
  ggplot(aes(x = x, y = y1)) +
  geom_abline(aes(intercept = coef(lm(y1~x))[1],
                  slope = coef(lm(y1~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y2 <- Quartet |> 
  ggplot(aes(x = x, y = y2)) +
  geom_abline(aes(intercept = coef(lm(y2~x))[1],
                  slope = coef(lm(y2~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y3 <- Quartet |> 
  ggplot(aes(x = x, y = y3)) +
  geom_abline(aes(intercept = coef(lm(y3~x))[1],
                  slope = coef(lm(y3~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y4 <- Quartet |> 
  ggplot(aes(x = x4, y = y4)) +
  geom_abline(aes(intercept = coef(lm(y4~x4))[1],
                  slope = coef(lm(y4~x4))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 
```

```{r}
#| echo: false

plot_y1 + plot_y2 + plot_y3 + plot_y4
```

Anscombe, Francis J. (1973) Graphs in statistical analysis. *American Statistician*, 27, 17â€“21. 

-----

```{r}
broom::tidy(lm(y1 ~ x, data = Quartet)) #<1>
```

1. See Quartet dataframe in `car` package.


SSE = `r round(sum(residuals(lm(y1 ~ x, data = Quartet))^2), 2)`


```{r}
#| code-fold: true

broom::tidy(lm(y2 ~ x, data = Quartet))
```

SSE = `r round(sum(residuals(lm(y2 ~ x, data = Quartet))^2), 2)`

```{r}
#| code-fold: true

broom::tidy(lm(y3 ~ x, data = Quartet))
```

SSE = `r round(sum(residuals(lm(y3 ~ x, data = Quartet))^2), 2)`

```{r}
#| code-fold: true

broom::tidy(lm(y4 ~ x4, data = Quartet))
```

SSE = `r round(sum(residuals(lm(y4 ~ x4, data = Quartet))^2), 2)`

-----

## Case Analysis

- Goal is to identify any unusual or excessively influential data. These data points may either bias results and/or reduce power to detect effects (inflate standard errors and/or decrease $R^2$).   

- Three aspects of individual observations we attend to:
  1. Leverage
  2. Regression Outlier
  3. Influence

- Case Analysis also provides an important first step as you get to *know* your data.  

-----

## Unit 7: Case Analysis - Unusual and Influential Data

```{r}
#| message: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(patchwork)
library(skimr)

theme_set(theme_classic()) 

path_data <- "data_lecture"
```


```{r}
data <- read_csv(here::here(path_data, "07_three_predictors_fps.csv"), 
                 show_col_types = FALSE) |> 
  glimpse()
```


Notice sex is of character class type. We can't use character stings in most of our analyes so we will want to handle character predictors immediately. For example, with our sex variable we could turn it into a factor, dummy code it, center it, or use any other contrast/combination of the above.  

-----

```{r}
data <- data |> 
  mutate(sex_c = if_else(sex == "female", -.5, .5),
         sex = factor(sex, 
                      levels = c("female", "male"))) |>
  glimpse()
```

-----

```{r}
m_1 <- lm(fps ~ bac + ta + sex_c, data = data)

broom::tidy(m_1)
```

-----

## Univariate Statistics

```{r}
data |> 
  skim(-subid, -sex) |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric") 
```

-----

## Univariate Plots

Histograms are a useful/common visualization for numeric variables.   

Here we show an example function to create histograms with density as y-axis overlayed with density and rug plots.

```{r}
plot_univariate <- function(data, var_name) {
  data |> 
    ggplot(aes(x = !!sym(var_name))) +
    geom_histogram(aes(y = after_stat(density)),
                   color = "black", fill = "light grey", bins = 10) +
    geom_density() +
    geom_rug(color = "red")

}
```


```{r}
(plot_univariate(data, "fps") + plot_univariate(data, "bac")) / 
  (plot_univariate(data, "ta") + plot_univariate(data, "sex_c"))
```


-----

Box plots can also be a useful visualization tool for numeric variables.   

Boxplots display:   

1. Median as line
2. 25th %ile and 75th %ile as hinges
3. Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25th %ile and 75th %ile)
4. Outliers outside of 1.5 * IQR

```{r}
#| code-fold: true

plot_box <- function(data, var_name) {
  data |> 
    ggplot(aes(x = 1, y = !!sym(var_name))) +
    geom_boxplot() +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())
}

plot_box(data, "fps") + plot_box(data, "bac") + plot_box(data, "ta")
```

-----

We could also overlay a violin plot over the box plot to clearly see the shape of the distribution and tails.

```{r}
#| code-fold: true

(plot_box(data, "fps") + 
geom_violin(aes(x = 1), fill = "green", color = NA, alpha = .4) +
  xlab(NULL)) +
  (plot_box(data, "bac") + 
geom_violin(aes(x = 1), fill = "green", color = NA, alpha = .4) +
  xlab(NULL)) +
  (plot_box(data, "ta") + 
geom_violin(aes(x = 1), fill = "green", color = NA, alpha = .4) +
  xlab(NULL))
```



-----


## Bivariate Correlations

```{r}
data |> 
  select(-subid, -sex) |> 
  cor() |> 
  round(2)
```

-----

## Bivariate Plots (Numeric Predictor)

Scatterplots are the preferred visualization when both variables (i.e., the predictor and outcome) are numeric.     

We also can add a simple line and a LOWESS line (Locally Weighted Scatterplot Smoothing) to help us consider the shape of the relationship.

```{r}
#| code-fold: true

plot_bivariate <- function(data, x_var, y_var, loess = TRUE) {
  plot <- data |> 
    ggplot(aes(x = !!sym(x_var), y = !!sym(y_var))) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x, col = "red") +
    theme(axis.text.x = element_text(size = 11),
          axis.text.y = element_text(size = 11))
  
  if(loess) {
    plot <- plot + geom_smooth(method = "loess", formula = y ~ x, col = "green")
  }
  
  return(plot)
}
```


```{r}
#| code-fold: true

plot_bivariate(data, "bac", "fps") + plot_bivariate(data, "ta", "fps") + 
  plot_bivariate(data, "sex_c", "fps", loess = FALSE)
```


-----

## Bivariate Plots (Categorical Predictor)

A grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables.

Lets look at an example of this with sex.
```{r}
 data |>
    ggplot(aes(x = sex, y = fps)) +
    geom_violin(fill = "green", color = NA) +
    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, hjust = 1),
          axis.text.y = element_text(size = 11))
```


-----


## Leverage (Cartoon Data)

**Check for high Leverage points**   

Leverage is a property of the predictors (DV is not considered for leverage analysis). An observation will have increased *leverage* on the results as its distance from the mean of all predictors increases. 


```{r}
#| echo: false

set.seed(102030)  
X1 <- rnorm(n = 30, mean = 30, sd = 10)
epsilon <- rnorm(n = 30, mean = 0, sd = 5)  # Normally distributed random noise
correlation_coefficient <- 0.8
Y <- 70 + correlation_coefficient * (X1 - mean(X1)) + epsilon
data_cartoon <- tibble(X1 = X1, Y = Y) |> 
  add_row(X1 = 10, Y = 100) |> 
  add_row(X1 = 30, Y = 100) |> 
  add_row(X1 = 45, Y = 100) |> 
  add_row(X1 = 45, Y = 81.75)

m_cartoon <- lm(Y ~ X1, data = data_cartoon)

plot_cartoon <- data_cartoon |> 
  ggplot() +
  geom_point(aes(x = X1, y = Y)) +
  geom_abline(aes(intercept = coef(m_cartoon)[1],
                  slope = coef(m_cartoon)[2]), color = "red", linewidth = 1) +
  scale_y_continuous(limits = c(50, 100)) +
  geom_point(aes(x = 10, y = 100), color = "yellow", size = 4) +
  geom_point(aes(x = 30, y = 100), color = "red", size = 4) +
  geom_point(aes(x = 45, y = 100), color = "blue", size = 4) +
  geom_point(aes(x = 45, y = 81.75), color = "green", size = 4)
```

```{r}
#| echo: false
#| warning: false

plot_cartoon
```


::: {.callout-important}
# Question

Which colored points have the most leverage in the 1 predictor example above?
:::


-----

## Leverage

Hat values ($h_i$) provide an index of leverage.   

In the one predictor case:  
$h_i = \frac{1}{N} + \frac{(X_i- \overline X)^2}{\sum(X_j- \overline X)^2}$ (for $j=1$ to $N$ in summation)  

With multiple predictors, $h_i$ measures the distance from the centroid (point of means) of the $X$s. Hat values are bounded between $\frac{1}{N}$ and 1. 

**Mean Hat value = $\frac{P}{N}$.**  

Rules of thumb:  

- $h_i > 3* \overline h$ for small samples ($N < 100$)
- $h_i > 2* \overline h$ for large samples   


**Do not blindly apply rules of thumb. Hat values should be separated from distribution of $h_i$. View a histogram of $h_i$.**

Note: Mahalanobis (Maha) distance = $(N - 1)(h_i - \frac{1}{N})$.   
SPSS reports centered leverage ($h - \frac{1}{N}$).

-----

## Leverage (Cartoon Data; Continued)


High leverage values are not always bad. In fact, in some cases they are good. Must also consider if they are regression outliers.  

::: {.callout-important}
# Question

Why?
:::


:::{.fragment}

$R^2 = \frac{\text{SSE}_{\text{mean-only}}- \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$  

$\text{SE}_{bi} = \frac{s_y}{s_i} = \frac{\sqrt{(1-R^2_y)}}{\sqrt{(N - k - 1)}}*\frac{1}{\sqrt{(1-R^2_i)}}$

[High leverage points that are fit well by the model increase the difference between $\text{SSE}_{\text{mean-only}}$ and $\text{SSE}_a$, which increases $R^2$.]{style="color:blue;"}   

[High leverage points that are fit well also increase variance for predictor. This reduces the SE for predictors and yields more power.]{style="color:blue;"}   

[Well fit, high leverage points do **not** alter $b$s.]{style="color:blue;"}    
:::


-----

## Leverage (Real Data)

We are going to use the following function (`case_analysis()`) to help us identify observations with high leverage.    

```{r}
case_analysis <- function(model, type, term = NULL, id = TRUE) {
  # type can be hat_values, residuals, cooks_d, df_betas, influence_plot, cov_ratio
  # term can be used to pass in a single parameter for the df_betas plot
  # Influence plot will label points with row id by default. Can turn off by setting id to FALSE
  
  model_formula <- as.character(model$call$formula)
  
  plot_hist <- function(data) {
    data |> 
      ggplot(aes(x = x)) +
      geom_histogram(fill = "light grey", color = "black", bins = 10) +
      geom_rug(color = "blue") +
      labs(x = str_to_sentence(str_replace(type, "_", " ")),
           y = "Frequency",
           title = str_c("Model:", model_formula[2], model_formula[1], model_formula[3],
                         sep = " "))
  }
  
  if(type == "hat_values") {
    p <- tibble(x = hatvalues(model)) |> 
      plot_hist() +
      geom_vline(xintercept = 2*mean(hatvalues(model)), color = "red", linewidth = .75) +
      labs(subtitle = "Cut point: 2 * mean(hat)")
    print(p)
  }
  
  if(type == "residuals") {
    t_cut_point <- qt(p = .025/nrow(data), 
                      df = nrow(data) - length(coef(model)) - 1 - 2, 
                      lower.tail = FALSE)
    
    p <- tibble(x = rstudent(model)) |> 
      plot_hist() +
      geom_vline(xintercept = c(-1, 1) * t_cut_point, color = "red", linewidth = .75) +
      labs(subtitle = "Bonferroni corrected p-value < 0.05 in red")
    print(p)
  }
  
  if(type == "cooks_d") {
    p <- tibble(x = cooks.distance(model)) |> 
      plot_hist() +
      geom_vline(xintercept = 4/(nrow(data) - length(coef(model)) - 1 - 1),
                 color = c("red"), linewidth = .75) +
      labs(subtitle = "4/(N-P) cut-off in red")
    print(p)
  }
  
  if(type == "df_betas") {
    m_1_coefs <- if(!is.null(term))term else names(as_tibble(dfbetas(m_1)))
    i <- 0
    for (term in m_1_coefs) {
      i <- i + 1
      p <- dfbetas(m_1) |>
        as_tibble() |>
        select(x = !!sym(term)) |>
        plot_hist() +
        labs(x = str_c("DFBETAS: ", term),
             subtitle = str_c("B = ", round(coef(m_1)[i], 6)))
      print(p)
    }
  }
  
  if(type == "cov_ratio") {
    p <- tibble(x = covratio(model)) |>
      plot_hist() +
      geom_vline(xintercept = abs((3 * (length(coef(model)))/nrow(data)) - 1), 
                 color = "red", linewidth = .75) +
      labs(subtitle = "3*(P/N) - 1 cut-off in red")
    print(p)
  }
  
  if(type == "influence_plot") {
    t_cut_point <- qt(p = .025/nrow(data), 
                      df = nrow(data) - length(coef(model)) - 1 - 2, 
                      lower.tail = FALSE)
    p <- tibble(residuals = rstudent(model),
             hat_values = hatvalues(model)) |>
      ggplot(aes(x = hat_values, y = residuals)) +
      geom_point(size = 10 * sqrt(cooks.distance(model))/max(cooks.distance(model)),
                 shape = 1) +
      geom_hline(yintercept = c(-1, 0, 1) * t_cut_point, color = "red", 
                 linetype = c("solid", "dashed", "solid"), linewidth = .75) +
      geom_vline(xintercept = c(1, 2, 3) * mean(hatvalues(model)), color = "red", 
                 linetype = c("dashed", "solid", "solid"), linewidth = .75) +
      labs(title = "Influence Bubble Plot",
           subtitle = str_c("Model:", model_formula[2], model_formula[1], model_formula[3],
                            sep = " "),
           x = "Hat Values",
           y = "Studentized Residuals")
    
    if(id) {
      p <- p +
        geom_text(aes(label = names(cooks.distance(model))), vjust = 0, hjust = 1)
    }
    print(p)
  }
  
}
```


----

```{r}
case_analysis(m_1, type = "hat_values")
```

`hatvalues()` returns a vector of named numbers (name = row id). This makes it easy to pull out rows corresponding to certain hat values.
```{r}
cut_hat <- 2*mean(hatvalues(m_1))

data |> 
    filter(hatvalues(m_1) > cut_hat)
```

-----


## Regression Outlier (Cartoon data)

**Check for regression outliers**

An observation that is not adequately fit by the regression model (i.e., falls very far from the prediction line).  

In essence, a regression outlier is a discrepant score with a large residual ($e_i$).

```{r}
#| echo: false
#| warning: false

plot_cartoon
```

::: {.callout-important}
# Question

Which point(s) are regression outliers?
:::

-----

## Regression Outlier

There are multiple quantitative indicators to identify regression outliers, including raw residuals ($e_i$), standardized residuals  ($e'_i$), and studentized residuals ($t'_i$). The preferred index is the **studentized residual**.   

$t'_i = \frac{e_i}{(SE_{e(-i)}*\sqrt{(1-h_i)})}$    

$t'_i$ follows a t-distribution with $n-P-1$ degrees of freedom.   


NOTE: SPSS calls these Studentized Deleted Residuals. Cohen calls these Externally Studentized Residual

-----
    

## Regression Outlier (Cartoon data; Continued)

Regression outliers are always bad but they can have two different types of bad effects.   

::: {.callout-important}
# Question

Why?
:::

:::{.fragment}

$R^2 = \frac{\text{SSE}_{\text{mean-only}}- \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$  

$\text{SE}_{bi} = \frac{s_y}{s_i} = \frac{\sqrt{(1-R^2_y)}}{\sqrt{(N - k - 1)}}*\frac{1}{\sqrt{(1-R^2_i)}}$

[Regression outliers increase $\text{SSE}_a$ which decreases $R^2$. Decreased $R^2$ leads to increased SEs for $b$s.]{style="color:blue;"}   

[If outlier also has leverage can alter (increase or decrease) $b$s.]{style="color:blue;"}
:::


-----

## Regression Outlier (Real data)

```{r}
case_analysis(m_1, type = "residuals")
```

The `car` package has a function `outlierTest()` that can be used to quickly identify regression outliers (residuals with a Bonferroni p-value < .05).
```{r}
car::outlierTest(m_1, cutoff = .05, labels = data$subid)
```

-----


## Influence (Cartoon data)

An observation is *influential* if it substantially alters the fitted regression model (i.e., the coefficients and/or intercept). Two commonly used assessment methods:    

- Cooks distance
- dfBetas

```{r}
#| echo: false
#| warning: false

plot_cartoon
```


::: {.callout-important}
# Question

Which point(s) have the most influence?
::: 


## Cook's Distance

Cookâ€™s distance ($\text{D}_i$) provides a single summary statistic to index how much influence each score has on the overall model.    

Cooks distance is based on both the *outlierness* (standardized residual) and leverage characteristics of the observation.   

$\text{D}_i = \frac{e'^2_i}{P} * \frac{h_i}{1-h_i}$   

$\text{D}_i > \frac{4}{N-P}$ has been proposed as a very liberal cutoff (identifies a lot of influential points). 

$\text{D}_i > \text{qf}(.5, P, N-P)$ has also been employed as very conservative.   


Identification of problematic scores should be considered in the context of the overall distribution of $\text{D}_i$.


## Cook's Distance (Real data)

```{r}
case_analysis(m_1, type = "cooks_d")
```

```{r}
cut_cook <- 4/(nrow(data) - length(coef(m_1)) - 1 - 1)

data |> 
  filter(cooks.distance(m_1) > cut_cook)
```

```{r}
data |> 
  filter(cooks.distance(m_1) > .15)
```



## Influence Bubble Plot (Real data)

```{r}
case_analysis(m_1, type = "influence_plot")
```

```{r}
data |> 
  slice(c(23, 62))
```


::: {.callout-important}
# Question

What are the expected effects of each of these points on the model?</span> 
:::

## dfBetas

$\text{dfBeta}_{ij}$ is an index of how much each regression coefficient ($j= 0 â€“ k$) would change if the $i^{\text{th}}$ score was deleted.   

$\text{dfBeta}_{ij} = b_j - b_{j(-i)}$

dfBetas (preferred) is the standardized form of the index.   

$\text{dfBetas} = \frac{\text{dfBeta}}{\text{SE}_{b_{j(-i)}}}$ 

$|{\text{dfBetas}}|>2$ may be problematic.   

$|{\text{dfBetas}}|> \frac{2}{\sqrt{N}}$ in larger samples (Belsley et al., 1980).   

Consider distribution with histogram! Also can visualize with added variable plot.

Problem is there can be many dfBetas (a set for each predictor and intercept). Most helpful when there is one *critical/focal effect.*

-----


## dfBetas (Real data)

```{r}
case_analysis(m_1, type = "df_betas")
```


-----

## Added Variable Plot (Real data)

```{r}
data <- as.data.frame(data)
rownames(data) <- data$subid
m_1 <- lm(fps ~ bac + ta + sex_c, data = data)

car::avPlots(m_1,intercept=TRUE,
             id=list(method=list(abs(residuals(m_1, type="pearson"))), 
                     n=5, cex=1, col='red', location="lr"))
```

-----

## Impact on SEs

In addition to altering regression coefficients (and reducing $R^2$), problematic scores can increase the SEs (i.e., precision of estimation) of the regression coefficients.    

COVRATIO is an index that indicates how individual scores affect the overall precision of estimation (joint confidence region for set of coefficients) of the regression coefficients.

Observations that decrease the precision of estimation have COVRATIOS < 1.0.

Belsley et al., (1980) proposed a cut off of:    

$\text{COVRATIO}_i < |3* \frac{P}{N} - 1|$

-----

## Impact on SEs (Real data)

```{r}
case_analysis(m_1, type = "cov_ratio")
```

-----

## Four Examples with Fake Data

```{r}
#| echo: false

plot_orig <- tibble(x = c(1, 2, 3, 4, 5),
       y = c(1, 2, 3, 4, 5)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "Original Data")

plot_lev_wellfit <- tibble(x = c(1, 2, 3, 4, 5, 8),
       y = c(1, 2, 3, 4, 5, 8)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 8, y = 8), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "High Leverage/Well-Fit",
       subtitle = "Test Data")

plot_outlier <- tibble(x = c(1, 2, 3, 4, 5, 3),
       y = c(1, 2, 3, 4, 5, 0)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 3, y = 0), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "Low Leverage/Regression Outlier",
       subtitle = "Test Data")

plot_lev_outlier <- tibble(x = c(1, 2, 3, 4, 5, 5),
       y = c(1, 2, 3, 4, 5, 0)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 5, y = 0), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "High Leverage/Regression Outlier",
       subtitle = "Test Data")
```

```{r}
#| echo: false
#| warning: false

(plot_orig + plot_lev_wellfit)/(plot_outlier + plot_lev_outlier)
```



-----


## Enter the Real World

::: {.callout-important}
# Question

So what do you do?

:::


-----

## Overall Impact of Problem Scores: Real Data

```{r}
broom::tidy(m_1)
```

**SSE = `r round(sum(residuals(m_1)^2),2)`**

-----

```{r}
data_rm_outliers <- data |> 
  filter(!subid %in% c("0125", "2112"))

m_2 <- lm(fps ~ bac + ta + sex_c, data = data_rm_outliers) 

broom::tidy(m_2)
```

**SSE = `r round(sum(residuals(m_2)^2),2)`**

-----

## What to Do

- <span style="color: blue;">Donâ€™t worry about leverage alone.</span>

- <span style="color: blue;">Worry about model outliers always.</span>

- <span style="color: blue;">Worry about influence (overall model or predictors by field).</span>

- <span style="color: blue;">Drop, retain, or bring model outliers to the fence.</span>

- <span style="color: blue;">Drop or retain influential participants.</span>

- <span style="color: blue;">Report both ways (in olden dayâ€¦).</span>

- <span style="color: blue;">Get with the program and pre-register instead (what you worry about, how you define it, and what you do)!</span>


