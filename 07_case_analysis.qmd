# Unit 7: Dealing with Messy Data I - Case Analysis

```{r}
#| echo: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(patchwork)
library(broom) 

path_data <- "data_lecture"
theme_set(theme_classic()) 
options(scipen = 999) # turns off scientific notation

sse <- function(model){
  sum(residuals(model)^2)
}
```

## Anscombe's Quartet

Anscombe, Francis J. (1973) Graphs in statistical analysis. *American Statistician*, 27, 17–21. 

```{r}
#| echo: false

data("Quartet", package = "carData")

plot_y1 <- Quartet |> 
  ggplot(aes(x = x, y = y1)) +
  geom_abline(aes(intercept = coef(lm(y1~x))[1],
                  slope = coef(lm(y1~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y2 <- Quartet |> 
  ggplot(aes(x = x, y = y2)) +
  geom_abline(aes(intercept = coef(lm(y2~x))[1],
                  slope = coef(lm(y2~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y3 <- Quartet |> 
  ggplot(aes(x = x, y = y3)) +
  geom_abline(aes(intercept = coef(lm(y3~x))[1],
                  slope = coef(lm(y3~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y4 <- Quartet |> 
  ggplot(aes(x = x4, y = y4)) +
  geom_abline(aes(intercept = coef(lm(y4~x4))[1],
                  slope = coef(lm(y4~x4))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 14) +
  xlim(0, 20) 

plot_y1 + plot_y2 + plot_y3 + plot_y4
```

--------------------------------------------------------------------------------

**Set 1**
```{r}
ex_1 <- lm(y1 ~ x, data = Quartet)
ex_1 |> tidy()
ex_1 |> sse() |> round(1)
```

**Set 2**
```{r}
#| code-fold: true

ex_2 <- lm(y2 ~ x, data = Quartet)
ex_2 |> tidy()
ex_2 |> sse() |> round(1)
```

**Set 3**
```{r}
#| code-fold: true

ex_3 <- lm(y3 ~ x, data = Quartet)
ex_3 |> tidy()
ex_3 |> sse() |> round(1)
```

**Set 4**
```{r}
#| code-fold: true

ex_4 <- lm(y4 ~ x4, data = Quartet)
ex_4 |> tidy()
ex_4 |> sse() |> round(1)
```

:::{.callout-note}
See the Quartet dataset in the `carData` package
:::

--------------------------------------------------------------------------------

## Case Analysis

- Goal is to identify any unusual or excessively influential data. 

- These data points may either bias results and/or reduce power to detect effects (inflate standard errors and/or decrease $R^2$).   

- Three aspects of individual observations we attend to:
  1. Leverage
  2. Regression Outlier
  3. Influence

- Case Analysis also provides an important first step as you get to *know* your data.  

--------------------------------------------------------------------------------

```{r}
data <- read_csv(here::here(path_data, "07_three_predictors_fps.csv"), 
                 show_col_types = FALSE) |> 
  glimpse()
```

```{r}
data |> head()
```

--------------------------------------------------------------------------------

We prefer to use text labels for our categorical variables (e.g., male/female vs. 0/1)

It is often easier to work with categorical variables if they are converted to factors

We can also create a regressor for `sex` that is unit-weighted and centered and a mean-centered regressor for `ta`, but leave `bac` as is.     
```{r}
data <- data |> 
  mutate(sex = fct(sex, levels = c("female", "male")), 
         sex_c = if_else(sex == "female", -.5, .5),
         ta_c = ta - mean(ta)) 
```

```{r}
levels(data$sex)
```

```{r}
data |> head()
```

---------------------------------------------------------------------------

Lets fit our new three predictor model

```{r}
m <- lm(fps ~ bac + ta_c + sex_c, data = data)
m |> tidy()
```

---------------------------------------------------------------------------

## Univariate Statistics

The first step of case analysis is to review the univariate statistics

As before, we can use the `skim()` function from the `skimr` package to get a quick overview of our data, but we like to customize the function a bit.

```{r}
library(skimr)
my_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),
                                n_missing = ~sum(is.na(.), na.rm = TRUE)),
                     numeric = sfl(p25 = NULL,
                                   p75 = NULL,
                                   hist = NULL),
                     character = sfl(min = NULL, max = NULL),
                     factor = sfl(ordered = NULL))
```

---------------------------------------------------------------------------

```{r}
data |> my_skim(-subid)
```

---------------------------------------------------------------------------

## Univariate Plots

Histograms, density plots, and rug plots are all useful/common visualization for numeric variables.   

As always, lets by DRY by writing functions (and add to your personal function library?)

```{r}
#| code-fold: true

plot_univariate <- function(data, x_name, n_bins = 10) {
  data |> 
    ggplot(aes(x = !!sym(x_name))) +
    geom_histogram(aes(y = after_stat(density)),
                   color = "black", fill = "light grey", bins = n_bins) +
    geom_density() +
    geom_rug(color = "red")
}
```

:::{.callout-tip}
# Programming Tip
All of the case analysis functions can be sourced at once from a script in my lab_support repository.
`devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/case_analysis.R?raw=true")`
:::

---------------------------------------------------------------------------

And now use it (making four plots together with `patchwork`)
```{r}
#| code-fold: true
#| fig-height: 8
#| fig-width: 10

(plot_univariate(data, "fps") + plot_univariate(data, "bac")) / 
  (plot_univariate(data, "ta") + plot_univariate(data, "sex_c"))
```

----------------------------------------------------------------------

Box plots can also be a useful visualization tool for numeric variables.   

Boxplots display:   

1. Median as line
2. 25th %ile and 75th %ile as hinges
3. Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25th %ile and 75th %ile)
4. Outliers outside of 1.5 * IQR

Here is a simple boxplot function
```{r}
#| code-fold: true

plot_box <- function(data, x_name) {
  data |> 
    ggplot(aes(x = 1, y = !!sym(x_name))) +
    geom_boxplot() +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.x = element_blank())
}
```

---------------------------------------------------------------------------

```{r}
#| code-fold: true
#| fig-height: 6
#| fig-width: 10

plot_box(data, "fps") + plot_box(data, "bac") + plot_box(data, "ta")
```

--------------------------------------------------------------------------

We could also overlay a violin plot over the box plot to clearly see the shape of the distribution and tails.  Here is an example for `fps`

```{r}
#| code-fold: true

plot_box(data, "fps") + 
  geom_violin(aes(x = 1), fill = "green", color = NA, alpha = .4)
```

---------------------------------------------------------------------------


## Bivariate Correlations

The next step in case analysis is to review the bivariate correlations among the predictors and the outcome.  

This should be limited to the numeric variables (and only one of the `ta` variables)

```{r}
data |> 
  select(where(is.numeric), -ta) |>
  cor() |> 
  round(2)
```

--------------------------------------------------------------------------

## Bivariate Plots between Outcome and Numeric Predictors

Scatterplots are the preferred visualization when both variables (i.e., the predictor and outcome) are numeric.     

We also can add a simple line and a LOWESS line (Locally Weighted Scatterplot Smoothing) to help us consider the shape of the relationship.

How about another function?
```{r}
#| code-fold: true

plot_bivariate <- function(data, x_name, y_name, loess = TRUE) {
  plot <- data |> 
    ggplot(aes(x = !!sym(x_name), y = !!sym(y_name))) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x, col = "red") +
    theme(axis.text.x = element_text(size = 11),
          axis.text.y = element_text(size = 11))
  
  if(loess) {
    plot <- plot + geom_smooth(method = "loess", formula = y ~ x, col = "green")
  }
  
  return(plot)
}
```


```{r}
#| code-fold: true

plot_bivariate(data, "bac", "fps") + plot_bivariate(data, "ta_c", "fps") + 
  plot_bivariate(data, "sex_c", "fps", loess = FALSE)
```

---------------------------------------------------------------------------

GGally is a great package for creating a matrix of bivariate plots

```{r}
GGally::ggpairs(data, columns = c("fps", "bac", "ta_c", "sex_c"))
```

## Bivariate Plots (Categorical Predictor)

A grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables.

Lets look at an example of this with sex.
```{r}
#| code-fold: true

 data |>
    ggplot(aes(x = sex, y = fps)) +
    geom_violin(fill = "green", color = NA) +
    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, hjust = 1),
          axis.text.y = element_text(size = 11))
```

## Leverage

Leverage is a property of the predictors (DV is not considered for leverage analysis). An observation will have increased *leverage* on the results as its distance from the mean of all predictors increases. 

```{r}
#| echo: false

set.seed(102030)  
X1 <- rnorm(n = 30, mean = 30, sd = 10)
epsilon <- rnorm(n = 30, mean = 0, sd = 5)  # Normally distributed random noise
correlation_coefficient <- 0.8
Y <- 70 + correlation_coefficient * (X1 - mean(X1)) + epsilon
data_cartoon <- tibble(X1 = X1, Y = Y) |> 
  add_row(X1 = 10, Y = 100) |> 
  add_row(X1 = 30, Y = 100) |> 
  add_row(X1 = 45, Y = 100) |> 
  add_row(X1 = 45, Y = 81.75)

m_cartoon <- lm(Y ~ X1, data = data_cartoon)

plot_cartoon <- data_cartoon |> 
  ggplot() +
  geom_point(aes(x = X1, y = Y)) +
  geom_abline(aes(intercept = coef(m_cartoon)[1],
                  slope = coef(m_cartoon)[2]), color = "red", linewidth = 1) +
  scale_y_continuous(limits = c(50, 100)) +
  geom_point(aes(x = 10, y = 100), color = "yellow", size = 4) +
  geom_point(aes(x = 30, y = 100), color = "red", size = 4) +
  geom_point(aes(x = 45, y = 100), color = "blue", size = 4) +
  geom_point(aes(x = 45, y = 81.75), color = "green", size = 4)
```

```{r}
#| echo: false
#| warning: false
#| fig-height: 6
#| fig-width: 8

plot_cartoon
```

::: {.callout-important}
# Question
Which colored points have the most leverage in the 1 predictor cartoon data example above?
:::

:::{.fragment}
[Blue, Yellow AND Green]{style="color:blue;"}
:::

---------------------------------------------------------------------------

Hat values ($h_i$) provide an index of leverage.   

In the one predictor case:  

- $h_i = \frac{1}{N} + \frac{(X_i- \overline X)^2}{\sum(X_j- \overline X)^2}$ (for $j=1$ to $N$ in summation)  

With multiple predictors, $h_i$ measures the distance from the centroid (point of means) of the $X$s. Hat values are bounded between $\frac{1}{N}$ and 1. 

---------------------------------------------------------------------------

The mean Hat value in a sample is a function of $P$ and $N$

- Mean Hat value = $\frac{P}{N}$

There are coarse rules of thumb for identifying high leverage points

- $h_i > 3* \overline h$ for small samples ($N < 100$)
- $h_i > 2* \overline h$ for large samples   


BUT... 

- Do not blindly apply rules of thumb. 
- Hat values should be separated from distribution of $h_i$. 
- View a histogram of $h_i$.

:::{.callout-note}
# Note
Mahalanobis (Maha) distance = $(N - 1)(h_i - \frac{1}{N})$.   
SPSS reports centered leverage ($h - \frac{1}{N}$).
:::

---------------------------------------------------------------------------

High leverage values are not always bad. In fact, in some cases they are good. Must also consider if they are regression outliers.  

::: {.callout-important}
# Question
Why?
:::

:::{.fragment}

$R^2 = \frac{\text{SSE}_{\text{mean-only}}- \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$  

$\text{SE}_{bi} = \frac{s_y}{s_i} = \frac{\sqrt{(1-R^2_y)}}{\sqrt{(N - k - 1)}}*\frac{1}{\sqrt{(1-R^2_i)}}$

[High leverage points that are fit well by the model increase the difference between $\text{SSE}_{\text{mean-only}}$ and $\text{SSE}_a$, which increases $R^2$.]{style="color:blue;"}   

[High leverage points that are fit well also increase variance for predictor. This reduces the SE for predictors and yields more power.]{style="color:blue;"}   

[Well fit, high leverage points do **not** alter $b$s.]{style="color:blue;"}    
:::

---------------------------------------------------------------------------

To identify high leverage points, we can add `hatvalues()` to the dataframe to see which observations have the most leverage

You could also filter to values with high hats

```{r}
cut_hat <- 2 * mean(hatvalues(m))

data |> 
  mutate(hat = hatvalues(m)) |> 
  arrange(desc(hat)) |> 
  filter(hat > cut_hat) |> 
  print() 
```

---------------------------------------------------------------------------

BUT we should view these values within the distribution of all the hats for the dataset

Here is a function to do this (add to your library?)
```{r}
#| code-fold: true

plot_leverage <- function(model) {
  tibble(x = hatvalues(model)) |> 
    ggplot(aes(x = x)) +
      geom_histogram(fill = "light grey", color = "black", bins = 10) +
      geom_rug(color = "blue") +
      labs(x = "hat values",
           y = "Frequency") +
      geom_vline(xintercept = 2 * mean(hatvalues(model)), color = "yellow", linewidth = .75) +
      geom_vline(xintercept = 3 * mean(hatvalues(model)), color = "red", linewidth = .75) +
      labs(subtitle = "") +
      labs(subtitle = "Red: 3 * mean(hat) for N < 100;  Yellow: 2 * mean(hat) for N >= 100")
}
```

```{r}
plot_leverage(m)
```

## Regression Outliers

An observation that is not adequately fit by the regression model (i.e., falls very far from the prediction line).  

In essence, a regression outlier is a discrepant score with a large residual ($e_i$).

```{r}
#| echo: false
#| warning: false
#| fig-height: 6
#| fig-width: 8

plot_cartoon
```

::: {.callout-important}
# Question
Which point(s) are regression outliers?
:::

:::{.fragment}
[Yellow, Red, and Blue in descending order of magnitude]{style="color:blue;"}
:::

---------------------------------------------------------------------------

There are multiple quantitative indicators to identify regression outliers

- raw residuals ($e_i$) 
- standardized residuals ($e'_i$) 
- studentized residuals ($t'_i$) 

The preferred index is the **studentized residual**.   

- $t'_i = \frac{e_i}{(SE_{e(-i)}*\sqrt{(1-h_i)})}$    
- $t'_i$ follows a t-distribution with $n-P-1$ degrees of freedom.   

:::{.callout-note}
# NOTE 
SPSS calls these Studentized Deleted Residuals. 

Cohen calls these Externally Studentized Residual
:::

---------------------------------------------------------------------------
    
Regression outliers are always bad but they can have two different types of bad effects.   

::: {.callout-important}
# Question
What are these two bad effects and why?
:::

:::{.fragment}
$R^2 = \frac{\text{SSE}_{\text{mean-only}}- \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$  

$\text{SE}_{bi} = \frac{s_y}{s_i} = \frac{\sqrt{(1-R^2_y)}}{\sqrt{(N - k - 1)}}*\frac{1}{\sqrt{(1-R^2_i)}}$

[Regression outliers increase $\text{SSE}_a$ which decreases $R^2$. Decreased $R^2$ leads to increased SEs for $b$s.]{style="color:blue;"}   

[If outlier also has leverage can alter (increase or decrease) $b$s.]{style="color:blue;"}
:::

---------------------------------------------------------------------------

You can add the studentized residuals to the dataframe to see which observations are regression outliers
```{r}
data |> 
  mutate(residuals = rstudent(m)) |> 
  arrange(desc(abs(residuals))) |> 
  head()
```

\

The `car` package has a function `outlierTest()` that can be used to quickly identify regression outliers (residuals with a Bonferroni p-value < .05).
```{r}
car::outlierTest(m, cutoff = .05, labels = data$subid)
```

---------------------------------------------------------------------------

But as with Leverage, we should view these values within the distribution of all the residuals for the dataset

Here is a function to do this (add to your library?)
```{r}
#| code-fold: true

plot_residuals <- function(model){
  res <-tibble(x = rstudent(model))
  t_cut_point <- qt(p = .025/nrow(res), 
                    df = nrow(res) - length(coef(model)) - 1 - 2, 
                    lower.tail = FALSE)
  res |> 
      ggplot(aes(x = x)) +
        geom_histogram(fill = "light grey", color = "black", bins = 10) +
        geom_rug(color = "blue") +
        labs(x = "Studentized Residuals",
           y = "Frequency") +
        geom_vline(xintercept = c(-1, 1) * t_cut_point, color = "red", linewidth = .75) +
        labs(subtitle = "Red cut: Bonferroni corrected p-value < 0.05")
}
```

```{r}
plot_residuals(m)
```

## Influence

An observation is *influential* if it substantially alters the fitted regression model (i.e., the coefficients and/or intercept). Two commonly used assessment methods:    

- Cooks distance
- dfBetas


```{r}
#| echo: false
#| warning: false
#| fig-height: 6
#| fig-width: 8
 
plot_cartoon
```


::: {.callout-important}
# Question
Which point(s) have the most influence?
::: 

:::{.fragment}
[Yellow and Blue because they will alter $b_1$ due to their combined leverage and influence.]{style="color:blue;"}
:::

## Cook's Distance

Cook’s distance ($\text{D}_i$) provides a single summary statistic to index how much influence each score has on the overall model.    

Cooks distance is based on both the *outlierness* (standardized residual) and leverage characteristics of the observation.   

- $\text{D}_i = \frac{e'^2_i}{P} * \frac{h_i}{1-h_i}$   

- $\text{D}_i > \frac{4}{N-P}$ has been proposed as a very liberal cutoff (identifies a lot of influential points). 

- $\text{D}_i > \text{qf}(.5, P, N-P)$ has also been employed as very conservative.   


As with the other case analysis metrics, identification of problematic scores should be considered in the context of the overall distribution of $\text{D}_i$.

---------------------------------------------------------------------------

We can add Cook's distance to the dataframe to see which observations have the most influence on the overall model.

You might also want to see the leverage and residuals for these points

```{r}
cut_cooks <- 4/(nrow(data) - length(coef(m)))

data |> 
  mutate(hats = hatvalues(m),
         residuals = rstudent(m),
         cooks = cooks.distance(m)) |> 
  arrange(desc(cooks)) |> 
  filter(cooks > cut_cooks)
```

---------------------------------------------------------------------------

Here is a function to view the distribution of cooks distance (add to your library?)
```{r}
#| code-fold: true

plot_cooks <- function(model){
  cooks <-tibble(x = cooks.distance(model))
  cut_cooks <- 4/(nrow(cooks) - length(coef(model)))
  
  cooks |> 
    ggplot(aes(x = x)) +
      geom_histogram(fill = "light grey", color = "black", bins = 10) +
      geom_rug(color = "blue") +
      labs(x = "Cooks Distances",
           y = "Frequency") +
      geom_vline(xintercept = 4/(nrow(data) - length(coef(model)) - 1 - 1),
                 color = c("red"), linewidth = .75) +
      labs(subtitle = "Cut: 4/(N-P)")
}
```

```{r}
plot_cooks(m)
```


## dfBeta and dfBetas

$dfBeta_{ij}$ is an index of how much each regression coefficient ($j= 0 – k$) would change if the $i^{th}$ score was deleted.   

- $\text{dfBeta}_{ij} = b_j - b_{j(-i)}$

dfBetas (preferred) is the standardized form of the index.   

- $dfBetas = \frac{dfBeta}{SE_{b_{j(-i)}}}$ 

- $|dfBetas|>2$ may be problematic.   

- $|dfBetas|> \frac{2}{\sqrt{N}}$ in larger samples (Belsley et al., 1980).   


As always, consider distribution with histogram! Also can visualize with added variable plot.

Problem is there can be many dfBetas (a set for each predictor and intercept). Most helpful when there is one *critical/focal effect.*

---------------------------------------------------------------------------

```{r}
#| code-fold: true
 
plot_dfbetas <- function(model, x_name){
   dfb <- as_tibble(dfbetas(model)) |> 
     select(matches(x_name))
   colnames(dfb) <- "x" 
  
  dfb |> 
    ggplot(aes(x = x)) +
      geom_histogram(fill = "light grey", color = "black", bins = 10) +
      geom_rug(color = "blue") +
      labs(x = "dfBetas",
           y = "Frequency") +
      geom_vline(xintercept = c(-2, 2),
                 color = c("red"), linewidth = .75) +
      labs(title = x_name, subtitle = "Cut at |2|")
}
```

```{r}
plot_dfbetas(m, "bac")
```


## Added Variable Plot

An added variable plot (AKA partial regression plot), can be used to detect observations that are influencing the relationship between a specific $X$ and $Y$ 

- Residuals for $Y$ are plotted against the residuals of a focal $X$ after regressing out the other independent variables. 
- This helps to isolate the effect of that specific variable, making it easier to see its unique contribution to the model.
- It is also easier to see observations that impact that effect.

The steps to create an added variable plot typically involve:

1. Regressing $Y$ on all $X$ except for the focal $X$ to obtain the residuals for $Y$
2. Regressing the focal $X$ on all other $X$ to obtain the residuals for that focal $X$
3. Plotting the residuals of $Y$ against the residuals of $X$

---------------------------------------------------------------------------

The `avPlots() function in the `car` package can be used to create added variable plots. 

But we have preferences for how we set all the function parameters so we will write our own function to wrap around it.

```{r}
#| code-fold: true

plot_av <- function(data, model, label_name = "subid"){
 
  # avPlots() wants a traditional dataframe with rownames set to subid or 
  # other label for points
  data <- as.data.frame(data)
  rownames(data) <- pull(data, matches(label_name))

  car::avPlots(model, intercept = TRUE,
              id = list(method = list(abs(residuals(model, type = "pearson"))), 
              n = 5, cex = 1, col = 'red', location = "lr"))
}
```

```{r}
data |> plot_av(model = m, label_name = "subid")
```

## Impact on SEs

In addition to altering regression coefficients (and reducing $R^2$), problematic scores can increase the SEs (i.e., precision of estimation) of the regression coefficients.    

COVRATIO is an index that indicates how individual scores affect the overall precision of estimation (joint confidence region for set of coefficients) of the regression coefficients.

Observations that decrease the precision of estimation have COVRATIOS < 1.0.

Belsley et al., (1980) proposed a cut off of:    

$\text{COVRATIO}_i < |3* \frac{P}{N} - 1|$

---------------------------------------------------------------------------

```{r}
#| code-fold: true
 
plot_covratio <- function(model){
   cr <-tibble(x = covratio(model)) 
  
  cr |> 
    ggplot(aes(x = x)) +
      geom_histogram(fill = "light grey", color = "black", bins = 10) +
      geom_rug(color = "blue") +
      labs(x = "Cov Ratio",
           y = "Frequency") +
      geom_vline(xintercept = abs((3 * (length(coef(model)))/nrow(cr) - 1)),
                 color = "red", linewidth = .75) +
      labs(subtitle = "Cut: 3*(P/N) - 1")
}
```

```{r}
plot_covratio(m)
```


## Four Examples with Fake Data

```{r}
#| echo: false

plot_orig <- tibble(x = c(1, 2, 3, 4, 5),
       y = c(1, 2, 3, 4, 5)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "Original Data")

plot_lev_wellfit <- tibble(x = c(1, 2, 3, 4, 5, 8),
       y = c(1, 2, 3, 4, 5, 8)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 8, y = 8), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "High Leverage/Well-Fit")

plot_outlier <- tibble(x = c(1, 2, 3, 4, 5, 3),
       y = c(1, 2, 3, 4, 5, 0)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 3, y = 0), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "Low Leverage/Regression Outlier")

plot_lev_outlier <- tibble(x = c(1, 2, 3, 4, 5, 5),
       y = c(1, 2, 3, 4, 5, 0)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_abline(aes(intercept = coef(lm(y~x))[1],
                  slope = coef(lm(y~x))[2]),
              color = "blue", linewidth = .5) +
  geom_point() +
  geom_point(aes(x = 5, y = 0), color = "red") +
  ylim(0, 8) +
  xlim(0, 8) +
  labs(title = "High Leverage/Regression Outlier")
```

```{r}
#| echo: false
#| warning: false

(plot_orig + plot_lev_wellfit)/(plot_outlier + plot_lev_outlier)
```


## Enter the Real World

::: {.callout-important}
# Question
So what do you do?
:::


-----

## Overall Impact of Problem Scores: Real Data

```{r}
tidy(m)
```

SSE = `r round(sum(residuals(m)^2))`

-----

```{r}
data_rm_outliers <- data |> 
  filter(!subid %in% c("0125", "2112"))

m_2 <- lm(fps ~ bac + ta_c + sex_c, data = data_rm_outliers) 

tidy(m_2)
```

SSE = `r round(sum(residuals(m_2)^2))`

-----

## What to Do

What to worry about

- Don’t worry about leverage alone
- Worry about model outliers always
- Worry about influence (overall model or predictors by field)


What to do

- Drop, retain, or bring model outliers to the fence?
- Drop or retain influential participants?
  - Report both ways (in olden day…)
  - Get with the program and pre-register instead (what you worry about, how you define it, and what you do)!