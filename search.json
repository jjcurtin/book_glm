[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the General Linear Model",
    "section": "",
    "text": "Course Syllabus",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-syllabus",
    "href": "index.html#course-syllabus",
    "title": "Introduction to the General Linear Model",
    "section": "",
    "text": "Instructor\nJohn Curtin\n\nOffice hours: Thursdays, 1-2 pm or by appointment in Brogden 326\nEmail: jjcurtin@wisc.edu (but please use Slack DM or channel posts for all course communications during this semester)\n\n\n\nTeaching Assistants\nMichelle Marji\n\nOffice hours: Wednesdays, 10-11 am in Brogden 391 or by appointment\nEmail: michelle.marji@wisc.edu\n\nKendra Wyant\n\nOffice hours: Tuesdays, 12:30-1:30 pm in Brodgen 325 or by appointment\nEmail: kpaquette2@wisc.edu\n\n\n\nCourse Website\nhttps://jjcurtin.github.io/book_iaml/\n\n\nCommunications\nAll course communications will occur in the course’s Slack workspace (https://iaml-2024.slack.com/). You should have received an invitation to join the workspace. If you have difficulty joining, please contact me by my email above. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me. However, I may share the DM with the TAs unless you request otherwise. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.\n\n\nMeeting Times\nThe scheduled course meeting times are Tuesdays and Thursdays from 11:00 - 12:15 pm. Tuesdays are generally used by the TAs to discuss application issues from the homework or in the course more generally. Thursdays are generally led by John and used to discuss topics from the video lectures and readings.\nAll required videos, readings, and application assignments are described on the course website at the beginning of each unit.\n\n\nCourse Description\nThis course is designed to introduce students to a variety of computational approaches in machine learning. The course is designed with two key foci. First, students will focus on the application of common, “out-of-the-box” statistical learning algorithms that have good performance and are implemented in tidymodels in R. Second, students will focus on the application of these approaches in the context of common questions in behavioral science in academia and industry.\n\n\nRequisites\nStudents are required to have completed Psychology 610 with a grade of B or better or a comparable course with my consent.\n\n\nLearning Outcomes\n\nStudents will develop and refine best practices for data wrangling, general programming, and analysis in R.\nStudents will distinguish among a variety of machine learning settings: supervised learning vs. unsupervised learning, regression vs. classification\nStudents will be able to implement a broad toolbox of well-supported machine-learning methods: decision trees, nearest neighbor, general and generalized linear models, penalized models including ridge, lasso, and elastic-nets, neural nets, random forests.\nStudents will develop expertise with common feature extraction techniques for quantitative and categorical predictors.\nStudents will be able to use natural language processing approaches to extract meaningful features from text data.\nStudents will know how to characterize how well their regression and classification models perform and they will employ appropriate methodology for evaluating their: cross validation, ROC and PR curves, hypothesis testing.\nStudents will learn to apply their skills to common learning problems in psychology and behavioral sciences more generally.\n\n\n\nCourse Topics\n\nOverview of Machine Learning Concepts and Uses\nData wrangling in R using tidyverse and tidymodels\nIterations, functions, simulations in R\nRegression models\nClassification models\nModel performance metrics\nROCs\nCross validation and other resampling methods\nModel selection and regularization\nApproaches to parallel processing\nFeature engineering techniques\nNatural language processing\nTree based methods\nBagging and boosting\nNeural networks\nDimensionality reduction and feature selection\nExplanatory methods including variable importance, partial dependence plots, etc\nEthics and privacy issues\n\n\n\nSchedule\nThe course is organized around 14 weeks of academic instruction covering the following topics:\n\nIntroduction to course and machine learning\nExploratory data analysis\nRegression models\nClassification models\nResampling methods for model selection and evaluation\nRegularization and penalized models\nMidterm exam/project\nAdvanced performance metrics\nModel comparisons\nAdvanced models: Random forests and ensemble methods (bagging, boosting, stacking)\nAdvanced models: Neural networks\nNatural Language Processing I: Text processing and feature engineering\nApplications\nEthics\n\n\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\n\n\n\nRequired Textbooks and Software\nAll required textbooks are freely available online (though hard copies can also be purchased if desired). There are eight required textbooks for the course. The primary text for which we will read many chapters is:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. An Introduction to Statistical Learning: With Applications in R (2023; 2nd Edition). (website)\n\nWe will also read sections to chapters in each of the following texts:\n\nWickham, H. & Grolemund, G. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (1st ed.). Sebastopol, CA: O’Reilly Media, Inc. (website)\nHvitfeldt, E. & Silge, J. Supervised Machine Learning for Text Analysis in R (website)\nKuhn, M. & Johnson, K. Applied Predictive Modeling. New York, NYL Springer Science. (website)\nKuhn, M., & Johnson, K. Feature Engineering and Selection: A Practical Approach for Predictive Models (1st ed.). Boca Raton, FL: Chapman and Hall/CRC. (website)\nKuhn, M. & Silge, J. Tidy Modeling with R. (website)\nMolnar, C. Intepretable Machine Learning: A Guide for Makiong Black Box Models Explainable (2nd ed.). (website\nSilge, J., & Robinson, D. Text Mining with R: A Tidy Approach (1st ed.). Beijing; Boston: O’Reilly Media. (website)\nWickham, H. The Tidy Style Guide. (website)\nBoehmke, Brad and Greenwell, Brandon M. (2019). Hands-On Machine Learning with R. Chapman and Hall/CRC. (website)\nNg, Andrew (2018). Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI. (website)\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC. (website)\n\nAdditional articles will be assigned and provided by pdf through the course website.\nAll data processing and analysis will be accomplished using R (and we recommend the RStudio IDE). R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\n\n\nGrading\n\nQuizzes (13 anticipated): 15%\nApplication assignments (11 anticipated): 25%\nMidterm application exam: 15%\nMidterm conceptual exam: 15%\nFinal application exam: 15%\nFinal conceptual exam: 15%\n\nFinal letter grades may be curved upward, but a minimum guarantee is made of an A for 93 or above, AB for 88 - 92, B for 83 - 87, BC for 78 - 82, C for 70 - 77, D for 60-69, and F for &lt; 60.\n\n\nExams, Application Assignments and Quizzes\n\nThe midterm application exam will be due during the 7th week of the course on Wednesday, March 6th at 8 pm.\nThe midterm conceptual exam will be administered during class on Thursday, March 7th.\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\nApproximately weekly quizzes will be administered through Canvas and due each Wednesday at 8 pm\nApproximately weekly application assignments will be submitted via Canvas and due each Wednesday at 8 pm.\n\n\n\nApplication Assignments\nThe approximately weekly application assignments are due on Wednesdays at 8 pm through Canvas. These assignments are to be done individually. Please do not share answers or code. You are also encouraged to make use of online resources (e.g., stack overflow) for assistance. All assignments will be completed using R markdown to provide both the code and documentation as might be provided to your mentor or employer to fully describe your solution. Late assignments are not accepted because problem solutions are provided immediately after the due date. Application assignments are graded on a three-point scale (0 = not completed, 1 = partially completed and/or with many errors, 2 = fully completed and at least mostly correct). Grades for each assignment will be posted by the following Monday at the latest.\n\n\nChatGPT\nI suspect you have all seen discussions of all that ChatGPT can do by now and its impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it. Given that I expect AI like ChatGPT to become a useful tool in our workflow as professionals, now is the time to start to learn how it can help. Therefore, you are free to use it during any of our application assignments AND the application questions on the mid-term and final exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful. In contrast, you CANNOT use ChatGPT to answer the conceptual questions on the two exams or the weekly quizzes. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head and I want to be 100% clear that use of ChatGPT to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating. It will be reported to the Dean of Students on first offense.\n\n\nStudent Ethics\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester.\nIn the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.\n\n\nDiversity and Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/\n\n\nAcademic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to http://studentconduct.wiscweb.wisc.edu/academic-integrity\n\n\nAccommodations Polices\nMcBurney Disability Resource Center syllabus statement: “The University of Wisconsin-Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/\n\n\nComplaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawngreen@wisc.edu.\nIf you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system\n\n\nPrivacy of Student Information & Digital Tools\nThe privacy and security of faculty, staff and students’ personal information is a top priority for UW-Madison. The university carefully reviews and vets all campus-supported digital tools used to support teaching and learning, to help support success through learning analytics, and to enable proctoring capabilities. UW-Madison takes necessary steps to ensure that the providers of such tools prioritize proper handling of sensitive data in alignment with FERPA, industry standards and best practices. Under the Family Educational Rights and Privacy Act (FERPA which protects the privacy of student education records), student consent is not required for the university to share with school officials those student education records necessary for carrying out those university functions in which they have legitimate educational interest. 34 CFR 99.31(a)(1)(i)(B). FERPA specifically allows universities to designate vendors such as digital tool providers as school officials, and accordingly to share with them personally identifiable information from student education records if they perform appropriate services for the university and are subject to all applicable requirements governing the use, disclosure and protection of student data.\n\n\nPrivacy of Student Records & the Use of Audio Recorded Lectures\nSee information about privacy of student records and the usage of audio-recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW-Madison. Students in this course may use the materials and recordings for their personal use related to participation in this class. Students may also take notes solely for their personal use. If a lecture is not already recorded, you are not authorized to record my lectures without my permission unless you are considered by the university to be a qualified student with a disability requiring accommodation. [Regent Policy Document 4-1] Students may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities. Students are also prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nAcademic Calendar & Religious Observances\nStudents who wish to inquire about religious observance accommodations for exams or assignments should contact the instructor within the first two weeks of class, following the university’s policy on religious observance conflicts",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html",
    "href": "02_sampling_distributions.html",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "",
    "text": "2.1 Inferential Statistics\nInferential statistics are used to estimate parameters in the population from parameter estimates in a sample drawn from that population.\nIn inferential statistics, we use these parameter estimates to test hypotheses (predictions; null and alternative hypotheses) about the size of the population parameter.\nThese predictions about the size of population parameters typically map directly onto research questions about (causal) relationships between variables (IVs and DV).\nAnswers from inferential statistics are probabilistic. In other words, all answers have the potential to be wrong and you will provide an index of that probability along with your results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#populations",
    "href": "02_sampling_distributions.html#populations",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.2 Populations",
    "text": "2.2 Populations\nA population is any clearly defined set of objects or events (people, occurrences, animals, etc.). Populations usually represent all events in a particular class (e.g., all college students, all alcoholics, all depressed people, all people). It is often an abstract concept because in many/most instances you will never have access to the entire population.\nFor example, many of our studies may have the population of all people as its target.\nNonetheless, researchers usually want to describe or draw conclusions about populations (e.g., We don’t care if some new drug is an effective treatment for 100 people in your sample. Will it work, on average, for everyone we might treat?).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#population-parameters",
    "href": "02_sampling_distributions.html#population-parameters",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.3 (Population) Parameters",
    "text": "2.3 (Population) Parameters\nA parameter is a value used to describe a certain characteristic of a population. It is usually unknown and therefore has to be estimated.\nFor example, the population mean is a parameter that is often used to indicate the average/typical value of a variable in the population.\nWithin a population, a parameter is a fixed value which does not vary within the population at the time of measurement (e.g., the mean height of people in the US at the present moment).\nYou typically can’t calculate these parameters directly because you don’t have access to the entire population.\nWe use Greek letters to represent population parameters (\\(\\mu\\), \\(\\sigma\\), \\(\\sigma^2\\), \\(\\beta_0\\), \\(\\beta_j\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#samples-parameter-estimates",
    "href": "02_sampling_distributions.html#samples-parameter-estimates",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.4 Samples & Parameter Estimates",
    "text": "2.4 Samples & Parameter Estimates\nA sample is a finite group of units (e.g., participants) selected from the population of interest.\nA sample is generally selected for a study because the population is too large to study in its entirety. We typically have only one sample in a study.\nWe use the sample to estimate and test parameters in the population.\nThese estimates are called parameter estimates.\nWe use Roman letters to represent sample parameter estimates (\\(\\overline{X}\\), \\(s\\), \\(s^2\\), \\(b_0\\), \\(b_j\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-error",
    "href": "02_sampling_distributions.html#sampling-error",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.5 Sampling Error",
    "text": "2.5 Sampling Error\nSince a sample does not include all members of the population, parameter estimates generally differ from parameters on the entire population (e.g., use mean height of a sample of 1000 people to estimate mean height of US population).\nThe difference between the (sample) parameter estimate and the (population) parameter is sampling error.\nYou will not be able to calculate the sampling error of your parameter estimate directly because you don’t know the value of the population parameter. However, you can estimate it by probabilistic modeling of the hypothetical sampling distibution for that parameter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#hypothetical-sampling-distribution",
    "href": "02_sampling_distributions.html#hypothetical-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.6 Hypothetical Sampling Distribution",
    "text": "2.6 Hypothetical Sampling Distribution\nA sampling distribution is a probability distribution for a parameter estimate drawn from all possible samples of size \\(N\\) taken from a population.\nA sampling distribution can be formed for any population parameter.\nEach time you draw a sample of size \\(N\\) from a population you can calculate an estimate of that population parameter from that sample.\nBecause of sampling error, these parameter estimates will not exactly equal the population parameter. They will not equal each other either. They will form a distribution.\nA sampling distribution, like a population, is an abstract concept that represents the outcome of repeated (infinite) sampling. You will typically only have one sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#what-if-we-didnt-need-samples",
    "href": "02_sampling_distributions.html#what-if-we-didnt-need-samples",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.7 What if we didn’t need samples?",
    "text": "2.7 What if we didn’t need samples?\nResearch question: How do inhabitants of a remote pacific island feel about the ocean? Population size = 10,000.\nDependent measure: Ocean liking scale scores that range from -100 (strongly dislike) to 100 (strongly like). 0 represents neutral.\nHypotheses: \\(H_0: \\mu = 0; H_a: \\mu \\neq 0\\)\nQuestion: How would you answer this question if you had unlimited resources (e.g., time, money, and patience)?\n\nAdminister the Ocean liking scale to all 10,000 inhabitants in the population and calculate the population mean score. Is it 0? If not, the inhabitants are not neutral on average.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#ocean-liking-scale-scores-in-full-population",
    "href": "02_sampling_distributions.html#ocean-liking-scale-scores-in-full-population",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.8 Ocean Liking Scale Scores in Full Population",
    "text": "2.8 Ocean Liking Scale Scores in Full Population\n\n\nCode\nlibrary(tidyverse)\n1path_data &lt;- \"data_lecture\"\n\n2data &lt;- read_csv(here::here(path_data, \"2_sampling_distributions_like.csv\"),\n                 show_col_types = FALSE) \n\n\n\n1\n\nThis path points to where your data is and should be a relative path from your R project.\n\n2\n\nwe use the here() function in the here package (here::here()) to define paths within a function. This approach (vs. file.path()) works well when using R Projects.\n\n\n\n\n\nSee also:\nview() allows you to open up the data frame in rstudio.\nglimpse() is a useful function that you can pipe tibbles into when first reading them in. It shows you useful information, like number of rows and column, variable (column) names, a sample of what the data look like, and the class of each variable (e.g., double, character, factor).\n\n\nCode\ndata |&gt; \n  glimpse()\n\n\nRows: 10,000\nColumns: 2\n$ subid      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ like_score &lt;dbl&gt; -23.608554, -9.011961, 30.536791, 5.886441, -9.164940, 23.6…\n\n\n\nskim() and other functions in the skimr package are helpful for quick summaries of your data (e.g., missingness, distribution, type of data)\n\n\nCode\ndata |&gt; \n  skimr::skim()\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsubid\n0\n1\n5000.5\n2886.90\n1.00\n2500.75\n5000.5\n7500.25\n10000.00\n▇▇▇▇▇\n\n\nlike_score\n0\n1\n0.0\n23.67\n-86.64\n-16.08\n0.1\n16.14\n84.46\n▁▃▇▃▁\n\n\n\n\n\n\nIf we want less information about the distribution we can use summarise and specify the descriptive statistics we want.\n\n\nCode\ndata |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n\n# A tibble: 1 × 3\n      n      mean    sd\n  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 10000 -6.27e-16  23.7\n\n\n\nYou can also pull out a few rows to look at your data. This can be done using slice_head() to pull out \\(n\\) top rows of the data set, slice_tail() to pull out \\(n\\) bottom rows of the data set, or slice_sample() to pull out a random \\(n\\) of rows from the data set.\n\n\nCode\ndata |&gt; \n  slice_head(n = 5) \n\n\n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     1     -23.6 \n2     2      -9.01\n3     3      30.5 \n4     4       5.89\n5     5      -9.16\n\n\n\n\nCode\ndata |&gt; \n  slice_tail(n = 5) \n\n\n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  9996     -16.6 \n2  9997      -8.37\n3  9998      18.6 \n4  9999     -16.2 \n5 10000     -17.0 \n\n\n\n\n\nCode\n1set.seed(101)\ndata |&gt; \n  slice_sample(n = 5)\n\n\n\n1\n\nWhenever you are using random numbers it is important to set a seed first (set.seed()). This ensures you can reproduce that randomness!\n\n\n\n\n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  8009     -27.0 \n2  2873     -41.5 \n3  3281      17.8 \n4  5562      27.4 \n5  5471      -9.84\n\n\n\n\n\nCode\nplot_raw &lt;- data |&gt; \n  ggplot(aes(x = like_score)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(limits = c(-100, 100)) +\n1  theme_classic()\n\n\n\n1\n\nWe can use themes to customize the output of our figures. This can be piped into your ggplot() code or set globally at the top of your script using the code below:\n\n\n\n\n\n\nCode\ntheme_set(theme_classic()) \n\n\n\n\n\nCode\nplot_raw",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#parameter-estimation-and-testing",
    "href": "02_sampling_distributions.html#parameter-estimation-and-testing",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.9 Parameter Estimation and Testing",
    "text": "2.9 Parameter Estimation and Testing\nQuestion: What do you conclude?\n\nInhabitants of the island are neutral on average on the Ocean Liking Scale; \\(\\mu\\) = 0.\n\nQuestion: How confident are you about this conclusion?\n\nExcluding issues of measurement of the scale (i.e., reliability), you are 100% confident that the population mean score on this scale is 0 (\\(\\mu\\) = 0).\n\nQuestion: Of course, this approach to answering a research question is not typical. Why? And how would you normally answer this question?\n\nYou will very rarely have access to all scores in the population. Instead, you have to use inferential statistics to “infer” (estimate) the size of the population parameter from a sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#obtain-a-sample",
    "href": "02_sampling_distributions.html#obtain-a-sample",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.10 Obtain a Sample",
    "text": "2.10 Obtain a Sample\nYou are a poor graduate student. All you can afford is \\(N = 10\\).\n\n\nCode\nset.seed(2005) \ndata_sample_1 &lt;- data |&gt; \n   slice_sample(n = 10)\n\ndata_sample_1 |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n\n# A tibble: 1 × 3\n      n  mean    sd\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10  2.14  19.4\n\n\nQuestion: What do you conclude and why?\n\nA sample mean of 2.14 is not 0. However, you know that the sample mean will not match the population mean exactly. How likely is it to get a sample mean of 2.14 if the population mean is 0 (think about it!)?\n\nYour friend is a poor graduate student too. All she can afford is \\(N = 10\\) too.\n\n\nCode\ndata_sample_2 &lt;- data |&gt; \n   slice_sample(n = 10)\n\ndata_sample_2 |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n\n# A tibble: 1 × 3\n      n  mean    sd\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10  1.74  23.0\n\n\nQuestion: What does she conclude and why?\n\nA sample mean of 1.74 is not 0. However, she knows that the sample mean will not match the population mean exactly. It is more likely to get a sample mean of 1.74 than 2.14 if the population mean is 0 but she still doesn’t know how likely either outcome is. What if she obtained a sample with mean of 30?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-distribution-of-the-mean",
    "href": "02_sampling_distributions.html#sampling-distribution-of-the-mean",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.11 Sampling Distribution of the Mean",
    "text": "2.11 Sampling Distribution of the Mean\nYou can construct a sampling distribution for any parameter estimate (e.g., mean, \\(s\\), min, max, \\(r\\), \\(b_0\\), \\(b_1\\)).\nFor the mean, you can think of the sampling distribution conceptually as follows:\n\nImagine drawing many samples (lets say 1000 samples but in theory, the sampling distribution is infinite) of \\(N\\)=10 participants (10 participants in each sample) from your population.\nNext, calculate the mean for each of these samples of 10 participants.\nFinally, create a histogram (or density plot) of these sample means.\n\n\n\n2.11.1 1000 Samples of \\(N\\)=10\n\n\nCode\n1get_sample_mean &lt;- function(data, n_sub) {\n  data |&gt; \n  slice_sample(n = n_sub) |&gt; \n  summarise(across(everything(), list(mean = mean, sd = sd), .names = \"{fn}\")) |&gt; \n  mutate(n = n_sub) \n}\n\n\n\n1\n\nHere we are writing a function that we will use to generate descriptive statistics over 1000 samples. We are keeping this function generic because we are going to use this function again later in the chapter for more simulation examples!\n\n\n\n\n\n\n\nCode\nset.seed(101)\nsamples &lt;- 1:1000 |&gt; \n1  map(\\(i) get_sample_mean(data = data[, \"like_score\"], n_sub = 10)) |&gt;\n  list_rbind()\n\nsamples\n\n\n\n1\n\nWe use the map() function to repeat our function 1000 times.\n\n\n\n\n# A tibble: 1,000 × 3\n     mean    sd     n\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -6.89   22.4    10\n 2 11.9    20.2    10\n 3  0.147  15.9    10\n 4 -8.93   27.3    10\n 5  8.52   20.0    10\n 6  2.02   10.6    10\n 7 -0.198  16.9    10\n 8 -0.230  21.1    10\n 9  1.01   22.7    10\n10  6.26   28.4    10\n# ℹ 990 more rows\n\n\n\n\n\n2.11.2 Sampling Distribution of the Mean\n\n\nCode\nsamples |&gt; \n  summarise(n = n(), \n            mean_m = mean(mean), \n            sd = sd(mean))\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000 -0.238  7.69\n\n\nCode\nplot_samples &lt;- samples |&gt; \n  ggplot(aes(x = mean)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(\"sample means\", limits = c(-100, 100))\n\n\n\n\n\nCode\nplot_samples\n\n\n\n\n\n\n\n\n\nNote: In your research, you don’t form a sampling distribution by repeated sampling. You (typically) only have one sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#raw-score-distribution-vs.-sampling-distribution",
    "href": "02_sampling_distributions.html#raw-score-distribution-vs.-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.12 Raw Score Distribution vs. Sampling Distribution",
    "text": "2.12 Raw Score Distribution vs. Sampling Distribution\nThe distinction between the raw score distribution and your sample distribution is very important to keep clear in your mind!\n\n\nCode\n1library(patchwork)\n\n\n\n1\n\nThe patchwork package is an easy and customizable way to combine ggplot() objects. For more information see https://patchwork.data-imaginist.com/.\n\n\n\n\n\n\n\nCode\nplot_raw + plot_samples",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-distribution-of-the-mean-2",
    "href": "02_sampling_distributions.html#sampling-distribution-of-the-mean-2",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.13 Sampling Distribution of the Mean",
    "text": "2.13 Sampling Distribution of the Mean\nQuestion: What will the mean of the sample means be? In other words, what is the mean of the sampling distribution?\n\nThe mean of the sample means (i.e., the mean of the sampling distribution) will equal the population mean of raw scores on the dependent measure. This is important because it indicates that the sample mean is an unbiased estimator of the population mean.\n\n\nCode\nplot_samples\n\n\n\n\n\n\n\n\n\n\nThe mean is an unbiased estimator: The mean of the sample means will equal the mean of the population. Therefore, individual sample means will neither systematically under or overestimate the population mean.\nRaw Ocean Liking Scores:\n\n\nCode\ndata |&gt; \n  summarise(n = n(), \n            mean_m = round(mean(like_score), 2), \n            sd = sd(like_score))\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 10000      0  23.7\n\n\nSample (N=10) Means:\n\n\nCode\nsamples |&gt; \n  summarise(n = n(), \n            mean_m = round(mean(mean), 2), \n            sd = sd(mean))\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000  -0.24  7.69\n\n\n\nThe sample variance (\\(s^2\\); with n-1 denominator) is also an unbiased estimator of the population variance (\\(\\sigma^2\\)). In other words, the mean of the sample \\(s^2\\)’s will approximate the population variance. Sample \\(s\\) is negatively biased.\n\nQuestion: Will all of the sample means be the same?\n\nNo, there was a distribution of means that varied from each other. The mean of the sampling distribution was the population mean but the standard deviation was not zero.\n\n\nCode\nsamples |&gt; \n  summarise(n = n(), \n            mean_m = round(mean(mean), 2), \n            sd = sd(mean))\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000  -0.24  7.69\n\n\n\n\nCode\nplot_samples",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#standard-errorse",
    "href": "02_sampling_distributions.html#standard-errorse",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.14 Standard Error(SE)",
    "text": "2.14 Standard Error(SE)\nThe standard deviation of the sampling distribution (i.e., standard deviation of the infinite sample means) is equal to:\n\\(\\frac{\\sigma}{\\sqrt{N_{sample}}}\\)\nWhere \\(\\sigma\\) is the standard deviation of the population raw scores.\nThis variability in the sampling distribution is due to sampling error.\nTherefore, because we use parameter estimates calculated in our sample to estimate population parameters, we would like to minimize sampling error.\nThe standard deviation of the sampling distribution for a parameter estimate has a technical name. It is called the standard error of the parameter estimate. Here, we are talking about the standard error of the mean.\n\nQuestion: What factors affect the size of the sampling error of the mean (i.e., the standard error)?\n\nThe standard deviation of the population raw scores and the sample size.\n\nQuestion: Variation among raw scores for a variable in the population is broadly caused by two factors. What are they?\n\n1. Individual differences.\n2. Measurement error (the opposite of reliability)\n\nQuestion: What is the relationship between population variability (\\(\\sigma\\)) and SE?\n\nAs the variability of the variable increases in the population, the SE increases..\n\nQuestion: What would happen to SE if there was no variation in population scores?\n\nThe SE would equal 0 no matter which participants you sampled. They would all have the same scores!\n\nQuestion: What is the relationship between sample size and SE?\n\nAs the sample size increases, the SE for the statistic will decrease.\n\nQuestion: What would the SE be if the sample size equalled population size?\n\nIf the sample contained all participants from the population, the SE would be equal to 0 because each sample mean would have exactly the same value as the overall population mean (because all same scores).\n\nQuestion: What would happen if the samples contained only 1 participant?\n\nIf each sample contained only 1 participant, the SE would be equal to the variation (\\(\\sigma\\)) observed within the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#shape-of-the-sampling-distribution",
    "href": "02_sampling_distributions.html#shape-of-the-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.15 Shape of the Sampling Distribution",
    "text": "2.15 Shape of the Sampling Distribution\nCentral Limit Theorem: The shape of the sampling distribution approaches normal as \\(N\\) increases.\nThe shape is roughly normal even for moderate sample sizes assuming that the original distribution isn’t really weird (i.e., non-normal).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#normal-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#normal-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.16 Normal Population and Various Sampling Distributions",
    "text": "2.16 Normal Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#uniform-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#uniform-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.17 Uniform Population and Various Sampling Distributions",
    "text": "2.17 Uniform Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#skewed-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#skewed-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.18 Skewed Population and Various Sampling Distributions",
    "text": "2.18 Skewed Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#an-important-normal-distribution-z-scores",
    "href": "02_sampling_distributions.html#an-important-normal-distribution-z-scores",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.19 An Important Normal Distribution: Z-scores",
    "text": "2.19 An Important Normal Distribution: Z-scores\nThe \\(z\\) distribution contains normally distributed scores with a mean of 0 and a standard deviation of 1.\nYou can therefore think of any specific z-score as telling you the position of the score in terms of standard deviations above the mean.\nThe probability distribution is known for the \\(z\\) distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#probability-of-parameter-estimate-given-h_0",
    "href": "02_sampling_distributions.html#probability-of-parameter-estimate-given-h_0",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.20 Probability of Parameter Estimate Given \\(H_0\\)",
    "text": "2.20 Probability of Parameter Estimate Given \\(H_0\\)\nHow could you use the \\(z\\) distribution to determine the probability of obtaining a sample mean (parameter estimate) of 2.40 if you draw a sample of \\(N=10\\) from a population of Ocean Liking scores with a population mean (parameter) of 0?\nThink about it……",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#hypothetical-sampling-distribution-for-h_0",
    "href": "02_sampling_distributions.html#hypothetical-sampling-distribution-for-h_0",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.21 Hypothetical Sampling Distribution for \\(H_0\\)",
    "text": "2.21 Hypothetical Sampling Distribution for \\(H_0\\)\nIf \\(H_0\\) is true; the sampling distribution has a mean of 0 and standard deviation of \\(\\frac{\\sigma}{\\sqrt{N_{sample}}} =  \\frac{23.7}{\\sqrt{10}}  =  7.5\\).\n\n\n\n\n\n\n\n\n\n\nQuestion: If \\(H_0\\) is true and this is the sampling distribution (in blue), how likely is it to get a sample mean of 2.4 or more extreme?\n\n\n\n\n\n\n\n\n\n\nPretty likely…\nBut we can do better than that!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#our-first-inferential-test-the-z-test",
    "href": "02_sampling_distributions.html#our-first-inferential-test-the-z-test",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.22 Our First Inferential Test: The z-test",
    "text": "2.22 Our First Inferential Test: The z-test\n\\(z = \\frac{2.4 - 0}{7.5} = 0.32; p \\le .749\\)\n\n\nCode\npnorm(0.32, mean=0, sd=1, lower.tail=FALSE) * 2 \n\n\n[1] 0.7489683",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#t-vs.-z",
    "href": "02_sampling_distributions.html#t-vs.-z",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.23 \\(t\\) vs. \\(z\\)",
    "text": "2.23 \\(t\\) vs. \\(z\\)\n\\(z = \\frac{2.4 - 0}{7.5} = 0.32\\)\nQuestion: Where did we get the 2.4 from in our z-test?\n\nOur sample mean from our study. This is our parameter estimate of the population mean of OLS (like_score) scores.\n\nQuestion: Where did we get the 7.5 from in our z-test and what is the problem with this?\n\nThis was our estimate of the standard deviation of the sampling distribution.\n\\(\\frac{\\sigma}{\\sqrt{N_{sample}}}\\)\nWe do not know \\(\\sigma\\).\n\nQuestion: How can we estimate \\(\\sigma\\)?\n\nWe can use our sample standard deviation (\\(s\\)), but \\(s\\) is a negatively biased parameter estimate. On average, it will underestimate \\(\\sigma\\).\n\nQuestion: So what do we do?\n\nWe account for this underestimation of \\(\\sigma\\) and therefore of the standard deviation (standard error) of the sampling distribution by using the \\(t\\) distribution rather than the \\(z\\) distribution to calculate the probability of our parameter estimate if \\(H_0\\) is true.\nThe \\(t\\) distribution is slightly wider, particularly for small sample sizes to correct for our underestimate of the standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#our-second-inferential-test-one-sample-t-test",
    "href": "02_sampling_distributions.html#our-second-inferential-test-one-sample-t-test",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.24 Our Second Inferential Test: One Sample t-test",
    "text": "2.24 Our Second Inferential Test: One Sample t-test\n\\(t(df)\\) = \\(\\frac{\\text{Parameter estimate – Parameter:} H_0}{\\text{Standard error of parameter estimate}}\\)\nWhere SE is estimated using \\(s\\) from sample data.\n\\(df = N – P = 10 - 1 = 9\\)\n\n\n\n\n\n\n\n\n\n\nThe bias in \\(s\\) decreases with increasing \\(N\\). Therefore, \\(t\\) approaches \\(z\\) with larger sample sizes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#null-hypothesis-significance-testing-nhst",
    "href": "02_sampling_distributions.html#null-hypothesis-significance-testing-nhst",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.25 Null Hypothesis Significance Testing (NHST)",
    "text": "2.25 Null Hypothesis Significance Testing (NHST)\n\nDivide reality regarding the size of the population parameter into two non-overlapping possibilities: Null hypothesis (\\(H_0\\)) & Alternate hypothesis (\\(H_a\\)).\nAssume that \\(H_0\\) is true.\nCollect data.\nCalculate the probability (\\(p\\)-value) of obtaining your parameter estimate (or a more extreme estimate) given your assumption (i.e., \\(H_0\\) is true)\nCompare probability to some cut-off value (alpha level).\n\nIf this parameter estimate is less probable than cut-off value, reject \\(H_0\\) in favor of \\(H_a\\).\nIf data is not less probable, fail to reject \\(H_0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html",
    "href": "03_single_mean.html",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "",
    "text": "3.1 Units 3-4 Organization",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#units-3-4-organization",
    "href": "03_single_mean.html#units-3-4-organization",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "",
    "text": "First, consider details of simplest model (one parameter estimate; mean-only model; no \\(X\\)s).\nNext, examine simple regression (two parameter estimates; one \\(X\\) for one quantitative predictor variable).\nThese provide a critical foundation for all linear models.\nSubsequent units will generalize to one dichotomous variable (Unit 5), multiple predictor variables (Units 6-7), and beyond…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#linear-models-as-models",
    "href": "03_single_mean.html#linear-models-as-models",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.2 Linear Models as Models",
    "text": "3.2 Linear Models as Models\nLinear models (including regression) are models.\nDATA = MODEL + ERROR\nThree general uses for models:\n\nDescribe and summarize DATA (\\(Y\\)s) in a simpler form using MODEL.\nPredict DATA (\\(Y\\)s) from MODEL.\nUnderstand (test inferences about) complex relationships between individual regressors (\\(X\\)s) in MODEL and the DATA (\\(Y\\)s). How precise are estimates of relationship?\n\nMODELS are simplifications of reality. As such, there is ERROR. They also make assumptions that must be evaluated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#fear-potential-startle",
    "href": "03_single_mean.html#fear-potential-startle",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.3 Fear Potential Startle",
    "text": "3.3 Fear Potential Startle\nWe are interested in producing anxiety in the laboratory.\nTo do this, we develop a procedure where we expose people to periods of unpredictable electric shock administration alternating with periods of safety.\nWe measure their startle response in the shock and safe periods.\nWe use the difference between their startle during shock – safe to determine if they are anxious. This is called Fear potentiated startle (FPS). Our procedure works if FPS &gt; 0. We need a model of FPS scores to determine if FPS &gt; 0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#fear-potentiated-startle-one-parameter-model",
    "href": "03_single_mean.html#fear-potentiated-startle-one-parameter-model",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.4 Fear Potentiated Startle: One parameter model",
    "text": "3.4 Fear Potentiated Startle: One parameter model\nA very simple model for the population of FPS scores would predict the same value for everyone in the population.\n\\(\\hat{Y}_i=\\beta_0\\)\nWe would like this value to be the best prediction.\nQuestion: In the context of DATA = MODEL + ERROR, how can we quantify best?\n\nWe want to predict some characteristic about the population of FPS scores that minimizes the ERROR from our model.\nERROR = DATA – MODEL\n\\(\\varepsilon_i=Y_i-\\hat{Y}_i\\); There is an error (\\(\\varepsilon_i\\)) for each population score.\n\nQuestion: How can we quantify total model error?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#total-error",
    "href": "03_single_mean.html#total-error",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.5 Total Error",
    "text": "3.5 Total Error\nSum of errors across all scores in the population isn’t ideal because positive and negative errors will tend to cancel each other out.\n\\(\\sum(Y_i-\\hat{Y}_i)\\)\nSum of absolute values of errors could work. If we selected \\(\\beta_0\\) to minimize the sum of the absolute value of errors, \\(\\beta_0\\) would equal the median of the population.\n\\(\\sum(|Y_i-\\hat{Y}_i|)\\)\nSum of squared errors (SSE) could work. If we selected \\(\\beta_0\\) to minimize the sum of squared errors, \\(\\beta_0\\) would equal the mean of the population.\n\\(\\sum(Y_i-\\hat{Y}_i)^2\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#one-parameter-model-for-fps",
    "href": "03_single_mean.html#one-parameter-model-for-fps",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.6 One Parameter Model for FPS",
    "text": "3.6 One Parameter Model for FPS\nFor the moment, lets assume we prefer to minimize SSE (more on that in a moment). You should predict the population mean FPS for everyone.\n\\(\\hat{Y}_i=\\beta_0\\) where \\(\\beta_0=\\mu\\)\nQuestion: What is the problem with this model and how can we fix this problem?\n\nWe don’t know the population mean for FPS scores (\\(\\mu\\)).\nWe can collect a sample from the population and use the sample mean (\\(\\overline{X}\\)) as an estimate of the population mean (\\(\\mu\\)). \\(\\overline{X}\\) is an unbiased estimate for \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#model-parameter-estimation",
    "href": "03_single_mean.html#model-parameter-estimation",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.7 Model Parameter Estimation",
    "text": "3.7 Model Parameter Estimation\nPopulation model\n\\(\\hat{Y}_i=\\beta_0\\) where \\(\\beta_0=\\mu\\)\n\\(Y_i=\\beta_0+\\varepsilon_i\\)\nEstimate population parameters from sample\n\\(Y_i=b_0\\) where \\(b_0=\\overline{X}\\)\n\\(Y_i=b_0+e_i\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#least-squares-criterion",
    "href": "03_single_mean.html#least-squares-criterion",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.8 Least Squares Criterion",
    "text": "3.8 Least Squares Criterion\nIn ordinary least squares (OLS) regression and other least squares linear models, the model parameter estimates (e.g., \\(b_0\\)) are calculated such that they minimize the sum of squared errors (SSE) in the sample in which you estimate the model.\n\\(\\text{SSE}=\\sum(Y_i-\\hat{Y}_i)^2\\)\n\\(\\text{SSE}=\\sum e_i^2\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#properties-of-parameter-estimates",
    "href": "03_single_mean.html#properties-of-parameter-estimates",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.9 Properties of Parameter Estimates",
    "text": "3.9 Properties of Parameter Estimates\nThere are three properties that make a parameter estimate attractive.\n\nUnbiased: Mean of the sampling distribution for the parameter estimate is equal to the value for that parameter in the population.\nEfficient: The sample estimates are close to the population parameter. In other words, the narrower the sampling distribution for any specific sample size \\(N\\), the more efficient the estimator. Efficient means small SE for parameter estimate.\nConsistent: As the sample size increases, the sampling distribution becomes narrower (more efficient). Consistent means as \\(N\\) increases, SE for parameter estimate decreases",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#least-squares-criterion-continued",
    "href": "03_single_mean.html#least-squares-criterion-continued",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.10 Least Squares Criterion (Continued)",
    "text": "3.10 Least Squares Criterion (Continued)\nIf the \\(\\varepsilon_i\\) are normally distributed, both the median and the mean are unbiased and consistent estimators.\nThe variance of the sampling distribution for the mean is:\n\\(\\frac{\\sigma^2}{N}\\)\nThe variance of the sampling distribution for the mean is:\n\\(\\frac{\\pi\\sigma^2}{2N}\\)\nTherefore, the mean is the more efficient parameter.\nFor this reason, we tend to prefer to estimate our models by minimizing the sum of squared errors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#fear-potentiated-startle-during-threat-of-shock",
    "href": "03_single_mean.html#fear-potentiated-startle-during-threat-of-shock",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.11 Fear-potentiated Startle During Threat of Shock",
    "text": "3.11 Fear-potentiated Startle During Threat of Shock\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_classic()) \npath_data &lt;- \"data_lecture\"  \n\ndata &lt;- read_csv(here::here(path_data, \"3_single_mean_fps.csv\"), \n                 show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 96\nColumns: 2\n$ subid &lt;dbl&gt; 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 111, 112, 113, 1…\n$ fps   &lt;dbl&gt; 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#descriptives-and-univariate-plots",
    "href": "03_single_mean.html#descriptives-and-univariate-plots",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.12 Descriptives and Univariate Plots",
    "text": "3.12 Descriptives and Univariate Plots\n\n\nCode\ndata |&gt; \n  summarise(n = n(),\n            mean = mean(fps),\n            sd = sd(fps),\n            min = min(fps),\n            max = max(fps))\n\n\n# A tibble: 1 × 5\n      n  mean    sd   min   max\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    96  32.2  37.5 -98.1  163.\n\n\n\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = fps)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#fps-experiment-the-inference-details",
    "href": "03_single_mean.html#fps-experiment-the-inference-details",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.13 FPS Experiment: The Inference Details",
    "text": "3.13 FPS Experiment: The Inference Details\nGoal: Determine if our shock threat procedure is effective at potentiating startle (increasing startle during threat relative to safe).\nCreate a simple model of FPS scores in the population.\n\\(\\text{FPS}=\\beta_0\\)\nCollect sample of \\(N=96\\) to estimate \\(\\beta_0\\).\nCalculate sample parameter estimate (\\(b_0\\)) that minimizes SSE in sample.\nUse \\(b_0\\) to test hypotheses.\n\\(H_0: \\beta_0 = 0\\)\n\\(H_a: \\beta_0 \\neq 0\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#estimating-a-one-parameter-model-in-r",
    "href": "03_single_mean.html#estimating-a-one-parameter-model-in-r",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.14 Estimating a One Parameter Model in R",
    "text": "3.14 Estimating a One Parameter Model in R\n\n\nCode\n1m = lm(fps ~ 1, data = data)\n\n\n\n1\n\nHere we are fitting a linear model with FPS regressed on the intercept. In other words, we are fitting a model that predicts FPS using only the mean.\n\n\n\n\n\nWe can pull out the errors (\\(e_i=Y_i-\\hat{Y}i\\)) for each observation in the sample using residuals()\n\n\nCode\nresiduals(m)\n\n\n           1            2            3            4            5            6 \n -12.6999127   16.2161040  -54.7193405  -25.4670572   57.4678817    8.3829373 \n           7            8            9           10           11           12 \n  -2.7175738  -16.8541238   19.6643817   58.6873817   78.7543262   35.0963817 \n          13           14           15           16           17           18 \n -29.4627960   72.7258928  -31.7275460   36.5672151   19.1260706  -30.9964738 \n          19           20           21           22           23           24 \n   1.5669373   11.7176040    9.3662151  -25.3710072 -130.2886183   53.1913817 \n          25           26           27           28           29           30 \n  29.8681317   59.8164373  -14.1219516   34.7095484   17.9774928   47.3338484 \n          31           32           33           34           35           36 \n  61.4058262   67.7537262  104.6339928   36.5526595   14.2658262  -16.7506349 \n          37           38           39           40           41           42 \n -29.6592294   12.9909373   20.9858817  -29.1695572  -24.1598966  -19.2076849 \n          43           44           45           46           47           48 \n  11.7108262  -25.2434516  -18.4250627  -20.3317905   -8.4337683  -18.0094960 \n          49           50           51           52           53           54 \n -12.7704849    3.9210484  -58.2597294  -35.5108960  -32.0183927   -1.7377294 \n          55           56           57           58           59           60 \n   0.3123817  -35.5405016  -12.5921183   25.0772151  -20.6439405   37.4066428 \n          61           62           63           64           65           66 \n   9.3974373  130.5457706    5.2138262  -13.0036627   -9.8150183  -27.4784549 \n          67           68           69           70           71           72 \n  17.0578817   27.5951151  -28.0089794  -28.5735072  -23.4260627    4.5087151 \n          73           74           75           76           77           78 \n  77.8639373  -21.4575572  -18.5716738  -17.1700072   27.4325484  -26.4386960 \n          79           80           81           82           83           84 \n -18.1054016    6.1488262  -14.5139683    1.6943262  -21.4997294  -25.3833322 \n          85           86           87           88           89           90 \n -26.9358794  -17.5872294  -25.7722738    4.8073817  -26.9565572  -32.1845627 \n          91           92           93           94           95           96 \n -31.0086183  -34.0540127  -17.4630572  -31.4756127  -31.8114616  -15.9328183 \n\n\n\nWe can also easily calculate the SSE with the following code:\n\n\nCode\nsum(residuals(m)^2)\n\n\n[1] 133888.3\n\n\nThis tells us about how well the model fits the data. Specifically it is the sum of the squared differences between the predicted values and the actual participant scores\n\nWe can get the predicted value for each individual in the sample using this model with the function predict().\n\\(\\hat{Y}=32.19\\)\n\n\nCode\npredict(m)\n\n\n       1        2        3        4        5        6        7        8 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n       9       10       11       12       13       14       15       16 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      17       18       19       20       21       22       23       24 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      25       26       27       28       29       30       31       32 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      33       34       35       36       37       38       39       40 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      41       42       43       44       45       46       47       48 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      49       50       51       52       53       54       55       56 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      57       58       59       60       61       62       63       64 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      65       66       67       68       69       70       71       72 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      73       74       75       76       77       78       79       80 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      81       82       83       84       85       86       87       88 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      89       90       91       92       93       94       95       96 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n\n\n\nWe also may want to look at the parameter coefficients (in this case we are only looking at the intercept). We can use the tidy() function from the broom package to do this.\n\n\nCode\nm |&gt; \n  broom::tidy()\n\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     32.2      3.83      8.40 4.26e-13\n\n\nThe estimate is \\(b_0\\), the unbiased sample estimate of \\(\\beta_0\\), and its standard error. It is also called the intercept in regression (more on this later).\n\\(\\hat{Y}_i=b_0=32.2\\)\nThe statistic is the t-statistic to test the \\(H_0\\) that \\(\\beta_0=0\\).\nThe probability (p-value) of obtaining a sample \\(b_0=32.2\\) if \\(H_0\\) is true (\\(\\beta_0=0\\)) is &lt; .0001.\nDescribe the logic of how this was determined given your understanding of sampling distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#sampling-distribution-testing-inferences-about-beta_0",
    "href": "03_single_mean.html#sampling-distribution-testing-inferences-about-beta_0",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.15 Sampling Distribution: Testing Inferences About \\(\\beta_0\\)",
    "text": "3.15 Sampling Distribution: Testing Inferences About \\(\\beta_0\\)\n\\(H_0: \\beta_0 = 0; H_a: \\beta_0 \\neq 0\\)\nIf \\(H_0\\) is true, the sampling distribution for \\(\\beta_0\\) will have a mean of 0. We can estimate standard deviation of the sampling distribution with SE for \\(b_0\\).\n\\(t(df=N-P)=\\frac{b_0-0}{\\text{SE}_{b_0}}=\\frac{32.2-0}{3.8}=8.40\\)\n\\(b_0\\) is approximately 8 standard deviations above the expected mean of the distribution if \\(H_0\\) is true.\n\nWe can use pt() to calculate the \\(p\\) value.\n\n\nCode\npt(8.40,95,lower.tail=FALSE) * 2\n\n\n[1] 0.0000000000004293253\n\n\nThe probability of obtaining a sample \\(b_0\\) = 32.2 (or more extreme) if \\(H_0\\) is true is very low (&lt; .05). Therefore we reject \\(H_0\\) and conclude that \\(\\beta_0 \\neq 0\\) and \\(b_0\\) is our best (unbiased) estimate of it.\n\n\n\nCode\ntibble(b0 = seq(-40,40,.01),\n       probability = dt(b0/broom::tidy(m)$std.error, m$df.residual)) |&gt; \n  ggplot(aes(x = b0, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = broom::tidy(m)$estimate, color = \"red\") +\n  labs(title = \"Sampling Distribution for b0\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#statistical-inference-and-model-comparisons",
    "href": "03_single_mean.html#statistical-inference-and-model-comparisons",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.16 Statistical Inference and Model Comparisons",
    "text": "3.16 Statistical Inference and Model Comparisons\nStatistical inference about parameters is fundamentally about model comparisons.\nYou are implicitly (t-test of parameter estimate) or explicitly (F-test of model comparison) comparing two different models of your data.\nWe follow Judd et al and call these two models the compact model and the augmented model.\nThe compact model will represent reality as the null hypothesis predicts. The augmented model will represent reality as the alternative hypothesis predicts.\nThe compact model is simpler (fewer parameters) than the augmented model. It is also nested in the augmented model (i.e. a subset of parameters).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#model-comparisons-testing-inferences-about-beta_0",
    "href": "03_single_mean.html#model-comparisons-testing-inferences-about-beta_0",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.17 Model Comparisons: Testing Inferences about \\(\\beta_0\\)",
    "text": "3.17 Model Comparisons: Testing Inferences about \\(\\beta_0\\)\n\\(\\hat{\\text{FPS}_i}=\\beta_0\\)\n\\(H_0: \\beta_0 = 0\\)\n\\(H_a: \\beta_0 \\neq 0\\)\nCompact model: \\(\\hat{\\text{FPS}_i}=0\\)\nAugmented model: \\(\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)\\)\nWe estimate 0 parameters (\\(P=0\\)) in this compact model.\nWe estimate 1 parameter (\\(P=1\\)) in this augmented model.\nChoosing between these two models is equivalent to testing if \\(\\beta_0 = 0\\) as you did with the t-test.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#model-comparison-plots",
    "href": "03_single_mean.html#model-comparison-plots",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.18 Model Comparison Plots",
    "text": "3.18 Model Comparison Plots\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = \"\", y = fps)) +\n  geom_jitter(width = 0.1, alpha = .6, size = 2) +\n  theme(axis.line.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  xlab(NULL) +\n  geom_hline(yintercept = 0, color = \"red\", linewidth = 1) +\n  geom_hline(yintercept = broom::tidy(m)$estimate, color = \"blue\", linewidth = 1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#model-comparisons-testing-inferences-about-beta_0-continued",
    "href": "03_single_mean.html#model-comparisons-testing-inferences-about-beta_0-continued",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.19 Model Comparisons: Testing Inferences about \\(\\beta_0\\) (Continued)",
    "text": "3.19 Model Comparisons: Testing Inferences about \\(\\beta_0\\) (Continued)\nCompact model: \\(\\hat{\\text{FPS}_i}=0\\)\nAugmented model: \\(\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)\\)\nWe can compare (and choose between) these two models by comparing their total error (SSE) in our sample.\n\\(\\text{SSE}=\\sum(Y_i-\\hat{Y}_i)^2\\)\n\\(\\text{SSE}_c = \\sum(Y_i-0)^2\\)\n\n\nCode\nsum((data$fps - 0)^2)\n\n\n[1] 233368.3\n\n\n\\(\\text{SSE}_a = \\sum(Y_i- 32.2 )^2\\)\n\n\nCode\nsum((data$fps - broom::tidy(m)$estimate)^2) \n\n\n[1] 133888.3\n\n\nor\n\n\nCode\nsum(residuals(m)^2)\n\n\n[1] 133888.3\n\n\n\nCompact model: \\(\\hat{\\text{FPS}_i}=0\\)\nSSE = 233368.3\nP = 0\nAugmented model: \\(\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)\\)\nSSE = 133888.3\nP = 1\n\\(F(P_a-P_c, N-P_a) = \\frac{(\\text{SSE}_c-\\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\n\\(F(1-0, 96 -1)= \\frac{(233368.3 - 133888.3)/(1-0)}{133888.3 / (96 - 1)}\\)\n\\(F(1, 95 )= 70.59, p &lt; .0001\\)\n\n\nCode\npf(70.59, 1, 95, lower.tail = FALSE)\n\n\n[1] 0.0000000000004255967",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#sampling-distribution-vs.-model-comparison",
    "href": "03_single_mean.html#sampling-distribution-vs.-model-comparison",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.20 Sampling Distribution vs. Model Comparison",
    "text": "3.20 Sampling Distribution vs. Model Comparison\nThe two approaches to testing \\(H_0\\) about parameters (\\(\\beta_0, \\beta_j\\)) are statistically equivalent.\nThey are complementary approaches with respect to conceptual understanding of GLMs.\nSampling Distribution\n\nFocus on population parameters and their estimates.\n\nTight connection to sampling and probability distributions.\n\nUnderstanding of SE (sampling error/power; confidence intervals, graphic displays).\n\nModel Comparison\n\nFocus on models themselves.\n\nHighlights model fit (SSE) and model parsimony (P).\n\nClearer link to PRE (\\(\\eta_p^2\\)).\nTest comparisons that differ by &gt;1 parameter (discouraged).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#effect-sizes",
    "href": "03_single_mean.html#effect-sizes",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.21 Effect Sizes",
    "text": "3.21 Effect Sizes\nYour parameter estimates are descriptive. They describe effects in the original units of the (IVs) and DV. Report them in your paper.\nThere are many other effect size estimates available. You will learn two that we prefer.\nPartial eta squared (\\(\\eta_p^2\\)): Judd et al call this PRE (proportional reduction in error).\nEta squared (\\(\\eta^2\\)): This is also commonly referred to as \\(\\Delta R^2\\) in regression.\n\nCompact model: \\(\\hat{\\text{FPS}_i}=0\\)\nSSE = 233368.3\nP = 0\nAugmented model: \\(\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)\\)\nSSE = 133888.3\nP = 1\nHow much was the error reduced in the augmented model relative to the compact model?\n\\(\\frac{\\text{SSE}_c-\\text{SSE}_a}{\\text{SSE}_c} = \\frac{233368.3 - 133888.3}{233368.3} = 0.426\\)\nOur more complex model that includes \\(\\beta_0\\) reduces prediction error (SSE) by approximately 43%. Not bad!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#confidence-interval-for-b_0",
    "href": "03_single_mean.html#confidence-interval-for-b_0",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.22 Confidence Interval for \\(b_0\\)",
    "text": "3.22 Confidence Interval for \\(b_0\\)\nA confidence interval (CI) is an interval for a parameter estimate in which you can be fairly confident that you will capture the true population parameter (in this case, \\(\\beta_0\\)).Most commonly reported is the 95% CI. Across repeated samples, 95% of the calculated CIs will include the population parameter.\n\n\nCode\n1confint(m)\n\n\n\n1\n\nUse the confint() function to calculate confidence intervals. The default is to provide 95% CIs, but you can change this using the level parameter if you wish.\n\n\n\n\n               2.5 %   97.5 %\n(Intercept) 24.58426 39.79742\n\n\nQuestion: Given what you now know about confidence intervals and sampling distributions, what should the formula be?\n\n\\(\\text{CI}(b_0)= b_0 \\pm t(\\alpha;N-P) * \\text{SE}_{b_0}\\)\nFor the 95% confidence interval this is approximately 2 SEs around our unbiased estimate of \\(\\beta_0\\).\n\nQuestion: How can we tell if a parameter is significant from the confidence interval?\n\nIf a parameter estimate \\(\\neq\\) 0 at \\(\\alpha\\) = .05, then the 95% confidence interval for its parameter estimate should not include 0.\nThis is also true for testing whether the parameter estimate is equal to any other non-zero value for the population parameter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#the-one-parameter-mean-only-model-special-case",
    "href": "03_single_mean.html#the-one-parameter-mean-only-model-special-case",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.23 The one parameter (mean-only) model: Special Case",
    "text": "3.23 The one parameter (mean-only) model: Special Case\nQuestion: What special case (specific analytic test) is statistically equivalent to the test of the null hypothesis: \\(\\beta_0\\) = 0 in the one parameter model?\n\nThe one sample t-test testing if a population mean = 0.\n\n\nCode\nt.test(data$fps)\n\n\n\n    One Sample t-test\n\ndata:  data$fps\nt = 8.4015, df = 95, p-value = 0.0000000000004261\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 24.58426 39.79742\nsample estimates:\nmean of x \n 32.19084",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#testing-beta_0-non-zero-values",
    "href": "03_single_mean.html#testing-beta_0-non-zero-values",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.24 Testing \\(\\beta_0\\) = non-zero values",
    "text": "3.24 Testing \\(\\beta_0\\) = non-zero values\nQuestion: How could you test an \\(H_0\\) regarding \\(B_0\\) = some value other than 0 (e.g., 10)? HINT: There are at least three methods.\n\nOption 1: Compare SSE for the augmented model (\\(\\hat{Y}_i= \\beta_0\\)) to SSE from a different compact model for this new \\(H_0\\) (\\(\\hat{Y}_i= 10\\)).\nOption 2: Recalculate t-statistic using this new \\(H_0\\).\n\\(t= \\frac{b_0 - 10}{\\text{SE}_{b_0}}\\)\nOption 3: Does the confidence interval for the parameter estimate contain this other value? No p-value provided.\n\n\nCode\nconfint(m)\n\n\n               2.5 %   97.5 %\n(Intercept) 24.58426 39.79742",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "03_single_mean.html#intermission",
    "href": "03_single_mean.html#intermission",
    "title": "3  Inferences About a Single Mean (1 Parameter Models)",
    "section": "3.25 Intermission…",
    "text": "3.25 Intermission…\nOne parameter (\\(\\beta_0\\)) mean-only model\n\nDescription: \\(b_0\\) describes mean of \\(Y\\).\nPrediction: \\(b_0\\) is predicted value that minimizes sample SSE.\nInference: Use \\(b_0\\) to test id \\(\\beta_0 = 0\\) (default) or any other value. One sample t-test.\n\nTwo parameter (\\(\\beta_0, \\beta_1\\)) model\n\nDescription: \\(b_1\\) describes how \\(Y\\) changes as a function of \\(X_1\\). \\(b_0\\) describes expected value of \\(Y\\) ar specific value (0) for \\(X_1\\).\nPrediction: \\(b_0\\) and \\(b_1\\) yield predicted values that vary by \\(X_1\\) and minimize SSE in sample.\nInference: Test if \\(\\beta_1 = 0\\). Pearson’s \\(r\\); independent sample t-test. Test if \\(\\beta_0=0\\). Analogous to one-sample t-test controlling for \\(X_1\\), if \\(X_1\\) is mean-centered. Very flexible!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferences About a Single Mean (1 Parameter Models)</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html",
    "href": "04_single_quantitative.html",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "",
    "text": "4.1 Two Parameter (One Predictor) Models\nWe started with a very simple model of FPS: \\(\\hat{\\text{FPS}}= \\beta_0\\)\nWhat if some participants were drunk and we knew their blood alcohol concentrations (BAC)?\nQuestion: Would it help? What would the model look like? What question(s) does this model allow us to test?\nDATA = MODEL + ERROR\n\\(Y_i= \\beta_0+\\beta_1*X_1+\\varepsilon_i\\)\n\\(\\hat{Y}_i=\\beta_0+\\beta_1*X_1\\)\n\\(\\varepsilon_i = Y_i - \\hat{Y}_i\\)\n\\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_1\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#the-two-parameter-model",
    "href": "04_single_quantitative.html#the-two-parameter-model",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.2 The Two Parameter Model",
    "text": "4.2 The Two Parameter Model\n\\(\\hat{Y}_i=\\beta_0+\\beta_1*X_1\\)\nAs before, the population parameters in the model (\\(\\beta_0, \\beta_1\\)) are estimated by \\(b_0\\) & \\(b_1\\) calculated from sample data based on the least squares criterion such that they minimize SSE in the sample data.\nSample model:\n\\(\\hat{Y}_i=b_0+b_1*X_1\\)\nTo derive these parameter estimates you must solve a series of simultaneous equations using linear algebra and matrices (see supplemental reading).\nOr use R!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#least-squares-criterion",
    "href": "04_single_quantitative.html#least-squares-criterion",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.3 Least Squares Criterion",
    "text": "4.3 Least Squares Criterion\n\\(e_i = Y_i- \\hat{Y}_i\\)\n\\(\\text{SSE} = \\sum e_i^2\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#interpretation-of-b_0-in-two-parameter-model",
    "href": "04_single_quantitative.html#interpretation-of-b_0-in-two-parameter-model",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.4 Interpretation of \\(b_0\\) in Two Parameter Model",
    "text": "4.4 Interpretation of \\(b_0\\) in Two Parameter Model\n\\(\\hat{Y}_i=b_0+b_1*X_1\\)\n\\(b_0\\) is predicted value for \\(Y\\) when \\(X_1\\) = 0. Graphically, this is the \\(Y\\) intercept for the regression line (value of \\(Y\\) where regression line crosses Y-axis at \\(X_1\\) = 0).\nQuestion: Approximately what is \\(b_0\\) in this example?\n\n\n\n\n\n\n\n\n\n\n42.5\nIMPORTANT: Notice that \\(b_0\\) is very different in the two parameter model (42.5) than in the previous one parameter model (32.2).\n\n\n\n\n\n\n\n\n\nQuestion: Why?\n\nIn the one parameter model \\(b_0\\) was our sample estimate of the mean FPS score in everyone. \\(b_0\\) in the two parameter model is our sample estimate of the mean FPS score for people with BAC = 0, not everyone.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#interpretation-of-b_1-in-two-parameter-model",
    "href": "04_single_quantitative.html#interpretation-of-b_1-in-two-parameter-model",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.5 Interpretation of \\(b_1\\) in Two Parameter Model",
    "text": "4.5 Interpretation of \\(b_1\\) in Two Parameter Model\n\\(\\hat{Y}_i=b_0+b_1*X_1\\)\n\\(b_1\\) is the predicted change in \\(Y\\) for every one unit change in \\(X_1\\). Graphically it is represented by the slope of the regression line. If you understand the units of your predictor and DV, this is an attractive description of their relationship.\n\\(\\hat{\\text{FPS}}_i=42.5+ -184.1*\\text{BAC}_i\\)\nFor every 1% increase in BAC, FPS decreases by 184.1 microvolts.\nFor every .01% increase in BAC, FPS decreases by 1.841 microvolts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#testing-inferences-about-beta_1",
    "href": "04_single_quantitative.html#testing-inferences-about-beta_1",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.6 Testing Inferences about \\(\\beta_1\\)",
    "text": "4.6 Testing Inferences about \\(\\beta_1\\)\nDoes alcohol affect people’s anxiety?\n\\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i\\)\nQuestion: What are your null and alternative hypotheses about a model parameter to evaluate this question?\n\n\\(H_0: \\beta_1=0\\)\n\\(H_a: \\beta_1 \\neq 0\\)\nIf \\(\\beta_1 = 0\\), this means that FPS does not change with changes in BAC. In other words, there is no effect of BAC on FPS. If \\(\\beta_1 &lt; 0\\), this means that FPS decreases with increasing BAC (people are less anxious when drunk).\nIf \\(\\beta_1 &gt; 0\\), this means FPS increases with increasing BAC (people are more anxious when drunk).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#estimating-a-two-parameter-model-in-r",
    "href": "04_single_quantitative.html#estimating-a-two-parameter-model-in-r",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.7 Estimating a Two Parameter Model in R",
    "text": "4.7 Estimating a Two Parameter Model in R\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\n\ntheme_set(theme_classic()) \n\npath_data &lt;- \"data_lecture\" \n\ndata &lt;- read_csv(here::here(path_data, \"4_single_quantitative_bac_fps.csv\"),\n                 show_col_types = FALSE)\n\n\n\n\n\nCode\ndata |&gt; \n  skimr::skim_without_charts()\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n96\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubid\n0\n1\n4\n4\n0\n96\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nbac\n0\n1\n0.06\n0.04\n0.0\n0.02\n0.06\n0.08\n0.14\n\n\nfps\n0\n1\n32.19\n37.54\n-98.1\n6.79\n19.46\n50.46\n162.74\n\n\n\n\n\n\n\n\nCode\nm_2 &lt;- lm(fps ~ bac, data = data)\n\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nQuestion: Does BAC affect FPS? Explain this conclusion in terms of the parameter estimate, \\(b_1\\) and its standard error.\n\nUnder the \\(H_0: \\beta_1 = 0\\), the sampling distribution for \\(\\beta_1\\) will have a mean of 0 with an estimated standard deviation 95.89.\n\\(t (96 - 1) = \\frac{-184.09 - 0}{95.89 } = -1.92\\)\nOur value of the parameter estimate, \\(b_1\\), is 1.92 standard deviations below the expected mean of the sampling distribution for \\(H_0\\).\n\n\nCode\npt(-1.92, 94, lower.tail = TRUE)*2\n\n\n[1] 0.05788984\n\n\nA \\(b_1\\) of this size is not unlikely under the null, therefore you fail to reject the null and conclude that BAC has no effect on FPS.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#testing-inferences-about-beta_1-1",
    "href": "04_single_quantitative.html#testing-inferences-about-beta_1-1",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.8 Testing Inferences about \\(\\beta_1\\)",
    "text": "4.8 Testing Inferences about \\(\\beta_1\\)\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nOne tailed p-value:\n\n\nCode\npt(-1.92, 94, lower.tail = TRUE)\n\n\n[1] 0.02894492\n\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_1: \\beta_1 &lt; 0\\)\n\nTwo tailed p-value:\n\n\nCode\npt(-1.92, 94, lower.tail = TRUE)*2\n\n\n[1] 0.05788984\n\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_1: \\beta_1 \\neq 0\\)\n\n\nCode\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual)) |&gt; \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  geom_vline(xintercept = -subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  labs(title = \"Sampling Distribution for b1\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#model-comparison-testing-inferences-about-beta_1",
    "href": "04_single_quantitative.html#model-comparison-testing-inferences-about-beta_1",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.9 Model Comparison: Testing Inferences about \\(\\beta_1\\)",
    "text": "4.9 Model Comparison: Testing Inferences about \\(\\beta_1\\)\n\\(H_0: \\beta_1 = 0\\)\n\\(H_1: \\beta_1 \\neq 0\\)\nQuestion: What two models are you comparing when you test hypotheses about \\(\\beta_1\\)? Describe the logic.\n\nCompact model: \\(\\hat{\\text{FPS}}_i = \\beta_0+0*\\text{BAC}_i\\)\n\\(P_c=1\\)\n\\(\\text{SSE}_c=133888.3\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i = \\beta_0+\\beta_1*\\text{BAC}_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a=128837.1\\)\n\n\\(F(P_a-P_c, N-P_a) = \\frac{\\text{SSE}_c-\\text{SSE}_a/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\nF(1,94) = 3.685, p = 0.058\n\n\n\nCode\nm_1 &lt;- lm(fps ~ 1, data = data)   \nm_2 &lt;-  lm(fps ~ bac, data = data) \n\n\n\n\nCode\nanova(m_1, m_2)\n\n\nAnalysis of Variance Table\n\nModel 1: fps ~ 1\nModel 2: fps ~ bac\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     95 133888                              \n2     94 128837  1    5051.2 3.6854 0.05792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#sum-of-squared-errors",
    "href": "04_single_quantitative.html#sum-of-squared-errors",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.10 Sum of Squared Errors",
    "text": "4.10 Sum of Squared Errors\nQuestion: If there is a perfect relationship between \\(X_1\\) and \\(Y\\) in your sample, what will the SSE be in the two parameter model (augmented) and why?\n\n\\(\\text{SSE}_a=0\\). All data points will fall perfectly on the regression line. All errors will be 0.\n\nQuestion: If there is no relationship at all between \\(X_1\\) and \\(Y\\) in your sample (\\(b_1\\) = 0), what will the SSE be in the two parameter model (augmented) and why?\n\n\\(\\text{SSE}_a=\\text{SSE}\\) of the mean-only model. \\(X_1\\) provides no additional information about the DV. Your best prediction will still be the mean of the DV.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#testing-inferences-about-beta_0",
    "href": "04_single_quantitative.html#testing-inferences-about-beta_0",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.11 Testing Inferences about \\(\\beta_0\\)",
    "text": "4.11 Testing Inferences about \\(\\beta_0\\)\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nQuestion: What is the interpretation of \\(b_0\\) in this two parameter model?\n\nIt is the predicted FPS for a person with BAC = 0 (sober).\nThe test of this parameter estimate could inform us if the shock procedure worked among our sober participants. This is probably a more appropriate manipulation check than testing if it worked in everyone including drunk people given that alcohol could have reduced FPS.\n\nQuestion: What two models are being compared?\n\nCompact model: \\(\\hat{\\text{FPS}}_i= 0 + \\beta_1* \\text{BAC}_i\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i= \\beta_0 + \\beta_1* \\text{BAC}_i\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#mean-centering-predictor-variables",
    "href": "04_single_quantitative.html#mean-centering-predictor-variables",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.12 Mean Centering Predictor Variables",
    "text": "4.12 Mean Centering Predictor Variables\nIn this example, I have been using raw BAC. In many instances, we will mean center our quantitative predictor variables.\nMean centering simply involves subtracting the mean of the predictor variable from all scores for that predictor variable.\n\n\n\nCode\ndata &lt;- data |&gt; \n1  mutate(bac_c = bac - mean(bac))\n\n\n\n1\n\nWe use mutate() to update and create new variables (e.g., centered quantitative predictors, changing character variables to factors).\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  select(starts_with(\"bac\")) |&gt; \n1  pivot_longer(everything(), names_to = \"var\") |&gt;\n  group_by(var) |&gt; \n  summarize(mean = mean(value), \n            sd = sd(value), \n            min = min(value), \n            max = max(value)) |&gt; \n2  mutate(across(mean:max, ~round(.x, 2)))\n\n\n\n1\n\npivot_longer() and pivot_wider() are two tidy functions for transforming your data frame (i.e., wide to long data frame and long to wide data frame).\n\n\n2\n\nHere we use mutate() again, but are now combing it with across() so that we can apply our transformation (rounding to 2 decimal places) to several variables in one statement.\n\n\n\n\n# A tibble: 2 × 5\n  var    mean    sd   min   max\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 bac    0.06  0.04  0     0.14\n2 bac_c  0     0.04 -0.06  0.08\n\n\n\nQuestion: How would the parameter estimate values and interpretations change if I mean centered BAC?\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\n\n\n\nCode\nm_2_c &lt;- lm(fps ~ bac_c, data = data)  \n\nm_2_c |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     32.2      3.78      8.52 2.57e-13\n2 bac_c         -184.      95.9      -1.92 5.79e- 2\n\n\nThe value and interpretation of \\(b_0\\) will change because it is the predicted FPS score at 0 on \\(X\\). \\(b_0\\) is now the predicted value for someone with the mean BAC in the sample.\nNo change to interpretation of \\(b_1\\). Why? Think about it…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#raw-vs.-centered-bac",
    "href": "04_single_quantitative.html#raw-vs.-centered-bac",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.13 Raw vs. Centered BAC",
    "text": "4.13 Raw vs. Centered BAC\nQuestion: How would the graph look different? Where is \\(b_0\\) and \\(b_1\\) on the new figure? Would you center BAC in this example?\n\n\n\nCode\nplot_x &lt;- data |&gt; \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_2)[1],\n                  slope = coef(m_2)[2]),\n              color = \"red\", linewidth = 1) +\n  xlim(0, .15)\n\n\nplot_x_c &lt;- data |&gt; \n  ggplot(aes(x = bac_c, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_2)[1],\n                  slope = coef(m_2)[2]),\n              color = \"red\", linewidth = 1)\n\n\n\n\n\nCode\nplot_x + plot_x_c",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#confidence-interval-for-b_j-or-b_0",
    "href": "04_single_quantitative.html#confidence-interval-for-b_j-or-b_0",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.14 Confidence Interval for \\(b_j\\) or \\(b_0\\)",
    "text": "4.14 Confidence Interval for \\(b_j\\) or \\(b_0\\)\nYou can provide confidence intervals for each parameter estimate in your model.\n\n\nCode\nconfint(m_2)\n\n\n                 2.5 %    97.5 %\n(Intercept)   29.45597 55.457721\nbac         -374.49261  6.308724\n\n\nThe underlying logic from your understanding of sampling distributions remains the same.\n\\(\\text{CI}_b = b\\pm t(\\alpha; N-P)*\\text{SE}_b\\) where \\(P\\) = total # of parameters.\nQuestion: How can we tell if a parameter is significant from the confidence interval?\n\nIf a parameter \\(\\neq 0\\), at \\(\\alpha\\) = .05, then the 95% confidence interval should not include 0. True for any other non-zero value for \\(b\\) as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#partial-eta-squared-eta_p2-or-pre-for-beta_1",
    "href": "04_single_quantitative.html#partial-eta-squared-eta_p2-or-pre-for-beta_1",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.15 Partial Eta Squared (\\(\\eta_p^2\\)) or PRE for \\(\\beta_1\\)",
    "text": "4.15 Partial Eta Squared (\\(\\eta_p^2\\)) or PRE for \\(\\beta_1\\)\nQuestion: How can you calculate the effect size estimate \\(\\eta_p^2\\) (PRE) for \\(\\beta_1\\)? \n\nCompare the SSE across the two relevant models.\nCompact model: \\(\\hat{\\text{FPS}}_i= \\beta_0 + 0* \\text{BAC}_i\\)\n\\(\\text{SSE}_c =  133888.3\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i= \\beta_0 + \\beta_1* \\text{BAC}_i\\)\n\\(\\text{SSE}_a =  128837.1\\)\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}=\\frac{133888.3- 128837.1}{133888.3}= 0.038\\)\n\n\nCode\n(sum(residuals(m_1)^2)- sum(residuals(m_2)^2))/sum(residuals(m_1)^2) \n\n\n[1] 0.03772707\n\n\nOur augmented model that includes a non-zero effect for BAC reduces prediction error (SSE) by only 3.8% over the compact model that fixes this parameter at 0.\n\nQuestion: How can you calculate the effect size estimate \\(\\eta_p^2\\) (PRE) for \\(\\beta_0\\)? \n\nCompare the SSE across the two relevant models.\n\n\nCode\n1m_2_0 &lt;- lm(fps ~ bac - 1, data = data)\n\n\n\n1\n\nWe can use -1 to remove the intercept (i.e., set it equal to 0) from our new compact model.\n\n\n\n\nCompact model: \\(\\hat{\\text{FPS}}_i= 0 + \\beta_1* \\text{BAC}_i\\)\n\\(\\text{SSE}_c =  186462.4\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i= \\beta_0 + \\beta_1* \\text{BAC}_i\\)\n\\(\\text{SSE}_a =  128837.1\\)\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}=\\frac{186462.4- 128837.1}{186462.4}= 0.309\\)\n\n\nCode\n(sum(residuals(m_2_0)^2)- sum(residuals(m_2)^2))/sum(residuals(m_2_0)^2) \n\n\n[1] 0.3090451\n\n\nOur augmented model that allows FPS to be non-zero for people with BAC=0 (sober people) reduces prediction error (SSE) by 30.9% from the model that fixes FPS at 0 when BAC=0!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#coefficient-of-determination-r2",
    "href": "04_single_quantitative.html#coefficient-of-determination-r2",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.16 Coefficient of Determination (\\(R^2\\))",
    "text": "4.16 Coefficient of Determination (\\(R^2\\))\nCoefficient of Determination (\\(R^2\\)):\nProportion of explained variance (i.e., proportion of variance in \\(Y\\) accounted for by all \\(Xs\\) in model).\nDATA = MODEL + ERROR\nFor individuals:\n\\(Y_i=Y_i+e_i\\)\nWith respect to variance:\n\\(S_{Y_i}^2=S_{\\hat{Y}_i}^2+S_{e_i}^2\\)\n\\(R^2=\\frac{S_{\\hat{Y}_i}^2}{S_{Y_i}^2}\\)\n\nWe can calculate \\(R^2\\) manually by computing the ratio of the variance of the predicted values to the variance of the actual values.\n\n\nCode\nvar(predict(m_2))/ var(data$fps)\n\n\n[1] 0.03772707\n\n\n\nWe can also use the glance() function in the broom package to return various model fit indices.\n\n\nCode\nbroom::glance(m_2) |&gt;\n  glimpse()\n\n\nRows: 1\nColumns: 12\n$ r.squared     &lt;dbl&gt; 0.03772707\n$ adj.r.squared &lt;dbl&gt; 0.02749012\n$ sigma         &lt;dbl&gt; 37.02171\n$ statistic     &lt;dbl&gt; 3.685383\n$ p.value       &lt;dbl&gt; 0.05792374\n$ df            &lt;dbl&gt; 1\n$ logLik        &lt;dbl&gt; -481.912\n$ AIC           &lt;dbl&gt; 969.8239\n$ BIC           &lt;dbl&gt; 977.517\n$ deviance      &lt;dbl&gt; 128837.1\n$ df.residual   &lt;int&gt; 94\n$ nobs          &lt;int&gt; 96\n\n\nSince we are only interested in \\(R^2\\) we can use the following code to pull out this single value.\n\n\nCode\nbroom::glance(m_2)$r.squared\n\n\n[1] 0.03772707",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#r2-and-the-mean-only-model",
    "href": "04_single_quantitative.html#r2-and-the-mean-only-model",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.17 \\(R^2\\) and the Mean-Only Model",
    "text": "4.17 \\(R^2\\) and the Mean-Only Model\nQuestion: Why did the mean-only model not have an \\(R^2\\)? \n\nIt did but it was just 0. It explained no variance in \\(Y_i\\) because it predicted the same value (mean) for every person. The variance of the predicted values is 0 in the mean-only model.\n\\(R^2=\\frac{S_{\\hat{Y}_i}^2}{S_{Y_i}^2}\\)\n\n\nCode\nm_1 &lt;- lm(fps ~ 1, data = data)  \n\nbroom::glance(m_1)$r.squared\n\n\n[1] 0\n\n\nIn fact, the SSE for the mean-only model is the numerator of the formula for the variance for \\(Y_i\\).\n\\(\\text{SSE}=\\frac{\\sum(Y_i-\\hat{Y_i})^2}{}\\)\n\\(S^2 = \\frac{\\sum(Y_i-\\overline{Y})^2}{N-1}\\)\n\nThis leads to an alternative formula for \\(R^2\\) for an augmented model.\n\\(R^2=\\frac{\\text{SSE}_\\text{mean-only}-\\text{SSE}_a}{\\text{SSE}_\\text{mean-only}}\\)\nMean-only model: \\(\\hat{\\text{FPS}}_i=\\beta_0\\)\n\\(\\text{SSE}_{\\text{mean-only}} = 133888.3\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i=\\beta_0 + \\beta_1*\\text{BAC}_i\\)\n\\(\\text{SSE}_a = 128837.1\\)\n\\(R^2= \\frac{133888.3 - 128837.1}{133888.3} = 0.03773\\)\nIn this augmented model, \\(R^2\\) is fully accounted for by BAC. In more complex models, \\(R^2\\) will be the aggregate of multiple predictors. \\(R^2\\) is only defined for models that include \\(b_0\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#test-of-beta_1-in-two-parameter-model-special-case",
    "href": "04_single_quantitative.html#test-of-beta_1-in-two-parameter-model-special-case",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.18 Test of \\(\\beta_1\\) in Two Parameter Model: Special Case",
    "text": "4.18 Test of \\(\\beta_1\\) in Two Parameter Model: Special Case\nQuestion: When both the predictor variable and the dependent variable are quantitative, the test of \\(\\beta_1\\) = 0 is statistically equivalent to the what other common statistical test? \n\nThe test of the Pearson’s correlation coefficient, r.\n\n\nCode\npsych::corr.test(data$bac, data$fps)\n\n\nCall:psych::corr.test(x = data$bac, y = data$fps)\nCorrelation matrix \n[1] -0.19\nSample Size \n[1] 96\nThese are the unadjusted probability values.\n  The probability values  adjusted for multiple tests are in the p.adj object. \n[1] 0.06\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nFurthermore, \\(r^2=R^2\\) for this model only.\n\\(-0.194^2 = 0.038\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#visualizing-the-model",
    "href": "04_single_quantitative.html#visualizing-the-model",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.19 Visualizing the Model",
    "text": "4.19 Visualizing the Model\n\n\nCode\nx &lt;- tibble(bac = seq(from = min(data$bac), to = max(data$bac), by = .001))\n\n1preds &lt;- predict(m_2, x, interval = \"confidence\", level = .95) |&gt;\n  as_tibble() \n\n\n\n1\n\nWe can use our model to generate predictions. We use the interval = \"confidence\" and level = .95 arguments to return the upper and lower values of the 95% confidence interval.\n\n\n\n\n\n\n\nCode\npreds\n\n\n# A tibble: 140 × 3\n     fit   lwr   upr\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  42.5  29.5  55.5\n 2  42.3  29.4  55.1\n 3  42.1  29.4  54.8\n 4  41.9  29.4  54.4\n 5  41.7  29.3  54.1\n 6  41.5  29.3  53.8\n 7  41.4  29.3  53.4\n 8  41.2  29.2  53.1\n 9  41.0  29.2  52.8\n10  40.8  29.2  52.4\n# ℹ 130 more rows\n\n\n\n\n\nCode\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = x$bac, y = preds$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = x$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +\n  labs(x = \"BAC\",\n       y = \"FPS\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#error-band-for-haty_i",
    "href": "04_single_quantitative.html#error-band-for-haty_i",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.20 Error Band for \\(\\hat{Y}_i\\)",
    "text": "4.20 Error Band for \\(\\hat{Y}_i\\)\nYou are predicting the mean \\(Y\\) for any \\(X\\). There is a sampling distribution around this mean. The true population mean \\(Y\\) for any \\(X\\) is uncertain. You can display this uncertainty by displaying information about the sampling distribution at any/every \\(X\\). This is equivalent to error bars in ANOVA.\nThe effect() function calculates 95% CI for \\(\\hat{Y}_i\\). However, I prefer \\(\\pm\\) 1 SE for publications.\n\\(\\text{SEE} = \\sqrt{\\frac{\\text{SSE}}{N-P}}\\)\n\\(\\text{SE}_\\hat{Y_i}=\\text{SEE}\\sqrt{\\frac{1}{N}+\\frac{(X_i-\\overline{X})^2}{(N-1)s_x^2}}\\)\n\\(\\text{CI}_\\hat{Y_i}=\\hat{Y_i}\\pm t(\\alpha; N-k-1)\\text{SE}_\\hat{Y_i}\\)\n\nQuestion: Why are the error bands not linear? \n\n\nCode\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = x$bac, y = preds$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = x$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +\n  labs(x = \"BAC\",\n       y = \"FPS\") \n\n\n\n\n\n\n\n\n\n\nModel predictions are better (less error) near the center of your data (\\(X_i\\)). The regression line will always go through mean of \\(X\\) and \\(Y\\). Small changes in \\(b_1\\) across samples will produce bigger variation in \\(\\hat{Y}_i\\) at the edge of the model (far from the mean \\(X\\)).\n\\(\\hat{\\text{FPS}_i}=42.5 + -184.1 * \\text{BAC}_i\\)\n\\(\\text{SE}_\\hat{Y_i}=\\sqrt{\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{(X_i-\\overline{X})^2}{(N-1)s_x^2}}\\)\n\n\nCode\nconfint(m_2)\n\n\n                 2.5 %    97.5 %\n(Intercept)   29.45597 55.457721\nbac         -374.49261  6.308724\n\n\n\nCompare to the SE for \\(b_0\\). \n\n\\(\\text{SE}_\\hat{Y_i}=\\sqrt{\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{(X_i-\\overline{X})^2}{(N-1)s_x^2}}\\)\n\\(\\text{SE}_{b_0}=\\sqrt{\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{(\\overline{X})^2}{(N-1)s_x^2}}\\)\n\\(b_0\\) is simply the predicted value for \\(Y\\) when \\(X\\) = 0.\nWe can use additive transformations of \\(X\\) to make tests of the predicted value at \\(X\\) = 0. Most common in repeated measures designs but used elsewhere as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#publication-quality-figure",
    "href": "04_single_quantitative.html#publication-quality-figure",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.21 Publication Quality Figure",
    "text": "4.21 Publication Quality Figure\n\n\nCode\n1preds_pub &lt;- predict(m_2, x, se.fit = TRUE) |&gt;\n  as_tibble() |&gt; \n2  mutate(upper = fit + se.fit,\n         lower= fit - se.fit)\n\n\n\n1\n\nWe use the se.fit = TRUE argument to return the standard error for each prediction.\n\n2\n\nWe use mutate() to calculate \\(\\pm\\) 1 standard error values for each prediction.\n\n\n\n\n\n\n\nCode\npreds_pub\n\n\n# A tibble: 140 × 6\n     fit se.fit    df residual.scale upper lower\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  42.5   6.55    94           37.0  49.0  35.9\n 2  42.3   6.47    94           37.0  48.7  35.8\n 3  42.1   6.39    94           37.0  48.5  35.7\n 4  41.9   6.32    94           37.0  48.2  35.6\n 5  41.7   6.24    94           37.0  48.0  35.5\n 6  41.5   6.16    94           37.0  47.7  35.4\n 7  41.4   6.09    94           37.0  47.4  35.3\n 8  41.2   6.01    94           37.0  47.2  35.2\n 9  41.0   5.94    94           37.0  46.9  35.0\n10  40.8   5.86    94           37.0  46.7  34.9\n# ℹ 130 more rows\n\n\n\n\nCode\nplot_pub &lt;- ggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = x$bac, y = preds_pub$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = x$bac, ymin = preds_pub$lower, ymax = preds_pub$upper), alpha = 0.2) +\n  labs(x = \"Blood alcohol concentration\",\n       y = \"Fear-potentiated startle\") \n\n\n\n\n\nCode\nplot_pub",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "04_single_quantitative.html#review-of-concepts",
    "href": "04_single_quantitative.html#review-of-concepts",
    "title": "4  Inferences About a Single Quantitative Predictor",
    "section": "4.22 Review of Concepts",
    "text": "4.22 Review of Concepts\n\n\\(b_0, b_1\\)\nNHSTs\nEffect sizes\nCIs\nConclusions\n\n\n\n\nCode\nm_2 &lt;- lm(fps ~ bac + 1, data = data)\n\n\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_2)[1],\n                  slope = coef(m_2)[2]),\n              color = \"red\", linewidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nm_2_0 &lt;- lm(fps ~ bac - 1, data = data)\n\n\n\n\nCode\nm_2_0 |&gt; \n  broom::tidy()\n\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic    p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 bac       324.      66.2      4.89 0.00000412\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = 0,\n                  slope = coef(m_2_0)),\n              color = \"blue\", linewidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nm_1 &lt;- lm(fps ~ 1, data = data)\n\n\n\n\nCode\nm_1 |&gt; \n  broom::tidy()\n\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     32.2      3.83      8.40 4.26e-13\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_1),\n                  slope = 0),\n              color = \"blue\", linewidth = 1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inferences About a Single Quantitative Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html",
    "href": "05_single_dichotomous.html",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "",
    "text": "5.1 Categorical Variables in GLM",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#categorical-variables-in-glm",
    "href": "05_single_dichotomous.html#categorical-variables-in-glm",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "",
    "text": "One dichotomous predictor coded with one regressor (this unit).\nLearn models with multiple regressors in later units:\n\n\nOne-way between subject ANOVA (One predictor, multiple regressors)\nMultiple predictors and regressors\n\nAdditive: ANCOVA, General Additive Models\nInteractive: Factorial ANOVA, Attitude Treatment Interactions, General Interactive Models\n\n\n\nRepeated measures & mixed model designs",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#dichotomous-predictor",
    "href": "05_single_dichotomous.html#dichotomous-predictor",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.2 Dichotomous Predictor",
    "text": "5.2 Dichotomous Predictor\nQuestion: Consider a regression model with one dichotomous predictor and the typical continuous DV. What other statistical test is this regression equivalent to?\n\nAn independent samples (between subjects) t-test comparing the two groups that are represented by the two levels of the predictor on the mean of the DV.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#coding-dichotomous-predictors",
    "href": "05_single_dichotomous.html#coding-dichotomous-predictors",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.3 Coding Dichotomous Predictors",
    "text": "5.3 Coding Dichotomous Predictors\nQuestion: How do we handle analysis of dichotomous predictors in regression?\n\nVery simple! We will assign two “arbitrary” values to a regressor \\(X\\) to represent the two levels (groups) in the dichotomous predictor variable.\nThere are 2 coding schemes that we will need to learn:\n\nDummy coding (reference/control group)\nContrast coding (for now, unit weighted, centered coefficients).\n\nWe also need to learn distinction between weighted vs. unweighted means (optional).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#dummy-coding",
    "href": "05_single_dichotomous.html#dummy-coding",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.4 Dummy Coding",
    "text": "5.4 Dummy Coding\nDummy coding with a dichotomous variable:.\nDummy coding involves assigning 1’s and 0’s to the regressor in a specific pattern. For the two level simple case, assign a 0 to the control/reference group and a 1 to the target or experimental group. (Doesn’t really matter which is which; but what would change? What about -1 vs. 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#our-running-example",
    "href": "05_single_dichotomous.html#our-running-example",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.5 Our Running Example",
    "text": "5.5 Our Running Example\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\n\ntheme_set(theme_classic()) \n\npath_data &lt;- \"data_lecture\" \n\ndata &lt;- read_csv(here::here(path_data, \"5_single_dichotomous_bg_fps.csv\"),\n                 show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 96\nColumns: 4\n$ subid &lt;chr&gt; \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bg_2  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ sex   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"fem…\n$ fps   &lt;dbl&gt; 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n\n\n\n\n\nCode\ndata |&gt; \n  pivot_longer(where(is.numeric), names_to = \"var\") |&gt; \n  group_by(var) |&gt; \n  summarise(n = n(),\n            mean = mean(value),\n            sd = sd(value),\n            min = min(value),\n            max = max(value))\n\n\n# A tibble: 2 × 6\n  var       n  mean     sd   min   max\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 bg_2     96  0.75  0.435   0      1 \n2 fps      96 32.2  37.5   -98.1  163.\n\n\n\n\n\nCode\ndata |&gt; \n  group_by(bg_2) |&gt; \n  count()\n\n\n# A tibble: 2 × 2\n# Groups:   bg_2 [2]\n   bg_2     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0    24\n2     1    72",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#dichotomous-beverage-group",
    "href": "05_single_dichotomous.html#dichotomous-beverage-group",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.6 Dichotomous Beverage Group",
    "text": "5.6 Dichotomous Beverage Group\n\n\nCode\nm_d &lt;- lm(fps ~ bg_2, data = data)\n\nm_d |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    37.1       7.68     4.83  0.00000524\n2 bg_2           -6.57      8.87    -0.741 0.461     \n\n\n\n\nCode\nm_q &lt;- lm(fps ~ bac, data = data_q)\n\nm_q |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nQuestion: What is different about the NHST of the alcohol effect with bg_2 vs BAC and why?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#sse-leads-to-ses-and-power",
    "href": "05_single_dichotomous.html#sse-leads-to-ses-and-power",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.7 >SSE leads to >SEs and <Power",
    "text": "5.7 &gt;SSE leads to &gt;SEs and &lt;Power\np-value is worse (bigger) for bg_2 than BAC. Taking a quantitative variable and dichotomizing (or trichotomizing, etc.) throws away potentially valuable information in your predictor variable. The dichotomous variable will generally be a worse predictor. This will produce a model with more error (&gt; SSE) and less power (&gt; p-values) to test hypotheses.\nDon’t dichotomize unless there is a REALLY good reason. GLM can accommodate quantitative variables anywhere you would have previously used a categorical variable.\n\\(\\text{SE}_{b_0}=\\sqrt{\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{(\\overline{X})^2}{(N-1)*S_x^2}}\\)\n\\(\\text{SE}_{b_1}=\\frac{s_y}{s_x}*\\sqrt{\\frac{1-R^2}{N-P}}\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#dichotomous-beverage-group-cont.",
    "href": "05_single_dichotomous.html#dichotomous-beverage-group-cont.",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.8 Dichotomous Beverage Group (cont.)",
    "text": "5.8 Dichotomous Beverage Group (cont.)\n\n\nCode\nm_d |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    37.1       7.68     4.83  0.00000524\n2 bg_2           -6.57      8.87    -0.741 0.461     \n\n\nQuestion: What is the interpretation of \\(b_1\\) (bg_2) in this model?\n\nStill change in \\(Y\\) for one unit change in \\(X\\).\nHowever, a one unit change in \\(X\\) moves from the no-alcohol group (coded 0) to alcohol group (coded 1). Therefore \\(b_1\\) is now the predicted change in \\(Y\\) between no-alcohol and alcohol groups. This is simply the difference in the mean of the two groups!\n\nQuestion: Where is \\(b_1\\) (bg_2 effect) on this graph?\n\n\nCode\nplot_d &lt;- data |&gt; \n  ggplot(aes(x = bg_2, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_d)[1],\n                  slope = coef(m_d)[2]),\n                  linewidth = 1) \n\nplot_d\n\n\n\n\n\n\n\n\n\n\nIt is the slope of the line (as before). The line goes through the mean of FPS scores for each of the two groups.\n\n\nCode\nlabels &lt;- tibble(means = c(round(mean(subset(data, bg_2 == 0)$fps), 2), \n                           round(mean(subset(data, bg_2 == 1)$fps), 2)),\n                 vars = c(\"bg_1\", \"bg_2\"))\nplot_d +\n  annotate(\"text\", x = -.15, y = labels$means[1], label = labels$means[1], \n           size = 6, color = \"blue\") +\n  annotate(\"text\", x = 1.1, y = labels$means[2], label = labels$means[2], \n           size = 6, color = \"blue\") +\n  annotate(\"text\", x = 0, y = -140, label = \"No Alcohol\", \n           size = 5, color = \"blue\") +\n   annotate(\"text\", x = 1, y = -140, label = \"Alcohol\", \n           size = 5, color = \"blue\") +\n  coord_cartesian(xlim = c(0, 1),\n                  (ylim = c(-100, 175)),\n                  clip = \"off\") +\n  theme(plot.margin = unit(c(0, .75, .5, .75), \"inches\"))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nm_d |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    37.1       7.68     4.83  0.00000524\n2 bg_2           -6.57      8.87    -0.741 0.461     \n\n\nQuestion: What is the interpretation of \\(b_0\\) in this model?\n\nStill predicted value for \\(Y\\) when \\(X\\) = 0.\nIn this context, that is the predicted FPS score for the no-alcohol group (coded 0).\n\nQuestion: Where is \\(b_0\\) on this graph?\n\n\nCode\nplot_d\n\n\n\n\n\n\n\n\n\n\nIt is the \\(Y\\) intercept as before. The value of FPS when \\(X\\) = 0.\n\n\nCode\nplot_d +\n  annotate(\"text\", x = -.15, y = labels$means[1], label = labels$means[1], \n           size = 6, color = \"blue\") +\n  annotate(\"text\", x = 0, y = -140, label = \"No Alcohol\", \n           size = 5, color = \"blue\") +\n  coord_cartesian(xlim = c(0, 1),\n                  (ylim = c(-100, 175)),\n                  clip = \"off\") +\n  theme(plot.margin = unit(c(0, 0, .5, .75), \"inches\"))\n\n\n\n\n\n\n\n\n\n\nQuestion: Does alcohol affect FPS?\n\n\n\nCode\nm_d |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    37.1       7.68     4.83  0.00000524\n2 bg_2           -6.57      8.87    -0.741 0.461     \n\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_a: \\beta_1 \\neq 0\\)\nNo. The probability of getting a \\(b_1\\) of -6.568 or more extreme is 0.461 if the null hypothesis is true. This is not sufficient evidence to reject the null.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#model-comparisons-beta_1",
    "href": "05_single_dichotomous.html#model-comparisons-beta_1",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.9 Model Comparisons (\\(\\beta_1\\))",
    "text": "5.9 Model Comparisons (\\(\\beta_1\\))\nQuestion: What two models are you comparing to test your hypotheses about alcohol’s effect on FPS?\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_a: \\beta_1 \\neq 0\\)\nCompact model: \\(\\hat{\\text{FPS}}_i = \\beta_0+0*\\text{bg}\\_2_i\\)\n\\(P_c = 1\\)\n\\(\\text{SSE}_c = 133888.28\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a = 133111.76\\)\n\nQuestion: How do you use this information to calculate \\(\\eta_p^2\\) (PRE) for the bg_2 effect?\n\nCompact model: \\(\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{bg}\\_2_i\\)\n\\(P_c = 1\\)\n\\(\\text{SSE}_c = 133888.28\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a = 133111.76\\)\nPRE or \\(\\eta_p^2\\) is the proportion reduction in error due to the effect (the parameter). Use the same models that you would use to test the hypotheses about that parameter.\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}=\\frac{133888.28 - 133111.76}{133888.28} = 0.0058\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#testing-beta_0",
    "href": "05_single_dichotomous.html#testing-beta_0",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.10 Testing \\(\\beta_0\\)",
    "text": "5.10 Testing \\(\\beta_0\\)\nQuestion: Does our task produce non-zero fear potentiated startle in sober people?\n\n\n\nCode\nm_d |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    37.1       7.68     4.83  0.00000524\n2 bg_2           -6.57      8.87    -0.741 0.461     \n\n\n\\(H_0: \\beta_0 = 0\\)\n\\(H_a: \\beta_0 \\neq 0\\)\nYes. The probability of getting a \\(b_0\\) of 37.117 or more extreme is 0.0000052 if the null hypothesis is true. Very unlikely so reject null. Conclude \\(B_0 &gt; 0\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#model-comparisons-beta_0",
    "href": "05_single_dichotomous.html#model-comparisons-beta_0",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.11 Model Comparisons (\\(\\beta_0\\))",
    "text": "5.11 Model Comparisons (\\(\\beta_0\\))\nQuestion: What two models are you comparing to test your hypotheses about mean FPS for sober people?\n\n\\(H_0: \\beta_0 = 0\\)\n\\(H_a: \\beta_0 \\neq 0\\)\nCompact model: \\(\\hat{\\text{FPS}}_i=0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_c = 1\\)\n\\(\\text{SSE}_c = 166175.76\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a = 133111.76\\)\n\nQuestion: How do you use this information to calculate \\(\\eta_p^2\\) (PRE) for \\(\\beta_0\\)?\n\nCompact model: \\(\\hat{\\text{FPS}}_i=0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_c = 1\\)\n\\(\\text{SSE}_c = 166175.76\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a = 133111.76\\)\nPRE or \\(\\eta_p^2\\) is the proportion reduction in error due to the effect (the parameter). Use the same models that you would use to test the hypotheses about that parameter.\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}= \\frac{166175.76 - 133111.76}{166175.76 } = 0.199\\)\n\nQuestion: How do you use this information to calculate \\(R^2\\) for the augmented model?\n\nMean Only model: \\(\\hat{\\text{FPS}_i}=\\beta_0\\)\n\\(P_c = 1\\)\n\\(\\text{SSE}_c = 133888.28\\)\nAugmented model: \\(\\hat{\\text{FPS}}=\\beta_0+\\beta_1*\\text{bg}\\_2_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a = 133111.76\\)\n\\(R^2\\) is the proportion of explained variance over total variance in \\(Y\\). The total variance is equivalent to the SSE of the Mean Only model (WHY?).\n\\(\\frac{\\text{SSE}_\\text{mean}-\\text{SSE}_a}{\\text{SSE}_\\text{mean}}=\\frac{133888.28 - 133111.76}{133888.28}\\) = 0.0058$\nAt this point it equals PRE b/c \\(\\text{SSE}_c\\) is the mean only model!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#effect-sizes-in-r",
    "href": "05_single_dichotomous.html#effect-sizes-in-r",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.12 Effect Sizes in R",
    "text": "5.12 Effect Sizes in R\n\n\nCode\nm_mean &lt;- lm(fps ~ 1, data = data) \nm_d &lt;- lm(fps ~ bg_2, data = data)\n\n\n\\(\\eta_p^2\\)\n\n\nCode\n(sum(residuals(m_mean)^2)- sum(residuals(m_d)^2))/sum(residuals(m_mean)^2) \n\n\n[1] 0.005799798\n\n\n\\(R^2\\)\n\n\nCode\nbroom::glance(m_d)$r.squared \n\n\n[1] 0.005799798",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#unit-weighted-centered-coefficients",
    "href": "05_single_dichotomous.html#unit-weighted-centered-coefficients",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.13 Unit Weighted, Centered Coefficients",
    "text": "5.13 Unit Weighted, Centered Coefficients\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(bg_2c = if_else(bg_2 == 0, -.5, .5))\n\n\n\n\nCode\nm_c &lt;- lm(fps ~ bg_2c, data = data)\n\nm_c |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    33.8       4.43     7.63  1.90e-11\n2 bg_2c          -6.57      8.87    -0.741 4.61e- 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#visual-displays-of-dummy-vs.-centered",
    "href": "05_single_dichotomous.html#visual-displays-of-dummy-vs.-centered",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.14 Visual Displays of Dummy vs. Centered",
    "text": "5.14 Visual Displays of Dummy vs. Centered\n\n\nCode\nplot_d &lt;- plot_d +\n  labs(title = \"Dummy\")\n\nplot_c &lt;- data |&gt; \n  ggplot(aes(x = bg_2c, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_c)[1],\n                  slope = coef(m_c)[2]),\n                  linewidth = 1) +\n  labs(title = \"Zero-centered\")\n  \n  \nplot_d + plot_c\n\n\n\n\n\n\n\n\n\nQuestion: \\(b_0, b_1\\) in each?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#b_0-with-zero-centered-coefficients",
    "href": "05_single_dichotomous.html#b_0-with-zero-centered-coefficients",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.15 \\(b_0\\) with Zero-Centered Coefficients",
    "text": "5.15 \\(b_0\\) with Zero-Centered Coefficients\nQuestion: Is \\(b_0\\) (33.83) the mean of FPS in the sample?\n\nIt is the unweighted mean.\nWeighted and unweighted means emerge as concepts with grouped data. An unweighted mean is the mean of the group means, ignoring the \\(N\\) in each group.\n\n\nCode\ndata |&gt; \n  summarise(mean = mean(fps),\n            sd = sd(fps))\n\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  32.2  37.5\n\n\n\n\nCode\ndata |&gt; \n  group_by(bg_2) |&gt; \n  summarise(mean = mean(fps),\n            sd = sd(fps))\n\n\n# A tibble: 2 × 3\n   bg_2  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  37.1  46.1\n2     1  30.5  34.4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#b_0-as-weighted-mean",
    "href": "05_single_dichotomous.html#b_0-as-weighted-mean",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.16 \\(b_0\\) as Weighted Mean",
    "text": "5.16 \\(b_0\\) as Weighted Mean\nA weighted mean is the mean of the groups weighted or proportional to their sample sizes. It is also equal to the grand mean of the DV ignoring group.\nQuestion: How can you get \\(b_0\\) to reflect the weighted mean?.\n\nMean center \\(X\\). This is the same as we did with a quantitative variable to make \\(b_0\\) equal the grand mean.\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(bg_2mc = bg_2 - mean(bg_2))\n\n\n\n\nCode\ndata |&gt; \n  pull(bg_2mc)\n\n\n [1] -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75\n[13] -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75\n[25]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n[37]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n[49]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n[61]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n[73]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n[85]  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n\n\n\n\n\nCode\nmean(data$bg_2mc)\n\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#b_0-as-weighted-mean-1",
    "href": "05_single_dichotomous.html#b_0-as-weighted-mean-1",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.17 \\(b_0\\) as Weighted Mean",
    "text": "5.17 \\(b_0\\) as Weighted Mean\n\n\nCode\nm_mc &lt;- lm(fps ~ bg_2mc, data = data)\n\nm_mc |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    32.2       3.84     8.38  5.02e-13\n2 bg_2mc         -6.57      8.87    -0.741 4.61e- 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#comparing-b_0-across-models",
    "href": "05_single_dichotomous.html#comparing-b_0-across-models",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.18 Comparing \\(b_0\\) Across Models",
    "text": "5.18 Comparing \\(b_0\\) Across Models\n\n\nCode\nplot_d &lt;- plot_d +\n  labs(title = \"Dummy\") +\n  annotate(\"text\", x = -.1, y = round(coef(m_d)[1],2) + 10, label = round(coef(m_d)[1],2), \n           size = 5, color = \"blue\") +\n  coord_cartesian(clip = \"off\") \n\nplot_c &lt;- data |&gt; \n  ggplot(aes(x = bg_2c, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_c)[1],\n                  slope = coef(m_c)[2]),\n                  linewidth = 1) +\n  labs(title = \"Zero-centered\")  +\n  annotate(\"text\", x = 0, y = round(coef(m_c)[1],2) + 10, label = round(coef(m_c)[1],2), \n           size = 5, color = \"blue\") +\n  coord_cartesian(clip = \"off\") \n\nplot_mc &lt;- data |&gt; \n  ggplot(aes(x = bg_2mc, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_mc)[1],\n                  slope = coef(m_mc)[2]),\n                  linewidth = 1) +\n  labs(title = \"Mean-centered\") +\n  annotate(\"text\", x = .9, y = round(coef(m_mc)[1],2) + 10, label = round(coef(m_mc)[1],2), \n           size = 5, color = \"blue\") +\n  coord_cartesian(clip = \"off\") \n\n  \n  \nplot_d + plot_c + plot_mc",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#other-not-useful-coefficients",
    "href": "05_single_dichotomous.html#other-not-useful-coefficients",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.19 Other (not useful) Coefficients",
    "text": "5.19 Other (not useful) Coefficients\nWhat would be the interpretation of \\(b_0\\) and \\(b_1\\) if I used coefficients of -1 and 1 for bg_2?\n\n\\(b_0\\) will be the unweighted mean of FPS because 0 is halfway between the two groups.\n\\(b_1\\) will be half what it was in all previous models b/c a one unit change on \\(X\\) will only go halfway between no-alcohol and alcohol groups.\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(bg_2o = if_else(bg_2==0, -1, 1))\n\n\n\n\nCode\nm_o &lt;- lm(fps ~ bg_2o, data = data)\n\nm_o |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    33.8       4.43     7.63  1.90e-11\n2 bg_2o          -3.28      4.43    -0.741 4.61e- 1\n\n\n\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bg_2o, y = fps)) +\n  geom_point(alpha = .6, size = 2) +\n  geom_abline(aes(intercept = coef(m_o)[1],\n                  slope = coef(m_o)[2]),\n                  linewidth = 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "05_single_dichotomous.html#publication-quality-figure",
    "href": "05_single_dichotomous.html#publication-quality-figure",
    "title": "5  Inferences about a Single Dichotomous Predictor",
    "section": "5.20 Publication Quality Figure",
    "text": "5.20 Publication Quality Figure\n\n\nCode\nx &lt;- tibble(bg_2 = c(0,1))\n\npreds_pub &lt;- x |&gt; \n  bind_cols(predict(m_d, x, se.fit = TRUE) |&gt; \n  as_tibble())\n\n\n\n\nCode\nplot_pub &lt;- ggplot() +\n  geom_col(aes(x = as.factor(preds_pub$bg_2), y = preds_pub$fit, \n               fill = as.factor(preds_pub$bg_2)),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(data$bg_2), y = data$fps), width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds_pub$fit-preds_pub$se.fit, \n                    ymax = preds_pub$fit+preds_pub$se.fit, \n                    x = as.factor(preds_pub$bg_2)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"No alcohol\", \"Alcohol\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Group\",\n       y = \"Fear-potentiated startle\") \n\n\n\n\n\nCode\nplot_pub\n\n\n\n\n\nBars display point estimates for fear potentiated startle by group from the general linear model. Confidence intervals display +/- 1 standard error for these point estimates.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferences about a Single Dichotomous Predictor</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html",
    "href": "06_two_predictors.html",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "",
    "text": "6.1 Multiple Regression 2+ Predictors",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#multiple-regression-2-predictors",
    "href": "06_two_predictors.html#multiple-regression-2-predictors",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "",
    "text": "Consider how the concepts we have discussed so far generalize to the 2 predictor (3 parameter) model.\n\nWe will start with 2 quantitative predictors example. Will continue with 1 quantitative and 1 dichotomous predictor example.\n\nLearn how to quantify, test, and interpret ‘partial’ effects:\n\n\\(b_j\\)\n\\(\\Delta R^2, \\eta_p^2\\)\n\nMulticollinearity\nText, table and figure descriptions of results\nGeneralization to &gt;2 predictors is straightforward (Unit 7).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#benefits-of-multiple-predictors",
    "href": "06_two_predictors.html#benefits-of-multiple-predictors",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.2 Benefits of Multiple Predictors",
    "text": "6.2 Benefits of Multiple Predictors\n\nStatistical power: Goal is to increase power to test focal predictor’s effect on DV by adding it to model that contains additional known predictors of DV.\nAdditional explanatory power: Goal is to demonstrate that focal predictor adds explanatory power above and beyond other predictor(s) [Unique effect controlling for other predictors].\nEfficiency: Can test focal effects of two predictors in one study (each benefiting from increased power per point 1).\nMediation: We have identified a known cause of a DV. We add a new focal predictor to test if the effect of our known causal IV on the DV is mediated by our focal predictor (i.e., identify “mechanism” of IV effect).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#alcohol-and-stress-response-dampening-srd",
    "href": "06_two_predictors.html#alcohol-and-stress-response-dampening-srd",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.3 Alcohol and Stress Response Dampening (SRD)",
    "text": "6.3 Alcohol and Stress Response Dampening (SRD)\nTest for Alcohol Stress response dampening\nManipulate BAC (0.00% - 0.15%)\nStressor Task (threat of unpredictable shock)\nMeasure Stress Response (Fear potentiated startle)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#two-parameter-1-predictor-model",
    "href": "06_two_predictors.html#two-parameter-1-predictor-model",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.4 Two Parameter (1 Predictor) Model",
    "text": "6.4 Two Parameter (1 Predictor) Model\n\n\nCode\nm_2 &lt;- lm(fps ~ bac, data = data)\n\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nDescribe the interpretation of \\(b_1\\) (coefficient for BAC) and its significance test.\n\n\\(b_1\\) describes the relationship between BAC and FPS in the units of each measure. FPS will decrease by 184 µV for every 1% increase in BAC (It will decrease by 1.84µV for every .01 increase in BAC).\nThe significance test for \\(\\beta_1\\) tests the null hypothesis that the population relationship between BAC and FPS is 0 (i.e., \\(\\beta_1\\) = 0, no relationship). We fail to reject this \\(H_0\\). Conclude that alcohol does not affect FPS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#testing-inferences-about-beta_1",
    "href": "06_two_predictors.html#testing-inferences-about-beta_1",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.5 Testing Inferences about \\(\\beta_1\\)",
    "text": "6.5 Testing Inferences about \\(\\beta_1\\)\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_a: \\beta_1 \\neq 0\\)\nQuestion: What could we change about the sampling distribution that would make this \\(b_1\\) be less probable given \\(H_0\\) so that we reject the Null?\n\n\n\nCode\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual)) |&gt; \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  labs(title = \"Sampling Distribution for b1\")\n\n\n\n\n\n\n\n\n\nIf the standard deviation of the sampling distribution (its standard error) was smaller so that the distribution was narrower, \\(b_1\\) would be less probable given \\(H_0\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#standard-errors-of-glm-coefficients",
    "href": "06_two_predictors.html#standard-errors-of-glm-coefficients",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.6 Standard Errors of GLM Coefficients",
    "text": "6.6 Standard Errors of GLM Coefficients\nThe formula for the standard error for a coefficient \\(b_j\\) in multiple (more than one predictor) regression is:\n\\(\\text{SE}_{bj} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R^2_j)}}\\)\n\\(R^2_j\\) = variance in \\(X_j\\) accounted for by all other predictors in the model (i.e., how redundant is \\(X_{a_j}\\) in model?). Literally, predict \\(X_j\\) as DV with all other predictors as IVs.\nNote: Formula for the standard error for \\(b_0\\) is different.\nFor the one predictor model it is (need matrix notation for \\(k&gt;1\\)):\n\\(\\sqrt {\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{X^2}{(N-1)s_x^2}}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#se-for-b_j-and-r2",
    "href": "06_two_predictors.html#se-for-b_j-and-r2",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.7 SE for \\(b_j\\) and \\(R^2\\)",
    "text": "6.7 SE for \\(b_j\\) and \\(R^2\\)\n\\(\\text{SE}_{b_j}= \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{(N-P)}*\\frac{1}{\\sqrt{(1-R_j^2)}}\\)\nIf we increase \\(R_y^2\\), we would decrease the SE for our regression coefficient.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#model-comparison-testing-inferences-about-beta_1",
    "href": "06_two_predictors.html#model-comparison-testing-inferences-about-beta_1",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.8 Model Comparison: Testing Inferences about \\(\\beta_1\\)",
    "text": "6.8 Model Comparison: Testing Inferences about \\(\\beta_1\\)\n\\(H_0: \\beta_1 = 0; H_a: \\beta_1 \\neq 0\\)\nQuestion: What two models are you comparing when you test hypotheses about \\(\\beta_1\\) for BAC? Describe the logic.\n\nCompact model: \\(\\hat{\\text{FPS}}_i = \\beta_0+0*\\text{BAC}_i\\)\n\\(P_c=1\\)\n\\(\\text{SSE}_c=133888.3\\)\nAugmented model: \\(\\hat{\\text{FPS}}_i = \\beta_0+\\beta_1*\\text{BAC}_i\\)\n\\(P_a=2\\)\n\\(\\text{SSE}_a=128837.1\\)\n\n\\(F(P_a-P_c, N-P_a) = \\frac{\\text{SSE}_c-\\text{SSE}_a/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\nF(1,94) = 3.685, p = 0.058\n\n\\(F(P_a-P_c, N-P_a) = \\frac{\\text{SSE}_c-\\text{SSE}_a/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\nQuestion: What could you change from this model comparison perspective to increase \\(F\\) and probability to reject the \\(H_0\\) about \\(\\beta_1\\)?\n\nMake \\(\\text{SSE}_a\\) smaller by explaining more variance in \\(Y_i\\).\nOf course, \\(R^2 = \\frac{\\text{SSE}_\\text{mean-only} - \\text{SSE}_a}{\\text{SSE}_\\text{mean-only} }\\)\nIf you decrease \\(\\text{SSE}_a\\) or increase model \\(R^2\\), you will have more power to reject \\(H_0\\) regarding parameter estimates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#two-parameter-1-predictor-model-continued",
    "href": "06_two_predictors.html#two-parameter-1-predictor-model-continued",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.9 Two Parameter (1 Predictor) Model (Continued)",
    "text": "6.9 Two Parameter (1 Predictor) Model (Continued)\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\nQuestion: What can we do analytically to decrease SSE (increase model \\(R^2\\))?\n\nInclude another predictor (covariate) in the model that accounts for additional variance in \\(Y\\) (reduces SSE).\nIdeally, this covariate should be orthogonal (uncorrelated) with the other predictor (BAC).\nIn this experiment, I could have measured another predictor of stress response, Trait Anxiety (TA). TA might be expected to be a robust predictor of FPS. It also should be uncorrelated with BAC because I manipulated BAC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#open-view-preliminary-checks",
    "href": "06_two_predictors.html#open-view-preliminary-checks",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.10 Open, View, & Preliminary Checks",
    "text": "6.10 Open, View, & Preliminary Checks\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\ntheme_set(theme_classic()) \n\n\npath_data &lt;- \"data_lecture\" \n\ndata &lt;- read_csv(here::here(path_data, \"6_two_predictors_fps.csv\"),\n                 show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 96\nColumns: 4\n$ subid &lt;chr&gt; \"0125\", \"0013\", \"0113\", \"0116\", \"0111\", \"0014\", \"0124\", \"0022\", …\n$ bac   &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    &lt;dbl&gt; 110, 120, 35, 119, 26, 103, 52, 34, 208, 34, 254, 84, 249, 163, …\n$ fps   &lt;dbl&gt; -98.0977778, -22.5285000, 0.4632944, 1.1943667, 2.7280444, 6.723…\n\n\n\n\n\nCode\nskimr::skim_without_charts(data)\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n96\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubid\n0\n1\n4\n4\n0\n96\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nbac\n0\n1\n0.06\n0.04\n0.0\n0.02\n0.06\n0.08\n0.14\n\n\nta\n0\n1\n147.61\n105.73\n10.0\n73.75\n119.00\n208.75\n445.00\n\n\nfps\n0\n1\n32.19\n37.54\n-98.1\n6.79\n19.46\n50.46\n162.74\n\n\n\n\n\n\n\n\nCode\nplot_fps &lt;- data |&gt; \n  ggplot(aes(x = fps)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +\n  geom_rug(color = \"red\")\n\nplot_bac &lt;- data |&gt; \n  ggplot(aes(x = bac)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0,\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\")\n\nplot_ta &lt;- data |&gt; \n  ggplot(aes(x = ta)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\n\n\n\n\nCode\nplot_fps + plot_bac + plot_ta\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  select(where(is.numeric)) |&gt; \n           psych::corr.test()\n\n\nCall:psych::corr.test(x = select(data, where(is.numeric)))\nCorrelation matrix \n      bac    ta   fps\nbac  1.00 -0.02 -0.19\nta  -0.02  1.00  0.44\nfps -0.19  0.44  1.00\nSample Size \n[1] 96\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n     bac   ta  fps\nbac 0.00 0.87 0.12\nta  0.87 0.00 0.00\nfps 0.06 0.00 0.00\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = ta, y = fps)) +\n  geom_point(alpha = .6)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata |&gt; \n  ggplot(aes(x = bac, y = ta)) +\n  geom_point(alpha = .6)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#the-two-predictor-and-general-linear-models",
    "href": "06_two_predictors.html#the-two-predictor-and-general-linear-models",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.11 The Two Predictor and General Linear Models",
    "text": "6.11 The Two Predictor and General Linear Models\nDATA = MODEL + ERROR\nTwo Predictor Model for Sample Data\n\\(Y_i=b_0+b_1X_1+b_2X_2+e_i\\)\n\\(\\hat{Y_i}=b_0+b_1X_1+b_2X_2\\)\n\\(k\\) Predictor Model for Sample Data\n\\(Y_i=b_0+b_1X_1+...+b_kX_k+e_i\\)\n\\(\\hat{Y_i}=b_0+b_1X_1+...+b_kX_k\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#testing-bac-in-a-three-parameter-model-2-predictors",
    "href": "06_two_predictors.html#testing-bac-in-a-three-parameter-model-2-predictors",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.12 Testing BAC in a Three Parameter Model (2 Predictors)",
    "text": "6.12 Testing BAC in a Three Parameter Model (2 Predictors)\n\n\nCode\nm_3 &lt;- lm(fps ~ bac + ta, data = data)\n\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\n\\(\\hat{\\text{FPS}} = 19.4 + -177 * \\text{BAC} + 0.2 * \\text{TA}\\)\nQuestion: What parameter estimate is used to test our research question about the effect of BAC? What are our \\(H_0\\) and \\(H_a\\) for the associated population parameter?\n\nWe use \\(b_1\\) (-177) to test our hypothesis about the population effect of BAC (\\(\\beta_1\\)).\n\\(H_0:\\beta_1=0; H_a: \\beta_1 \\neq 0\\)\n\n\n\nCode\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\nDescribe conclusion and logic of the test of \\(H_0:\\beta_1=0\\) from sampling distribution perspective.\n\nIf \\(H_0\\) is true, we expect a sampling distribution for \\(b_1\\) to have a mean of 0 and an SE of 86.6 (red curve below).\n\n\nCode\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_3), term == \"bac\")$std.error, m_3$df.residual)) |&gt; \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line(color = \"red\") +\n  geom_vline(xintercept = subset(broom::tidy(m_3), term == \"bac\")$estimate, \n             color = \"black\") \n\n\n\n\n\n\n\n\n\nA sample \\(b_1\\) = -177.0 is unlikely (about 2 standard deviations below mean; p = .0437). Therefore, we reject our \\(H_0\\) and conclude that \\(\\beta_1 \\neq 0\\).\nConclusion is that BAC affects FPS.\n\nDescribe conclusion and logic of the test of \\(H_0:\\beta_1=0\\) from model comparison perspective.\n\n\\(H_0: \\beta_1 = 0; H_a: \\beta_1 \\neq 0\\)\nCompact Model: \\(\\hat{\\text{FPS}}=\\beta_0 + 0*\\text{BAC}+\\beta_2*\\text{TA}\\)\n\\(\\text{SSE}_c = 108547.9\\)\n\\(P_c = 2\\)\nAugmented Model: \\(\\hat{\\text{FPS}}=\\beta_0 + \\beta_1*\\text{BAC}+\\beta_2*\\text{TA}\\)\n\\(\\text{SSE}_a = 103877.2\\)\n\\(P_c = 3\\)\n\\(F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\n\\(F (1, 93) = 4.18, p = 0.0442\\)\n\nTwo parameter model test of BAC\n\n\nCode\nm_2 &lt;- lm(fps ~ bac, data = data)\n\nbroom::tidy(m_2)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\n\\(\\text{SSE} =\\) 128837.1\nThree parameter model test of BAC\n\n\nCode\nbroom::tidy(m_3)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\n\\(\\text{SSE} =\\) 103877.2\nQuestion: What changed about test of \\(\\beta_1\\) (BAC effect) and why?\n\nSE for BAC and SSE decreased.\nAs a result, \\(R^2\\) increases.\nTwo parameter model test of BAC\n\n\nCode\nbroom::glance(m_2) |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1    0.0377\n\n\nThree parameter model test of BAC\n\n\nCode\nbroom::glance(m_3) |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.224",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#standard-error-of-partial-regression-coefficient-b_j",
    "href": "06_two_predictors.html#standard-error-of-partial-regression-coefficient-b_j",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.13 Standard Error of Partial Regression Coefficient (\\(b_j\\))",
    "text": "6.13 Standard Error of Partial Regression Coefficient (\\(b_j\\))\n\\(t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}\\)\n\\(\\text{SE}_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}\\)\nQuestion: What happens to \\(\\text{SE}_{b_j}\\) as model \\(R^2\\) (\\(R^2_y\\)) increases (holding other factors constant)?\n\n\\(\\text{SE}_{b_j}\\) decreases as model \\(R^2\\) increases. In other words, the sampling distribution get narrower.\n\nQuestion: What happens to significance test of \\(b_j\\) as \\(\\text{SE}_{b_j}\\) decreases (holding other factors constant)?\n\n\\(t\\) increases and associated p-value deceases (More Power!).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#sampling-distributions-and-power",
    "href": "06_two_predictors.html#sampling-distributions-and-power",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.14 Sampling Distributions and Power",
    "text": "6.14 Sampling Distributions and Power\n\\(t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}\\)\nTwo parameter model test of BAC\n\n\nCode\nbroom::tidy(m_2)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\n\\(t(96-2) = \\frac{-184 - 0}{95.9}\\)\n\\(t(94) = -1.92\\)\n\\(p = .0579\\)\n\n\\(t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}\\)\nThree parameter model test of BAC\n\n\nCode\nbroom::tidy(m_3)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\n\\(t(96-3) = \\frac{-177 - 0}{86.6}\\)\n\\(t(93) = -2.04\\)\n\\(p = .0437\\)\n\n\n\nCode\ndistr_data &lt;- tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_3), term == \"bac\")$std.error, m_3$df.residual),\n       group = \"3 parameter model\") |&gt; \n  bind_rows(tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual),\n       group = \"2 parameter model\"))\n\ndistr_data |&gt; \n  ggplot(aes(x = b1, y = probability, color = group)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  geom_vline(xintercept = subset(broom::tidy(m_3), term == \"bac\")$estimate, \n             color = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#sampling-distributions-and-precision",
    "href": "06_two_predictors.html#sampling-distributions-and-precision",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.15 Sampling Distributions and Precision",
    "text": "6.15 Sampling Distributions and Precision\n\\(\\text{CI}_b=b \\pm t(\\alpha; N-P)*\\text{SE}_b\\)\n\n\nCode\nconfint(m_2)\n\n\n                 2.5 %    97.5 %\n(Intercept)   29.45597 55.457721\nbac         -374.49261  6.308724\n\n\n\\(\\Delta\\) 380.801\n\n\nCode\nconfint(m_3)\n\n\n                    2.5 %     97.5 %\n(Intercept)    4.22130089 34.6411333\nbac         -348.98099457 -5.1177113\nta             0.08891558  0.2177329\n\n\n\\(\\Delta\\) 343.863",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#standard-error-of-partial-regression-coefficient-b_j-continued",
    "href": "06_two_predictors.html#standard-error-of-partial-regression-coefficient-b_j-continued",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.16 Standard Error of Partial Regression Coefficient (\\(b_j\\); Continued)",
    "text": "6.16 Standard Error of Partial Regression Coefficient (\\(b_j\\); Continued)\n\\(t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}\\)\n\\(\\text{SE}_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_Y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}\\)\n\\(R^2_j\\) = variance in \\(X_j\\) accounted for by all other predictors in model (i.e., how redundant is \\(X_j\\) in model?).\nQuestion: What other factors affect SE for regression coefficients and how?\n\nIncreasing \\(N\\) decreases SE (increases power).\nIncreasing \\(P\\) increases SE (decreases power).\nIncreasing \\(s_y\\) increases SE (decreases power).\nIncreasing \\(s_j\\) decreases SE (increases power).\nIncreasing \\(R_j^2\\) increases SE (decreases power).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#power-and-sse-in-two-and-three-parameter-models",
    "href": "06_two_predictors.html#power-and-sse-in-two-and-three-parameter-models",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.17 Power and SSE in Two and Three Parameter Models",
    "text": "6.17 Power and SSE in Two and Three Parameter Models\nTwo parameter model test of BAC\nCompact Model: \\(\\hat{\\text{FPS}}= 32.2 + 0*\\text{BAC}\\)\n\\(\\text{SSE}_c = 133888.3\\)\n\\(P_c\\) = 1\nAugmented Model: \\(\\hat{\\text{FPS}}= 42.5 + -184.1 *\\text{BAC}\\)\n\\(\\text{SSE}_a = 128837.1\\)\n\\(P_a\\) = 2\nThree parameter model test of BAC\nCompact Model: \\(\\hat{\\text{FPS}}= 9.4 + 0*\\text{BAC} + 0.2 * \\text{TA}\\)\n\\(\\text{SSE}_c = 108547.9\\)\n\\(P_c\\) = 2\nAugmented Model: \\(\\hat{\\text{FPS}}= 19.4 + -177 *\\text{BAC} + 0.2 *\\text{TA}\\)\n\\(\\text{SSE}_a = 103877.2\\)\n\\(P_a\\) = 3\n\\(F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\n\nQuestion: How can you see the increase in power from the model comparison perspective?\n\nTwo parameter model test of BAC\n\\(F(2-1, 96-2) = \\frac{(133888.3 - 128837.1)/(2-1)}{128837.1 / (96-2)}\\)\n\\(F(1, 94) = \\frac{(5051.2)/(1)}{1370.6}\\)\n\\(F (1, 94) = 3.69, p = 0.0579\\)\nThree parameter model test of BAC\n\\(F(3-2, 96-3) = \\frac{(108547.9 - 103877.2)/(3-2)}{103877.2 / (99-3)}\\)\n\\(F(1, 93) = \\frac{(4670.7)/(1)}{1117}\\)\n\\(F (1, 93) = 4.18, p = 0.0442\\)\nDecreased \\(\\text{SSE}_a\\) in three parameter model. Flip side of increased model \\(R^2\\).\n\n\\(F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}\\)\nImpact of \\(N\\) on \\(P_a\\) also clear.\nImpact of \\(s_y\\) and \\(s_{x_j}\\) and multicollinearity less clear in formula.\nConnection to precision of parameter estimation less clear in formula.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#r2_j-and-multicollinearity",
    "href": "06_two_predictors.html#r2_j-and-multicollinearity",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.18 \\(R^2_j\\) and Multicollinearity",
    "text": "6.18 \\(R^2_j\\) and Multicollinearity\n\\(t(N-P) = \\frac{b-0}{\\text{SE}_b}\\)\n\\(\\text{CI}_b= b \\pm t(\\alpha; N-P)*\\text{SE}_b\\)\nThis decrease in power and precision for model parameter estimates (regression coefficients) associated with redundancy among the predictors is called the problem of Multicollinearity.\n\nIt is not sufficient to examine only bivariate correlations among predictors. To determine if a problem exists, calculate Variance Inflation Factors (VIF) for each predictor.\n\\(\\text{VIF}_j = \\frac{1}{(1-R^2_j)}\\)\nVIF tells you how much \\(\\text{SE}_{b_j}\\) is increased because of redundancy. VIFs \\(\\ge\\) 5 are considered problematic (SE increased by factor of 2.2).\n\nWe can use car::vif() to calculate VIFs in R.\n\n\nCode\ncar::vif(m_3)\n\n\n     bac       ta \n1.000296 1.000296 \n\n\nSPSS users may be more familiar with Tolerance (\\(X_i= 1- R^2_j\\)). As tolerance decreases toward 0, Multicollinearity increases.\n\nSolutions for Multicollinearity include:\n\nDrop redundant variable.\nFactor analysis (e.g., PCA) to produce factors that reflect major sources of variance among the redundant predictors.\nThis is only a problem for the variables in the model with high VIFs. If you don’t care about testing them, this is not a problem. Generally, you only care about VIFs for your focal variable(s).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#interpretation-of-multiple-regression-coefficients",
    "href": "06_two_predictors.html#interpretation-of-multiple-regression-coefficients",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.19 Interpretation of Multiple Regression Coefficients",
    "text": "6.19 Interpretation of Multiple Regression Coefficients\nQuestion: What did the value of \\(b_1\\) tell us in a regression model with one predictor?\n\nThe change in \\(Y\\) associated with a one unit increase in \\(X_1\\).\nFor every 1 unit increase in \\(X_1\\), there will be a \\(b_1\\) unit increase in \\(Y\\).\n\nQuestion: What about \\(b_j\\) with multiple (e.g., 2) predictors?\n\nThe change in \\(Y\\) associated with a one unit increase in \\(X_j\\) controlling for all other predictors in the model. “Controlling for” means holding constant.\nFor every 1 unit increase in \\(X_j\\), there will be a \\(b_j\\) unit increase in \\(Y\\) holding all other predictors in the model constant.\n\nQuestion: Do hours of studying per week affect exam performance in 610?\n\n\nCode\ndata_exam &lt;- read_csv(here::here(path_data, \"6_two_predictors_exam.csv\"), \n                 show_col_types = FALSE) \n\ndata_exam &lt;- data_exam |&gt; \n  mutate(study_hours_c = study_hours - mean(study_hours)) |&gt; \n  glimpse()\n\n\nRows: 200\nColumns: 5\n$ subid         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ study_hours   &lt;dbl&gt; 9.197649, 10.897476, 10.508091, 12.297003, 8.789881, 11.…\n$ iq            &lt;dbl&gt; 106.39393, 103.02859, 98.11384, 110.42777, 93.35694, 114…\n$ exam          &lt;dbl&gt; 79.66300, 59.58193, 70.21451, 59.25460, 55.80582, 72.951…\n$ study_hours_c &lt;dbl&gt; -0.80235118, 0.89747646, 0.50809122, 2.29700259, -1.2101…\n\n\n\n\nCode\nm_exam_1 &lt;- lm(exam ~ study_hours_c, data = data_exam)\n\nbroom::tidy(m_exam_1)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n\n\n\nYes, for every one hour of studying per week, students’ exam scores increase by 2.3 points.\n\n\nCode\nbroom::tidy(m_exam_1)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n\n\nInterpretation of \\(b_0\\)?\n\nMaybe study hours are related to exam scores only because intelligent students study more (learned early good study habits) and intelligent students do better on exams.\nQuestion: How could you assess the unique effect of study_hours, controlling for IQ?\n\n\nCode\ndata_exam |&gt; \n  select(study_hours, iq, exam) |&gt; \n  cor() |&gt; \n  round(2)\n\n\n            study_hours   iq exam\nstudy_hours         1.0 0.50 0.40\niq                  0.5 1.00 0.54\nexam                0.4 0.54 1.00\n\n\n\nModel exam scores as a function of both study_hours and IQ. \\(b_1\\) (effect of study hours) in this model is the unique effect of study_hours, controlling for IQ.\n\n\nCode\nm_exam_2 &lt;- lm(exam ~ study_hours_c + iq, data = data_exam)\n\nbroom::tidy(m_exam_2)\n\n\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n\n\nQuestion: Do hours of studying per week affect performance in 610 after controlling for student IQ?\n\nYes, for every one hour of studying per week, students’ exam scores increase by 1 point, controlling for IQ.\n\n\nCode\nbroom::tidy(m_exam_2)\n\n\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n\n\nInterpretation of \\(b_0\\)?\n\nQuestion: Why did the effect of study_hours get smaller after controlling for IQ?\n\n\nCode\nbroom::tidy(m_exam_1)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n\n\n\n\nCode\nbroom::tidy(m_exam_2)\n\n\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n\n\n\n1. When study_hours increases, IQ increases.\n2. When IQ increases, exam scores increase.\n3. The partial effect of study_hours on exam scores is smaller if IQ is not allowed to increase.\n\n\n\nCode\nbroom::tidy(m_exam_1)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n\n\n\\(b_1= 2.34\\)\n\n\nCode\nbroom::tidy(m_exam_2)\n\n\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n\n\n\\(b_1= 1.04\\)\n\n\nCode\nbroom::tidy(lm(iq ~ study_hours_c, data = data_exam))\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     100.       0.921    109.   2.10e-178\n2 study_hours_c     3.75     0.462      8.12 4.77e- 14\n\n\n\\(b_1= 3.75\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#causal-models",
    "href": "06_two_predictors.html#causal-models",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.20 Causal Models?",
    "text": "6.20 Causal Models?\nTotal effect of study_hours\n\n\nCode\nDiagrammeR::grViz(\"\ndigraph{\n  graph[rankdir=LR]\n  node [shape = box, fontsize = 5, width = .5, height = .2]\n    A [label = 'study_hours']\n    Y [label = 'exam']\n    A-&gt;Y [label = '2.34', fontsize = 5]\n}\n\")\n\n\n\n\n\n\n\nDirect and indirect/spurious effects on study_hours\n\n\nCode\nDiagrammeR::grViz(\"\ndigraph{\n  graph[rankdir=LR]\n  node [shape = box, fontsize = 5, width = .5, height = .2]\n    A [label = 'study_hours']\n    B [label = 'iq']\n    Y [label = 'exam']\n    A-&gt;Y [label = '1.04', fontsize = 5]\n    B-&gt;Y [label = '.35', fontsize = 5]\n    A-&gt;B [dir = both, label = '3.75', fontsize = 5]\n{ rank = same; A; B }\n}\n\")\n\n\n\n\n\n\nDirect + Indirect/Spurious = Total\n1.04 + (3.75 * 0.35) = 2.34",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#interpretation-of-multiple-regression-coefficients-continued",
    "href": "06_two_predictors.html#interpretation-of-multiple-regression-coefficients-continued",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.21 Interpretation of Multiple Regression Coefficients (Continued)",
    "text": "6.21 Interpretation of Multiple Regression Coefficients (Continued)\nLets return to our running example.\n\n\nCode\nm_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n\n\n\n\nCode\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\nQuestion: Why did \\(b\\) for BAC get smaller (less negative) when TA was controlled? Hint: consider the bivariate relationships between all variables.\n\n\nCode\ndata |&gt; \n  select(bac, ta, fps) |&gt; \n  cor() |&gt; \n  round(2)\n\n\n      bac    ta   fps\nbac  1.00 -0.02 -0.19\nta  -0.02  1.00  0.44\nfps -0.19  0.44  1.00\n\n\n\n1. When BAC increases, TA decreases.\n2. When TA decreases FPS decreases. This negative indirect/spurious effect contributes to the total effect of BAC (negative \\(b\\) for BAC in 1 predictor model).\n3. The partial effect of BAC on FPS, if TA was not allowed to decrease (i.e., is controlled or held constant), is less negative than when it is allowed to naturally decrease as BAC increases.\n\nQuestion: In what situation would \\(b_j\\) for a focal predictor not change when you added an additional predictor (covariate)? \n\n1. If the new predictor was completely uncorrelated (orthogonal) with the focal predictor in those sample data, there would be no change in the parameter estimate when you added this new predictor.\n2. This is why uncorrelated predictors/covariates are considered easier to interpret when trying to increase power. If they are related to the DV, they will increase power (&lt; SE) to test your focal variable but they will not change your estimate of the magnitude of the focal variables parameter estimate.\n3. Completely orthogonal variables are typically only observed in experimental designs. However, small/trivial, nonsystematic sample \\(r\\) will occur when population \\(r=0\\).\n\n\\(\\text{SSR}=\\text{SSE}_c - \\text{SSE}_a\\)\nTwo parameter model test of BAC\n\\(F(2-1, 96-2) = \\frac{(133888.3 - 128837.1)/(2-1)}{128837.1 / (96-2)}\\)\n\\(F(1, 94) = \\frac{(5051.2)/(1)}{1370.6}\\)\n\\(F (1, 94) = 3.69, p = 0.0579\\)\n\\(\\text{SSR} = 5051.2\\)\nThree parameter model test of BAC\n\\(F(3-2, 96-3) = \\frac{(108547.9 - 103877.2)/(3-2)}{103877.2 / (99-3)}\\)\n\\(F(1, 93) = \\frac{(4670.7)/(1)}{1117}\\)\n\\(F (1, 93) = 4.18, p = 0.0442\\)\n\\(\\text{SSR} = 4670.7\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#model-effect-size-coefficient-of-determination-r2",
    "href": "06_two_predictors.html#model-effect-size-coefficient-of-determination-r2",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.22 Model Effect Size: Coefficient of Determination (\\(R^2\\))",
    "text": "6.22 Model Effect Size: Coefficient of Determination (\\(R^2\\))\nCoefficient of Determination (\\(R^2\\)):\nProportion of variance in \\(Y\\) explained by the set of all model predictors (i.e., proportion of variance in \\(Y\\) predicted by all \\(X\\)s in model).\n\n\nCode\nbroom::glance(m_3) |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.224\n\n\n\n\\(R^2\\) for any augmented model is:\n\\(R^2 = \\frac{\\text{SSE}_\\text{mean-only} - \\text{SSE}_a}{\\text{SSE}_\\text{mean-only} }\\)\nMean-Only Model: \\(\\hat{\\text{FPS}}_i=\\beta_0\\)\n\\(\\text{SSE}_{\\text{mean-only}} = 133888.3\\)\nAugmented Model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_a = 103877.2\\)\n\\(R^2 = \\frac{133888.3 - 103877.2}{133888.3} = 0.2242\\)\nIn this augmented model, \\(R^2\\) describes the combined effect of BAC and TA. In more complex models, R2 will always be predictive strength of the set of all predictors.\n\n\n\nCode\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\n\n\nCode\nm_3 |&gt; \n  broom::glance() |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.224",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#predictor-effect-size-options",
    "href": "06_two_predictors.html#predictor-effect-size-options",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.23 Predictor Effect Size Options",
    "text": "6.23 Predictor Effect Size Options\n\nAs in the one predictor model, the parameter estimates in the two predictor model (and the \\(k\\) predictor model) are attractive effect size estimates.\nIn addition, there are variance based effect size estimates that are also attractive.\nYou have already learned about Partial eta-squared (\\(\\eta_p^2\\)), which Judd et al refer to as PRE.\nYou will now also learn about Delta \\(R^2\\) (\\(\\Delta R^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#variance-based-effect-sizes-eta_p2-or-pre",
    "href": "06_two_predictors.html#variance-based-effect-sizes-eta_p2-or-pre",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.24 Variance Based Effect Sizes: \\(\\eta_p^2\\) or PRE",
    "text": "6.24 Variance Based Effect Sizes: \\(\\eta_p^2\\) or PRE\n\\(\\eta_p^2\\) or PRE describes how much SSE was reduced (proportionally) in the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.\nCompact Model: \\(\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_c = 108547.9\\)\nAugmented Model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_a = 103877.2\\)\nQuestion: How much was the error reduced by estimating \\(\\beta_1\\) for BAC?\n\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c} = \\frac{108547.9 - 103877.2}{108547.9} = 0.043\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#variance-based-effect-sizes-delta-r2",
    "href": "06_two_predictors.html#variance-based-effect-sizes-delta-r2",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.25 Variance Based Effect Sizes: \\(\\Delta R^2\\)",
    "text": "6.25 Variance Based Effect Sizes: \\(\\Delta R^2\\)\n\\(\\Delta R^2\\) is the increase in model \\(R^2\\) for the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.\n\\(\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_c = 108547.9\\)\n\\(R^2 = 0.1893\\)\n\\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_a = 103877.2\\)\n\\(R^2 = 0.2242\\)\n\\(\\Delta R^2= 0.2242 -  0.1893 = 0.0349\\)\n\n\\(\\Delta R^2\\) can also be defined with respect to SSE.\nCompact Model: \\(\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_c = 108547.9\\)\nAugmented Model: \\(\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i\\)\n\\(\\text{SSE}_a = 103877.2\\)\nQuestion: How much was the error reduced by estimating \\(\\beta_1\\) for BAC?\n\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}=\\frac{108547.9 - 103877.2}{133888.3} = 0.0349\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#comparing-variance-based-effect-sizes",
    "href": "06_two_predictors.html#comparing-variance-based-effect-sizes",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.26 Comparing Variance Based Effect Sizes",
    "text": "6.26 Comparing Variance Based Effect Sizes\n\\(R^2 = \\frac{\\text{SSE}_{\\text{mean-only}} - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}\\)\n\\(\\Delta R^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}\\)\n\\(\\eta_p^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}\\)\n\n\\(R^2\\)\n\nDescribes proportion of explained variance in \\(Y\\) explained by full model relative to total variance in \\(Y\\).\n\nCan not be used for a specific predictor.\n\nNot used frequently in Psychology but useful in other fields.\n\n\\(\\Delta R^2\\)\n\nDescribes proportion of unique variance in \\(Y\\) explained by \\(X_j\\) relative to total variance in \\(Y\\).\n\nIf \\(X\\)s are orthogonal \\(\\Delta R^2\\) will sum to \\(R^2\\).\n\nAnchored to total \\(Y\\) variance.\n\nSame denominator for all \\(X\\)s.\n\n\\(\\eta_p^2\\)\n\nDescribes proportion of reduction of unexplained variance (SSE) by adding \\(X_j\\).\n\n\\(\\ge \\Delta R^2\\) (stupid!).\nSPSS reports it (stupid!).\nStable in experimental designs when additional IVs are added.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#visualizing-the-model",
    "href": "06_two_predictors.html#visualizing-the-model",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.27 Visualizing the Model",
    "text": "6.27 Visualizing the Model\n\\(\\hat{\\text{FPS}}= 19.4 + -177 *\\text{BAC} + 0.2 *\\text{TA}\\)\n\n\nCode\nlibrary(plot3D)\n\npreds_bac &lt;- seq(min(data$bac), max(data$bac), length.out = 100)\npreds_ta &lt;- seq(min(data$ta), max(data$ta), length.out = 100)\n\npreds_x &lt;- expand.grid(bac = preds_bac, ta = preds_ta)\n\npreds_y &lt;- matrix(predict(m_3, newdata = preds_x),\n                 nrow = 100,\n                 ncol = 100)\n\nscatter3D(data$bac, data$ta, data$fps, pch = 19, col = \"black\", ticktype = \"detailed\",\n          xlab = \"BAC\", ylab = \"TA\", zlab = \"FPS\",\n          surf = list(x = preds_bac, y = preds_ta, z = preds_y, alpha = .9, border = \"light grey\"))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npreds_x &lt;- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = mean(preds_ta)) \n\npreds&lt;- preds_x |&gt; \n  bind_cols(predict(m_3, preds_x, interval = \"confidence\", level = .95) |&gt;\n  as_tibble()) \n\n\n\n\nCode\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = preds$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +\n  labs(x = \"BAC\",\n       y = \"FPS\") \n\n\n\n\n\n\n\n\n\n\n\n\nCode\npreds_x &lt;- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = c(round(mean(preds_ta)), round(mean(preds_ta)-(sd(preds_ta)*1.5)), \n                              round(mean(preds_ta)+(sd(preds_ta)*1.5))))\n\npreds &lt;- preds_x |&gt; \n  bind_cols(predict(m_3, preds_x) |&gt;\n  as_tibble()) \n\n\n\n\nCode\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$value, color = as.factor(preds$ta)),\n              linewidth = 1) +\n  labs(x = \"BAC\",\n       y = \"FPS\",\n       color = \"TA\") \n\n\n\n\n\n\n\n\n\n\nQuestion: What would change if TA was mean centered?\n\n\nCode\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n\n\n\n\nCode\nm_3 |&gt; \n  broom::glance() |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.224\n\n\n\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(ta_c = ta - mean(ta))\n\nm_3_c &lt;- lm(fps ~ bac + ta_c, data = data) \n\nm_3_c |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   42.1      5.91        7.12 2.28e-10\n2 bac         -177.      86.6        -2.04 4.37e- 2\n3 ta_c           0.153    0.0324      4.73 8.07e- 6\n\n\n\n\nCode\nm_3_c |&gt; \n  broom::glance() |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.224\n\n\n\nQuestion: What do you report and why?\n\n\n\nCode\nm_3_c |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   42.1      5.91        7.12 2.28e-10\n2 bac         -177.      86.6        -2.04 4.37e- 2\n3 ta_c           0.153    0.0324      4.73 8.07e- 6\n\n\n\n\\(\\eta_p^2\\) effect size for BAC\n\n\nCode\nsse_c_bac &lt;- sum(residuals(lm(fps ~ ta_c, data = data))^2)\nsse_a &lt;- sum(residuals(m_3_c)^2)\n\n(p_eta_bac &lt;- (sse_c_bac - sse_a)/sse_c_bac)\n\n\n[1] 0.04302933\n\n\n\\(\\eta_p^2\\) effect size for TA\n\n\nCode\nsse_c_ta &lt;- sum(residuals(lm(fps ~ bac, data = data))^2)\n\n(p_eta_ta &lt;- (sse_c_ta - sse_a)/sse_c_ta)\n\n\n[1] 0.193732\n\n\n\\(\\eta_p^2\\) effect size for intercept\n\n\nCode\nsse_c_int &lt;- sum(residuals(lm(fps ~ bac + ta - 1, data = data))^2)\n\n(p_eta_int &lt;- (sse_c_int - sse_a)/sse_c_int)\n\n\n[1] 0.06472535\n\n\n\n95% CI\n\n\nCode\nconfint(m_3_c)\n\n\n                    2.5 %     97.5 %\n(Intercept)   30.32490549 53.8033111\nbac         -348.98099457 -5.1177113\nta_c           0.08891558  0.2177329",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#describing-model-results",
    "href": "06_two_predictors.html#describing-model-results",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.28 Describing Model Results",
    "text": "6.28 Describing Model Results\nWe regressed fear-potentiated startle (FPS) on Blood alcohol concentration (BAC). We included trait anxiety (mean-centered) as a covariate in this model to increase power to test substantive questions about BAC. We tested partial effects, controlling for all other predictors in the model, from the full model that included both predictors. We provide raw regression coefficients, 95% confidence intervals for these coefficients, and partial eta squared (\\(\\eta_p^2\\)) to quantify effect sizes for each predictor in Table 1.\nFPS was 42.1 \\(\\mu V\\) for participants with 0.00% BAC and average trait anxiety, \\(t(93) = 7.12, p&lt; .001\\), indicating that our threat manipulation successfully increased FPS above zero when sober. As expected, the effect of the trait anxiety covariate was significant and reduced error variance by approximately 19%, \\(t(93)= 4.73, p&lt; .001\\). FPS increased by 0.2 \\(\\mu V\\) for every 1 unit increase in trait anxiety.\nAs predicted, the effect of BAC was significant and reduced error variance by approximately 4%, \\(t(93)= -2.04, p= .044\\). FPS decreased 1.8 \\(\\mu V\\) for every .01% increase in BAC (see Figure 1).\n\n\n\nCode\ncoef &lt;- m_3_c |&gt; \n  broom::tidy() |&gt; \n  select(term, estimate, statistic, p.value) |&gt; \n  mutate(p.value = if_else(p.value &lt; .001, \"&lt;.001\", as.character(round(p.value, 2))))\n\nci &lt;- confint(m_3_c) |&gt; \n  round(2) |&gt; \n  as_tibble() |&gt; \n  unite(ci, `2.5 %`, `97.5 %`, sep = \", \") |&gt; \n  mutate(ci = str_c(\"(\", ci, \")\"))\n\np_eta &lt;- tibble (peta = c(p_eta_int, p_eta_bac, p_eta_ta)) \n\ntable &lt;- coef |&gt; \n  bind_cols(ci, p_eta) |&gt; \n  mutate(` ` = factor(term, levels = c(\"(Intercept)\", \"bac\", \"ta_c\"),\n                     labels = c(\"Intercept\", \"Blood Alcohol Concentration\", \"Trait Anxiety\"))) |&gt; \n  select(` `, \n         b = estimate,\n         `95% CI (b)` = ci,\n         `Partial eta squared` = peta,\n         t = statistic,\n         p = p.value) |&gt; \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"))\n\n\n\nTable 1\n\n\nCode\ntable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb\n95% CI (b)\nPartial eta squared\nt\np\n\n\n\n\nIntercept\n42.06\n(30.32, 53.8)\n0.06\n7.12\n&lt;.001\n\n\nBlood Alcohol Concentration\n-177.05\n(-348.98, -5.12)\n0.04\n-2.04\n0.04\n\n\nTrait Anxiety\n0.15\n(0.09, 0.22)\n0.19\n4.73\n&lt;.001\n\n\n\n\n\nNotes:\n\\(R^2 = 0.224, F(2,93) = 13.44, p &lt; .001\\)***\nTrait anxiety was mean-centered\n\n\n\nCode\npreds_x &lt;- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta_c = mean(data$ta_c))\n\npreds_pub &lt;- predict(m_3_c, preds_x, se.fit = TRUE) |&gt;\n  as_tibble() |&gt;\n  mutate(upper = fit + se.fit,\n         lower= fit - se.fit)\n\n\n\n\nCode\nplot_pub &lt;- ggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds_x$bac, y = preds_pub$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = preds_x$bac, ymin = preds_pub$lower, ymax = preds_pub$upper), alpha = 0.2) +\n  labs(x = \"Blood alcohol concentration\",\n       y = \"Fear-potentiated startle\") +\n  xlim(c(0, .15)) +\n  ylim(c(-100, 200))\n\n\n\n\n\nCode\nplot_pub\n\n\n\n\n\n\n\n\n\n\nQuestion: What other table should you consider in your results?\n\nTable of simple correlations between variables. Can summarize with other important info fairly concisely. Could also include reliability, skewness, kurtosis, etc.\n\n\nCode\ntibble(` ` = c(\"Trait Anxiety\", \"Blood Alcohol Concentration\", \"Mean\", \"SD\"),\n       `Fear Potentiated Startle` = c(cor(data$ta_c, data$fps), cor(data$bac, data$fps),\n                                      mean(data$fps), sd(data$fps)),\n       `Trait Anxiety` = c(NA, cor(data$bac, data$ta_c), mean(data$ta_c), sd(data$ta_c)),\n       `Blood Alcohol Concentration` = c(NA, NA, mean(data$bac), sd(data$bac))) |&gt; \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\"))\n\n\n\n\n\n\n\n\n\n\n\n\nFear Potentiated Startle\nTrait Anxiety\nBlood Alcohol Concentration\n\n\n\n\nTrait Anxiety\n0.44\n\n\n\n\nBlood Alcohol Concentration\n-0.19\n-0.02\n\n\n\nMean\n32.19\n0.00\n0.06\n\n\nSD\n37.54\n105.73\n0.04",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#multiple-predictors-dichotomous-predictor",
    "href": "06_two_predictors.html#multiple-predictors-dichotomous-predictor",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.29 Multiple Predictors: Dichotomous Predictor",
    "text": "6.29 Multiple Predictors: Dichotomous Predictor\nExample: Evaluate the effects of a new intervention for depression.\n\\(N=200\\) participants, randomly assigned to receive new invervention vs. standard of care control.\nMeasure depression with CES-D pre- (baseline) and post-intervention.\nQuestion: How do you evaluate the effect of the new intervention in a one predictor model?\n\n1. Code one regressor for intervention group using dummy or zero-centered coefficients. How different?\n2. Regress depression scores from post-intervention on intervention group.\n3. Test if \\(b_1\\) for Intervention group is non-zero.\n\n\n\nCode\ndata_int &lt;- read_csv(here::here(path_data, \"6_two_predictors_intervention.csv\"),\n                     show_col_types = FALSE)\n\n\n\n\nCode\nm_depress_1 &lt;- lm(depress_post ~ intervention_group, data = data_int)\n\nm_depress_1 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n\n\nQuestion: What do you conclude? What do you report?\n\nThe new intervention reduced CES-D depressions scores by approximately 3.6 units relative to the standard of care control group, \\(b = -3.6, t(198) = -2.40, p= .0175.\\)\n\nQuestion: What else do you report?\n\nPRE or \\(\\Delta R^2\\). Means/SD of two groups or better - figure with means, standard error and raw (or residual) data points.\n\n\nCode\nbroom::glance(m_depress_1)$r.squared - broom::glance(lm(depress_post ~ 1, data = data_int))$r.squared\n\n\n[1] 0.02818089\n\n\nNote: In this case \\(\\Delta R^2\\) is the same as model \\(R^2\\) because there is only one predictor (i.e., \\(R^2\\) in our compact/mean-only model is 0).\n\nQuestion: What is a better analysis to address this question about the effect of the intervention and why is it better?\n\nControl for baseline depressions scores to increase power.\nBaseline depression should be uncorrelated (in the population) with intervention group because participants were randomly assigned to group. Therefore, including baseline depression should not systematically change \\(b_1\\).\nHowever, baseline scores are likely strong predictors of post-intervention scores. This will increase model \\(R^2\\), reduce SSE, and therefore reduce the SE for intervention group. More power!\n\nPrevious one predictor model\n\n\nCode\nm_depress_1 |&gt; \n  broom::tidy()\n\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n\n\nTwo predictor model\n\n\nCode\ndata_int &lt;- data_int |&gt; \n  mutate(depress_base_c = depress_base - mean(depress_base))\n\nm_depress_2 &lt;- lm(depress_post ~ intervention_group + depress_base_c, data = data_int)\n\nm_depress_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n\n\n\n\\(\\eta_p^2\\) effect size for intercept\n\n\nCode\nsse_c_int &lt;- sum(residuals(lm(depress_post ~ intervention_group + depress_base_c - 1,\n                              data = data_int))^2)\nsse_a &lt;- sum(residuals(m_depress_2)^2)\n\n(p_eta_int &lt;- (sse_c_int - sse_a)/sse_c_int)\n\n\n[1] 0.8893157\n\n\n\\(\\eta_p^2\\) effect size for intervention group\n\n\nCode\nsse_c_group &lt;- sum(residuals(lm(depress_post ~ depress_base_c,\n                              data = data_int))^2)\n\n(p_eta_group &lt;- (sse_c_group - sse_a)/sse_c_group)\n\n\n[1] 0.05251917\n\n\n\\(\\eta_p^2\\) effect size for baseline depression\n\n\nCode\nsse_c_base &lt;- sum(residuals(lm(depress_post ~ intervention_group, data = data_int))^2)\n\n(p_eta_base &lt;- (sse_c_base - sse_a)/sse_c_base)\n\n\n[1] 0.2930961\n\n\nModel \\(R^2\\)\n\n\nCode\nm_depress_2 |&gt; \n  broom::glance() |&gt; \n  select(r.squared)\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.313",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#plot-option-1-raw-data",
    "href": "06_two_predictors.html#plot-option-1-raw-data",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.30 Plot Option 1: Raw Data",
    "text": "6.30 Plot Option 1: Raw Data\n\n\nCode\npreds_int &lt;- tibble(intervention_group = c(0,1),\n                depress_base_c = 0) \n\npreds_int &lt;- preds_int|&gt; \n  bind_cols(predict(m_depress_2, preds_int, se.fit = TRUE) |&gt;\n  as_tibble() |&gt;\n  mutate(upper = fit + se.fit,\n         lower= fit - se.fit))\n\n\n\n\nCode\nplot_raw &lt;- ggplot() +\n  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, \n               fill = as.factor(preds_int$intervention_group)),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(data_int$intervention_group), y = data_int$depress_post), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, \n                    ymax = preds_int$fit+preds_int$se.fit, \n                    x = as.factor(preds_int$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n\n\n\n\n\nCode\nplot_raw",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#plot-option-2-residuals",
    "href": "06_two_predictors.html#plot-option-2-residuals",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.31 Plot Option 2: Residuals",
    "text": "6.31 Plot Option 2: Residuals\n\n\nCode\npreds_res &lt;- data_int |&gt; \n  bind_cols(tibble(resids = residuals(m_depress_2))) |&gt; \n  mutate(depress_post_res = depress_post + resids)\n\n\n\n\nCode\nplot_res &lt;- ggplot() +\n  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, \n               fill = as.factor(preds_int$intervention_group)),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(preds_res$intervention_group), y = preds_res$depress_post_res), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, \n                    ymax = preds_int$fit+preds_int$se.fit, \n                    x = as.factor(preds_int$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n\n\n\n\n\nCode\nplot_res",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#adding-a-third-predictor",
    "href": "06_two_predictors.html#adding-a-third-predictor",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.32 Adding a Third Predictor",
    "text": "6.32 Adding a Third Predictor\n\n\nCode\nm_depress_2 |&gt; \n  broom::tidy()\n\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n\n\nQuestion: What would you do and why if you had also measured sex (unbalanced or balanced) across intervention groups? What would change?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "06_two_predictors.html#summary-new-and-familiar-concepts",
    "href": "06_two_predictors.html#summary-new-and-familiar-concepts",
    "title": "6  Inferences about two predictors (multiple regression without interaction)",
    "section": "6.33 Summary: New and Familiar Concepts",
    "text": "6.33 Summary: New and Familiar Concepts\n\nInterpretation of \\(b_0\\), \\(b_1\\), and \\(b_2\\), in 2+ predictor model with continuous and dichotomous focal predictor.\nImpact of centering \\(X_1\\) or \\(X_2\\) on \\(b_0\\), \\(b_1\\), and \\(b_2\\).\nWhat affects standard errors of \\(b_j\\).\nWhat is Multicollinearity, how to detect, what are implications if high, and what are solutions.\nModel effect size (\\(R^2\\)).\nEffect sizes for \\(X\\)s (\\(b_j\\)s, \\(\\Delta R^2\\), \\(\\eta_p^2\\)).\nText, table, and figure descriptions of results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferences about two predictors (multiple regression without interaction)</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html",
    "href": "07_case_analysis.html",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "",
    "text": "7.1 Anscombe’s Quartet\nAnscombe, Francis J. (1973) Graphs in statistical analysis. American Statistician, 27, 17–21.\nCode\n1broom::tidy(lm(y1 ~ x, data = Quartet))\n\n\n\n1\n\nSee Quartet dataframe in car package.\n\n\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    3.00      1.12       2.67 0.0257 \n2 x              0.500     0.118      4.24 0.00217\nSSE = 13.76\nCode\nbroom::tidy(lm(y2 ~ x, data = Quartet))\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     3.00     1.13       2.67 0.0258 \n2 x               0.5      0.118      4.24 0.00218\nSSE = 13.78\nCode\nbroom::tidy(lm(y3 ~ x, data = Quartet))\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x              0.500     0.118      4.24 0.00218\nSSE = 13.76\nCode\nbroom::tidy(lm(y4 ~ x4, data = Quartet))\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x4             0.500     0.118      4.24 0.00216\nSSE = 13.74",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#case-analysis",
    "href": "07_case_analysis.html#case-analysis",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.2 Case Analysis",
    "text": "7.2 Case Analysis\n\nGoal is to identify any unusual or excessively influential data. These data points may either bias results and/or reduce power to detect effects (inflate standard errors and/or decrease \\(R^2\\)).\nThree aspects of individual observations we attend to:\n\nLeverage\nRegression Outlier\nInfluence\n\nCase Analysis also provides an important first step as you get to know your data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#case-analysis-unusual-and-influential-data",
    "href": "07_case_analysis.html#case-analysis-unusual-and-influential-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.3 Case Analysis: Unusual and Influential Data",
    "text": "7.3 Case Analysis: Unusual and Influential Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(car)\n\ntheme_set(theme_classic()) \n\ndata &lt;- read_csv(here::here(\"data_lecture/7_three_predictors_fps.csv\"), \n                 show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 96\nColumns: 5\n$ subid &lt;chr&gt; \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bac   &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    &lt;dbl&gt; 208, 133, 120, 103, 97, 84, 34, 34, 296, 126, 200, 158, 26, 255,…\n$ fps   &lt;dbl&gt; 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n$ sex   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"fem…\n\n\nNotice sex is of character class type. We can’t use character stings in most of our analyes so we will want to handle character predictors immediately. For example, with our sex variable we could turn it into a factor, dummy code it, center it, or use any other contrast/combination of the above.\n\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(sex_c = if_else(sex == \"female\", -.5, .5),\n         sex = factor(sex, \n                      levels = c(\"female\", \"male\"))) |&gt;\n  glimpse()\n\n\nRows: 96\nColumns: 6\n$ subid &lt;chr&gt; \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bac   &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    &lt;dbl&gt; 208, 133, 120, 103, 97, 84, 34, 34, 296, 126, 200, 158, 26, 255,…\n$ fps   &lt;dbl&gt; 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n$ sex   &lt;fct&gt; female, female, female, female, female, female, female, female, …\n$ sex_c &lt;dbl&gt; -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5…\n\n\n\n\n\nCode\nm_1 &lt;- lm(fps ~ bac + ta + sex_c, data = data)\n\nbroom::tidy(m_1)\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   18.9      7.47        2.53 0.0132    \n2 bac         -165.      84.6        -1.95 0.0545    \n3 ta             0.152    0.0316      4.82 0.00000570\n4 sex_c        -16.0      6.67       -2.40 0.0184",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#univariate-statistics",
    "href": "07_case_analysis.html#univariate-statistics",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.4 Univariate Statistics",
    "text": "7.4 Univariate Statistics\n\n\nCode\ndata |&gt; \n  select(-subid, -sex) |&gt; \n  pivot_longer(everything(), names_to = \"var\") |&gt;\n  group_by(var) |&gt; \n  summarize( n = n(),\n             mean = mean(value), \n             sd = sd(value), \n             min = min(value), \n             max = max(value)) |&gt; \n  mutate(across(mean:max, ~round(.x, 2)))\n\n\n# A tibble: 4 × 6\n  var       n   mean     sd   min    max\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 bac      96   0.06   0.04   0     0.14\n2 fps      96  32.2   37.5  -98.1 163.  \n3 sex_c    96   0      0.5   -0.5   0.5 \n4 ta       96 148.   106.    10   445",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#univariate-plots",
    "href": "07_case_analysis.html#univariate-plots",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.5 Univariate Plots",
    "text": "7.5 Univariate Plots\nHistograms are a useful/common visualization for numeric variables.\nHere we have histograms with density as y-axis overlayed with density and rug plots (which you have seen earlier in the course!).\n\n\nCode\nplot_fps &lt;- data |&gt; \n  ggplot(aes(x = fps)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +\n  geom_rug(color = \"red\")\n\nplot_bac &lt;- data |&gt; \n  ggplot(aes(x = bac)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0,\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\")\n\nplot_ta &lt;- data |&gt; \n  ggplot(aes(x = ta)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\nplot_sex &lt;- data |&gt; \n  ggplot(aes(x = sex_c)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\n\n(plot_fps + plot_bac) / (plot_ta + plot_sex)\n\n\n\n\n\n\n\n\n\n\nBox plots can also be a useful visualization tool for numeric variables.\nBoxplots display:\n\nMedian as line\n25th %ile and 75th %ile as hinges\nHighest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25th %ile and 75th %ile)\nOutliers outside of 1.5 * IQR\n\n\n\nCode\nbox_fps &lt;-  data |&gt;\n    ggplot(aes(x = fps)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nbox_bac &lt;-  data |&gt;\n    ggplot(aes(x = bac)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nbox_ta &lt;-  data |&gt;\n    ggplot(aes(x = ta)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\nCode\nbox_fps + box_bac + box_ta\n\n\n\n\n\n\n\n\n\n\nWe could also overlay a violin plot over the box plot to clearly see the shape of the distribution and tails.\n\n\nCode\n(box_fps +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL)) +\n\n(box_bac +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL)) +\n\n(box_ta +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#bivariate-correlations",
    "href": "07_case_analysis.html#bivariate-correlations",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.6 Bivariate Correlations",
    "text": "7.6 Bivariate Correlations\n\n\nCode\ndata |&gt; \n  select(-subid, -sex) |&gt; \n  cor() |&gt; \n  round(2)\n\n\n        bac    ta   fps sex_c\nbac    1.00 -0.02 -0.19  0.06\nta    -0.02  1.00  0.44 -0.01\nfps   -0.19  0.44  1.00 -0.23\nsex_c  0.06 -0.01 -0.23  1.00",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#bivariate-plots-numeric-predictor",
    "href": "07_case_analysis.html#bivariate-plots-numeric-predictor",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.7 Bivariate Plots (Numeric Predictor)",
    "text": "7.7 Bivariate Plots (Numeric Predictor)\nScatterplots are the preferred visualization when both variables (i.e., the predictor and outcome) are numeric.\nWe also can add a simple line and a LOWESS line (Locally Weighted Scatterplot Smoothing) to help us consider the shape of the relationship.\n\n\nCode\nbivar_bac &lt;- data |&gt;\n    ggplot(aes(x = bac, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n\nbivar_ta &lt;- data |&gt;\n    ggplot(aes(x = ta, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n\nbivar_sex &lt;- data |&gt;\n    ggplot(aes(x = sex_c, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n\n\n\n\n\nCode\nbivar_bac + bivar_ta + bivar_sex",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#bivariate-plots-categorical-predictor",
    "href": "07_case_analysis.html#bivariate-plots-categorical-predictor",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.8 Bivariate Plots (Categorical Predictor)",
    "text": "7.8 Bivariate Plots (Categorical Predictor)\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables.\nLets look at an example of this with sex.\n\n\nCode\n data |&gt;\n    ggplot(aes(x = sex, y = fps)) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#leverage-cartoon-data",
    "href": "07_case_analysis.html#leverage-cartoon-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.9 Leverage (Cartoon Data)",
    "text": "7.9 Leverage (Cartoon Data)\nCheck for high Leverage points\nLeverage is a property of the predictors (DV is not considered for leverage analysis). An observation will have increased leverage on the results as its distance from the mean of all predictors increases.\n\n\n\n\n\n\n\n\n\nQuestion: Which colored points have the most leverage in the 1 predictor example above?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#leverage",
    "href": "07_case_analysis.html#leverage",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.10 Leverage",
    "text": "7.10 Leverage\nHat values (\\(h_i\\)) provide an index of leverage.\nIn the one predictor case:\n\\(h_i = \\frac{1}{N} + \\frac{(X_i- \\overline X)^2}{\\sum(X_j- \\overline X)^2}\\) (for \\(j=1\\) to \\(N\\) in summation)\nWith multiple predictors, \\(h_i\\) measures the distance from the centroid (point of means) of the \\(X\\)s. Hat values are bounded between \\(\\frac{1}{N}\\) and 1.\nMean Hat value = \\(\\frac{P}{N}\\).\nRules of thumb:\n\n\\(h_i &gt; 3* \\overline h\\) for small samples (\\(N &lt; 100\\))\n\\(h_i &gt; 2* \\overline h\\) for large samples\n\nDo not blindly apply rules of thumb. Hat values should be separated from distribution of \\(h_i\\). View a histogram of \\(h_i\\).\nNote: Mahalanobis (Maha) distance = \\((N - 1)(h_i - \\frac{1}{N})\\).\nSPSS reports centered leverage (\\(h - \\frac{1}{N}\\)).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#leverage-cartoon-data-continued",
    "href": "07_case_analysis.html#leverage-cartoon-data-continued",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.11 Leverage (Cartoon Data; Continued)",
    "text": "7.11 Leverage (Cartoon Data; Continued)\nHigh leverage values are not always bad. In fact, in some cases they are good. Must also consider if they are regression outliers.\nQuestion: Why?\n\n\\(R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}\\)\n\\(\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}\\)\n\nHigh leverage points that are fit well by the model increase the difference between \\(\\text{SSE}_{\\text{mean-only}}\\) and \\(\\text{SSE}_a\\), which increases \\(R^2\\).\nHigh leverage points that are fit well also increase variance for predictor. This reduces the SE for predictors and yields more power.\nWell fit, high leverage points do not alter \\(b\\)s.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#leverage-real-data",
    "href": "07_case_analysis.html#leverage-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.12 Leverage (Real Data)",
    "text": "7.12 Leverage (Real Data)\nThere is a function written by John called case_analysis(). It is available to help you identify observations with high leverage.\nSource the function with the following code\n\n\nCode\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/case_analysis.R?raw=true\")\n\n\n\n\n\nCode\ncase_analysis(m_1, Type = \"hatvalues\")\n\n\n\n\n\n\n\n\n\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#regression-outlier-cartoon-data",
    "href": "07_case_analysis.html#regression-outlier-cartoon-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.13 Regression Outlier (Cartoon data)",
    "text": "7.13 Regression Outlier (Cartoon data)\nCheck for regression outliers\nAn observation that is not adequately fit by the regression model (i.e., falls very far from the prediction line).\nIn essence, a regression outlier is a discrepant score with a large residual (\\(e_i\\)).\n\n\n\n\n\n\n\n\n\nQuestion: Which point(s) are regression outliers?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#regression-outlier",
    "href": "07_case_analysis.html#regression-outlier",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.14 Regression Outlier",
    "text": "7.14 Regression Outlier\nThere are multiple quantitative indicators to identify regression outliers, including raw residuals (\\(e_i\\)), standardized residuals (\\(e'_i\\)), and studentized residuals (\\(t'_i\\)). The preferred index is the studentized residual.\n\\(t'_i = \\frac{e_i}{(\\text{SE}_{e(-i)}*\\sqrt{(1-h_i)})}\\)\n\\(t'_i\\) follows a t-distribution with n-P-1 degrees of freedom.\nNOTE: SPSS calls these Studentized Deleted Residuals. Cohen calls these Externally Studentized Residual",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#regression-outlier-cartoon-data-continued",
    "href": "07_case_analysis.html#regression-outlier-cartoon-data-continued",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.15 Regression Outlier (Cartoon data; Continued)",
    "text": "7.15 Regression Outlier (Cartoon data; Continued)\nRegression outliers are always bad but they can have two different types of bad effects.\nQuestion: Why?\n\n\\(R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}\\)\n\\(\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}\\)\n\nRegression outliers increase \\(\\text{SSE}_a\\) which decreases \\(R^2\\). Decreased \\(R^2\\) leads to increased SEs for \\(b\\)s.\nIf outlier also has leverage can alter (increase or decrease) \\(b\\)s.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#regression-outlier-real-data",
    "href": "07_case_analysis.html#regression-outlier-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.16 Regression Outlier (Real data)",
    "text": "7.16 Regression Outlier (Real data)\n\n\nCode\ncase_analysis(m_1, Type = \"residuals\")\n\n\n\n\n\n\n\n\n\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)\n\n\n\n\n\nCode\ncar::outlierTest(m_1, cutoff = .05, labels = data$subid)\n\n\n      rstudent unadjusted p-value Bonferroni p\n0125 -4.307609        0.000041622    0.0039958\n2112  3.765010        0.000294880    0.0283080",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#influence-cartoon-data",
    "href": "07_case_analysis.html#influence-cartoon-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.17 Influence (Cartoon data)",
    "text": "7.17 Influence (Cartoon data)\nAn observation is influential if it substantially alters the fitted regression model (i.e., the coefficients and/or intercept). Two commonly used assessment methods:\n\nCooks distance\ndfBetas\n\n\n\n\n\n\n\n\n\n\nQuestion: Which point(s) have the most influence?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#cooks-distance",
    "href": "07_case_analysis.html#cooks-distance",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.18 Cook’s Distance",
    "text": "7.18 Cook’s Distance\nCook’s distance (\\(\\text{D}_i\\)) provides a single summary statistic to index how much influence each score has on the overall model.\nCooks distance is based on both the outlierness (standardized residual) and leverage characteristics of the observation.\n\\(\\text{D}_i = \\frac{e'^2_i}{P} * \\frac{h_i}{1-h_i}\\)\n\\(\\text{D}_i &gt; \\frac{4}{N-P}\\) has been proposed as a very liberal cutoff (identifies a lot of influential points).\n\\(\\text{D}_i &gt; \\text{qf}(.5, P, N-P)\\) has also been employed as very conservative.\nIdentification of problematic scores should be considered in the context of the overall distribution of \\(\\text{D}_i\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#cooks-distance-real-data",
    "href": "07_case_analysis.html#cooks-distance-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.19 Cook’s Distance (Real data)",
    "text": "7.19 Cook’s Distance (Real data)\n\n\nCode\ncase_analysis(m_1, Type = \"cooksd\")\n\n\n\n\n\n\n\n\n\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#influence-bubble-plot-real-data",
    "href": "07_case_analysis.html#influence-bubble-plot-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.20 Influence Bubble Plot (Real data)",
    "text": "7.20 Influence Bubble Plot (Real data)\n\n\nCode\ncase_analysis(m_1, Type = \"influenceplot\")\n\n\n\n\n\n\n\n\n\n$Rownames\ncharacter(0)\n\n$Values\nnumeric(0)\n\n\nQuestion: What are the expected effects of each of these points on the model?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#dfbetas",
    "href": "07_case_analysis.html#dfbetas",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.21 dfBetas",
    "text": "7.21 dfBetas\n\\(\\text{dfBeta}_{ij}\\) is an index of how much each regression coefficient (\\(j= 0 – k\\)) would change if the \\(i^{\\text{th}}\\) score was deleted.\n\\(\\text{dfBeta}_{ij} = b_j - b_{j(-i)}\\)\ndfBetas (preferred) is the standardized form of the index.\n\\(\\text{dfBetas} = \\frac{\\text{dfBeta}}{\\text{SE}_{b_{j(-i)}}}\\)\n\\(|{\\text{dfBetas}}|&gt;2\\) may be problematic.\n\\(|{\\text{dfBetas}}|&gt; \\frac{2}{\\sqrt{N}}\\) in larger samples (Belsley et al., 1980).\nConsider distribution with histogram! Also can visualize with added variable plot.\nProblem is there can be many dfBetas (a set for each predictor and intercept). Most helpful when there is one critical/focal effect.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#dfbetas-real-data",
    "href": "07_case_analysis.html#dfbetas-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.22 dfBetas (Real data)",
    "text": "7.22 dfBetas (Real data)\n\n\nCode\ncase_analysis(m_1, Type = \"dfbetas\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#added-variable-plot-real-data",
    "href": "07_case_analysis.html#added-variable-plot-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.23 Added Variable Plot (Real data)",
    "text": "7.23 Added Variable Plot (Real data)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#impact-on-ses",
    "href": "07_case_analysis.html#impact-on-ses",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.24 Impact on SEs",
    "text": "7.24 Impact on SEs\nIn addition to altering regression coefficients (and reducing \\(R^2\\)), problematic scores can increase the SEs (i.e., precision of estimation) of the regression coefficients.\nCOVRATIO is an index that indicates how individual scores affect the overall precision of estimation (joint confidence region for set of coefficients) of the regression coefficients.\nObservations that decrease the precision of estimation have COVRATIOS &lt; 1.0.\nBelsley et al., (1980) proposed a cut off of:\n\\(\\text{COVRATIO}_i &lt; |3* \\frac{P}{N} - 1|\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#impact-on-ses-real-data",
    "href": "07_case_analysis.html#impact-on-ses-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.25 Impact on SEs (Real data)",
    "text": "7.25 Impact on SEs (Real data)\n\n\nCode\ncase_analysis(m_1, Type = \"covratio\")\n\n\n\n\n\n\n\n\n\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#four-examples-with-fake-data",
    "href": "07_case_analysis.html#four-examples-with-fake-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.26 Four Examples with Fake Data",
    "text": "7.26 Four Examples with Fake Data",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#enter-the-real-world",
    "href": "07_case_analysis.html#enter-the-real-world",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.27 Enter the Real World",
    "text": "7.27 Enter the Real World\nQuestion: So what do you do?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#overall-impact-of-problem-scores-real-data",
    "href": "07_case_analysis.html#overall-impact-of-problem-scores-real-data",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.28 Overall Impact of Problem Scores: Real Data",
    "text": "7.28 Overall Impact of Problem Scores: Real Data\n\n\nCode\nbroom::tidy(m_1)\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)   18.9      7.47        2.53 0.0132    \n2 bac         -165.      84.6        -1.95 0.0545    \n3 ta             0.152    0.0316      4.82 0.00000570\n4 sex_c        -16.0      6.67       -2.40 0.0184    \n\n\nSSE = 97756.43\n\n\n\nCode\ndata_rm_outliers &lt;- data |&gt; \n  filter(!subid %in% c(\"0125\", \"2112\"))\n\nm_2 &lt;- lm(fps ~ bac + ta + sex_c, data = data_rm_outliers) \n\nbroom::tidy(m_2)\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   26.6      6.44        4.13 0.0000825\n2 bac         -229.      72.4        -3.16 0.00214  \n3 ta             0.126    0.0273      4.61 0.0000135\n4 sex_c        -15.5      5.70       -2.72 0.00790  \n\n\nSSE = 68262.98",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "07_case_analysis.html#what-to-do",
    "href": "07_case_analysis.html#what-to-do",
    "title": "7  Dealing with Messy Data I: Case Analysis",
    "section": "7.29 What to Do",
    "text": "7.29 What to Do\n\nDon’t worry about leverage alone.\nWorry about model outliers always.\nWorry about influence (overall model or predictors by field).\nDrop, retain, or bring model outliers to the fence.\nDrop or retain influential participants.\nReport both ways (in olden day…).\nGet with the program and pre-register instead (what you worry about, how you define it, and what you do)!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dealing with Messy Data I: Case Analysis</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html",
    "href": "08_model_assumptions.html",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "",
    "text": "8.1 Goals of Unit",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#goals-of-unit",
    "href": "08_model_assumptions.html#goals-of-unit",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "",
    "text": "Understand 5 assumptions of GLMs.\nUnderstand consequences of assumption violations.\n\nInefficient standard errors (low power, type II errors).\nInaccurate standard errors (incorrect statistical tests, type I errors).\nInaccurate parameter estimates.\n\nLearn how to detect violations (statistical tests and visual diagnosis).\nRecognize the range of options available when violations are detected. Implementation of these options will be covered over the semester.\nExamination of assumptions will further inform you about your data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#assessment-of-assumptions-for-sig.-tests",
    "href": "08_model_assumptions.html#assessment-of-assumptions-for-sig.-tests",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.2 Assessment of Assumptions for Sig. Tests",
    "text": "8.2 Assessment of Assumptions for Sig. Tests\nAll GLM procedures commonly make the 5 assumptions below. When these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). With the exception of \\(\\#1\\), these assumptions are expressed (and assessed) with respect to the residuals around the prediction line.\n\nExact X: The IVs are assumed to be known exactly (i.e., without measurement error).\nIndependence: Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).\nNormality: All residual distributions are normally distributed.\nConstant variance: All residual distributions have a constant variance, \\(\\text{SEE}^2\\).\nLinearity: All residual distributions (i.e., for each \\(Y'\\)) are assumed to have means equal to zero.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#exact-x",
    "href": "08_model_assumptions.html#exact-x",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.3 Exact X",
    "text": "8.3 Exact X\nViolations of the Exact X assumption lead to biased (i.e., inaccurate) estimates of regression coefficients.\nViolations are caused by problems with reliability of measurement of your predictors.\nQuestion: In simple, bivariate regression, how will reducing reliability affect the regression model?\n\n\n\n\n\n\n\n\n\n\nIt will reduce \\(b_1\\). We will underestimate the strength of relationship between \\(X\\) and \\(Y\\).\nIn multiple predictor models, the bias can be either positive or negative based on the nature of the correlations among the predictors. Use reliable variables!.\nQuestion: What are the implications of unreliable \\(X\\) for the use of covariates to control variables?\n\nCovariates only control from the construct they measure to the degree that they are reliable (and valid) measures of that construct.\nAnalysis that rely on unreliable covariates are not controlling the variance for the construct well.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#independence",
    "href": "08_model_assumptions.html#independence",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.4 Independence",
    "text": "8.4 Independence\nViolations of the independence of residuals assumption can compromise the validity of our statistical tests (inaccurate standard errors).\nViolations of residuals independence is a function of the research design caused by repeated measures on the same individual or related individuals/observations (participants in same family, school, etc).\nOften difficult to detect in data but clear from research design.\nCan be fixed by a variety of approaches including repeated measures analyses (later this semester) or multi-level, mixed effects, and/or hierarchical linear models (next semester).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#assessment-of-residuals-general-issues",
    "href": "08_model_assumptions.html#assessment-of-residuals-general-issues",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.5 Assessment of Residuals: General Issues",
    "text": "8.5 Assessment of Residuals: General Issues\nRemaining three assumptions (Normality distributed residuals with a mean of 0 and constant variance) can be assessed via examination of the residuals.\nUse of Graphical Methods is emphasized.\nStatistical tests of assumptions exist but should be used cautiously.\nAssessment of assumptions about residuals is an inexact science: Conclusions are tentative.\nThe process of examining residuals will increase your understanding of your data. - May suggest transformations of your data. - May suggest alternative analytic strategies. - Will increase your confidence in your conclusions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#residuals-by-haty-normal-m-0-s-constant",
    "href": "08_model_assumptions.html#residuals-by-haty-normal-m-0-s-constant",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.6 Residuals by \\(\\hat{Y}\\): Normal M = 0, S = Constant",
    "text": "8.6 Residuals by \\(\\hat{Y}\\): Normal M = 0, S = Constant\n\n\nCode\nnorm_tbl &lt;- tibble(`Y'` = rep(1:25, 1000),\n                   `E'` = rnorm(25000, mean = 0, sd = 1))\n\nnorm_tbl |&gt; \n  ggplot(aes(x = `Y'`,\n             y = `E'`)) +\n  geom_jitter(alpha = .4, width = .75, height = 0, size = .5) +\n  geom_hline(yintercept = 0, color = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = 10, color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 15, color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 20, color = \"red\", linewidth = 1) +\n  scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist_10 &lt;- norm_tbl |&gt; \n  filter(`Y'` == 10) |&gt; \n  ggplot(aes(x = `E'`)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"Y' = 10\")\n\n\nhist_15 &lt;- norm_tbl |&gt; \n  filter(`Y'` == 15) |&gt; \n  ggplot(aes(x = `E'`)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"Y' = 15\")\n\n\nhist_20 &lt;- norm_tbl |&gt; \n  filter(`Y'` == 20) |&gt; \n  ggplot(aes(x = `E'`)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"Y' = 20\")\n\nhist_10 + hist_15 + hist_20",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#normally-distributed-errors",
    "href": "08_model_assumptions.html#normally-distributed-errors",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.7 Normally Distributed Errors",
    "text": "8.7 Normally Distributed Errors\nThe errors for each \\(\\hat{Y}\\) are assumed to be normally distributed. Normally distributed errors are required for OLS regression coefficients to be MVUE but not BLUE.\nCentral limit theory indicates that even with non-normal errors, significance tests and confidence intervals are approximately correct with large \\(N\\).\nCoefficients are still best unbiased efficient estimators among linear solutions (i.e., BLUE) but more efficient non-linear solutions may exist (e.g., Generalized Linear Models such as Poisson regression for thick tailed distributions).\nMean may not be best measure of center of a highly skewed distribution.\nMultimodal error distributions suggest the omission of one or more categorical variables that divide the data into groups.\nTransformations may correct shape of residuals (Unit 9).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#normality-quantile-comparison-density-plots",
    "href": "08_model_assumptions.html#normality-quantile-comparison-density-plots",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.8 Normality: Quantile Comparison & Density Plots",
    "text": "8.8 Normality: Quantile Comparison & Density Plots\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(car) # needed for model_assumptions\nlibrary(psych) # needed for model_assumptions\nlibrary(gvlma) # needed for model_assumptions\n\ntheme_set(theme_classic()) \n\npath_data &lt;- \"data_lecture\"\n\ndata &lt;- read_csv(here::here(path_data, \"7_three_predictors_fps.csv\"), \n                 show_col_types = FALSE) |&gt; \n   mutate(sex_c = if_else(sex == \"female\", -.5, .5))\n\n\nWe have a second function written by John called model_assumptions() that you can source to help test model assumptions.\n\n\nCode\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/model_assumptions.R?raw=true\")\n\n\n\nLets refit our last model from the previous section.\n\n\nCode\ndata_rm_outliers &lt;- data |&gt; \n  filter(!subid %in% c(\"0125\", \"2112\"))\n\nm_2 &lt;- lm(fps ~ bac + ta + sex_c, data = data_rm_outliers) \n\n\n\n\n\nCode\nmodel_assumptions(m_2, Type = \"normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics for Studentized Residuals\n\n\nCode\npsych::describe(rstudent(m_2))  \n\n\n   vars  n mean   sd median trimmed  mad   min  max range skew kurtosis   se\nX1    1 94    0 1.02  -0.16   -0.02 0.83 -3.06 2.54   5.6 0.11      0.6 0.11",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#normality-q-q-plot-examples",
    "href": "08_model_assumptions.html#normality-q-q-plot-examples",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.9 Normality: q-q Plot Examples",
    "text": "8.9 Normality: q-q Plot Examples",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#constant-variance",
    "href": "08_model_assumptions.html#constant-variance",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.10 Constant Variance",
    "text": "8.10 Constant Variance\nThe errors for each \\(\\hat{Y}\\) are assumed have a constant variance (homoscedasticity). This is necessary for the OLS estimated coefficients to be BLUE.\nIf the errors are heteroscedastic, the coefficients remain unbiased but the efficiency (precision of estimation) is impaired and the coefficient SEs become inaccurate. The degree of the problem depends on severity of violation and sample size.\nRough rule is that estimation is seriously degraded if the ratio of largest to smallest variance is 10 or greater (or more conservatively, 4 or greater)\n\nTransformations may fix this issue (next unit).\nWeighted Least Squares provides an alternative to estimation when heteroscedasticity exists (maybe next semester?).\nCorrections also exist for SEs when errors are heteroscedastic (more on this in a moment).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#constant-variance-residual-and-spread-level-plots",
    "href": "08_model_assumptions.html#constant-variance-residual-and-spread-level-plots",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.11 Constant Variance: Residual and Spread-Level Plots",
    "text": "8.11 Constant Variance: Residual and Spread-Level Plots\n\n\nCode\nmodel_assumptions(m_2, Type = \"constant\")\n\n\n\n\n\n\n\n\n\n\n\n\nA spread level plot is a plot of the log(abs(studentized residuals) vs. log(fitted values). \\(1-b\\) (from the regression line below) is the suggested power transformation for \\(Y\\) to stabilize variance.\n\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.2555876",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#constant-variance-statistical-test",
    "href": "08_model_assumptions.html#constant-variance-statistical-test",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.12 Constant Variance: Statistical Test",
    "text": "8.12 Constant Variance: Statistical Test\nBreusch & Pagan (1979)1 and Cook & Weisberg (1983)2 independently developed test for constant variance.\nncvTest() in car package. Also provided by model_assumptions() as well.\nDo not use it blindly. All statistical tests are designed to be sensitive to specific types of violations. May miss other types of violations.May also provide false positive due to other aspects of the distribution.\n\n\nCode\ncar::ncvTest(m_2)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 21.89294, Df = 1, p = 0.0000028829",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#constant-variance-correcting-ses",
    "href": "08_model_assumptions.html#constant-variance-correcting-ses",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.13 Constant Variance: Correcting SEs",
    "text": "8.13 Constant Variance: Correcting SEs\nStandard errors are inaccurate when variance of residuals is not constant. A procedure to provide White (1980)3 corrected SEs is described in Fox (2008), chapter 12, pp 275-276.\nUncorrected Tests of Coefficients\n\n\nCode\nbroom::tidy(m_2)\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   26.6      6.44        4.13 0.0000825\n2 bac         -229.      72.4        -3.16 0.00214  \n3 ta             0.126    0.0273      4.61 0.0000135\n4 sex_c        -15.5      5.70       -2.72 0.00790  \n\n\nWhite (1980) Heteroscedasticity-corrected SEs and Tests\n\n\nCode\ncorrected_ses &lt;- sqrt(diag(hccm(m_2)))\n\nbroom::tidy(m_2) |&gt; \n  select(term, estimate) |&gt; \n  add_column(std.error = corrected_ses) |&gt; \n  mutate(statistic = estimate/std.error,\n         p.value = 2*(pt(abs(statistic), df=m_2$df.residual, lower.tail=FALSE)))\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   26.6      6.90        3.85 0.000220 \n2 bac         -229.      72.1        -3.18 0.00204  \n3 ta             0.126    0.0304      4.13 0.0000799\n4 sex_c        -15.5      5.70       -2.72 0.00791",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#linearity-component-residuals-plots",
    "href": "08_model_assumptions.html#linearity-component-residuals-plots",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.14 Linearity: Component + Residuals Plots",
    "text": "8.14 Linearity: Component + Residuals Plots\nIf Linearity assumption is not met, coefficients are biased.\nPlot partial residual (\\(e_{i(j)} = e_i + b_jX_{ij}\\)) by each predictor.\nCan include factors but can not include interactions with factors. Code regressors manually.\n\n\nCode\nmodel_assumptions(m_2, Type='linear')",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#global-test-of-model-assumptions",
    "href": "08_model_assumptions.html#global-test-of-model-assumptions",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.15 Global Test of Model Assumptions",
    "text": "8.15 Global Test of Model Assumptions\nPena and Slate (2006) validated a global test of linear model assumptions.4\nProvided by model_assumptions() using gvlma package\n\n\nCode\ngvlma(m_2)\n\n\n\nCall:\nlm(formula = fps ~ bac + ta + sex_c, data = data_rm_outliers)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n    26.5528    -228.8721       0.1256     -15.4874  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = m_2) \n\n                    Value  p-value                   Decision\nGlobal Stat        9.8263 0.043458 Assumptions NOT satisfied!\nSkewness           0.2921 0.588886    Assumptions acceptable.\nKurtosis           1.0393 0.307987    Assumptions acceptable.\nLink Function      0.1898 0.663091    Assumptions acceptable.\nHeteroscedasticity 8.3051 0.003953 Assumptions NOT satisfied!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#transformations-the-family-of-powers-roots",
    "href": "08_model_assumptions.html#transformations-the-family-of-powers-roots",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.16 Transformations: The Family of Powers & Roots",
    "text": "8.16 Transformations: The Family of Powers & Roots\n\nPower transformations (next unit) are very useful for correcting problems with normality, constant, variance, and linearity of errors.\nPolynomial regression (PSY 710) is useful when you have quadratic, cubic, etc. effects of \\(X\\)s on \\(Y\\).\nGeneralized linear models (e.g., Logistic regression; PSY 710) are also available.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#summary-of-violation-consequences-and-solutions",
    "href": "08_model_assumptions.html#summary-of-violation-consequences-and-solutions",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "8.17 Summary of Violation Consequences and Solutions",
    "text": "8.17 Summary of Violation Consequences and Solutions\n\nExact X\n\nInaccurate (biased) parameter estimates.\nUse reliable measures of \\(X\\)s, use SEM with latent variables.\n\nIndependence\n\nInaccurate standard errors.\nUse repeated measures, use multi-level models.\n\nNormally distributed errors\n\nInefficient standard errors.\nConsider omitted variables, use transformations, use generalized linear models.\n\nConstant variance for errors\n\nInaccurate and inefficient standard errors.\nUse SE corrections (but still inefficient), use transformations, use weighted least squares.\n\nLinearity (Error distributions all have mean of 0)\n\nInaccurate (biased) parameter estimates.\nUse transformations, use polynomial regression, use generalized linear models.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "08_model_assumptions.html#footnotes",
    "href": "08_model_assumptions.html#footnotes",
    "title": "8  Dealing with Messy Data II: Model Assumptions",
    "section": "",
    "text": "Breusch,T. S. and Pagan,A. R. (1979) A simple test for heteroscedasticity and random coefficient variation. Econometrica, 47, 1287-1294.↩︎\nCook,R. D. and Weisberg,S. (1983) Diagnostics for heteroscedasticity in regression. Biometrika, 70, 1-10.↩︎\nLong,J.S. and Ervin,L.H. (2000) Using heteroscedasity consistent standard errors in the linear regression model. The American Statistician 54, 217–224.;cWhite,H. (1980) A heterskedastic consistent covariance matrix estimator and a direct test of heteroskedasticity. Econometrica 48, 817–838.↩︎\nPena,E.A. and Slate,E.H. (2006). Global validation of linear model assumptions, Journal of the American Statistical Association, 101, 341-354.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dealing with Messy Data II: Model Assumptions</span>"
    ]
  },
  {
    "objectID": "09_transformations.html",
    "href": "09_transformations.html",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "",
    "text": "9.1 Transformations: The Family of Power and Roots\nThe Box-Cox family of modified power transformations, \\(X^{(p)}= (X^p - 1)/p\\), for values of \\(p = -1, 0, 1, 2, 3\\). When \\(p = 0, X^{(p)} = log_e X\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#transformations-the-family-of-power-and-roots",
    "href": "09_transformations.html#transformations-the-family-of-power-and-roots",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "",
    "text": "The GLM makes strong assumptions about the structure of data, assumptions which often fail to hold in practice.\nOne solution is to abandon the GLM for more complicated models (generalized linear models; weighted least squares; robust regression).\nAnother solution is to transform the data (either the \\(X\\)s or \\(Y\\)) so that they conform more closely to the assumptions.\nA particularly useful group of transformations is the family of powers and roots:\n\\(X\\) → \\(X^p\\)\n\nIf p is negative, then the transformation is an inverse power: \\(X^{-1} = \\frac{1}{X}\\), and \\(X^{−2} = \\frac{1}{X^2}\\)\nIf p is a fraction, then the transformation represents a root: \\(X^{\\frac{1}{2}} = \\sqrt{X}\\), and \\(X^{-\\frac{1}{2}} = \\frac{1}{\\sqrt{X}}\\)\n\n\n\n\nThe Box-Cox family of transformations provides a comparable but more convenient form (in some cases*).\n\\(X\\) → \\(X^{(p)} \\equiv \\frac{X^p- 1}{p}\\)\nSince \\(X^{(p)}\\) is a linear function of \\(X^p\\), the 2 transformations have the same essential effect on the data.\n\nDividing by p preserves the direction of \\(X\\), which otherwise would be reversed when p is negative.\n\nThis function also matches level and slope of curve at \\(X=1\\).\n\n\n\n\n\n\nThe power transformation \\(X^{(0)}\\) is useless, but the very useful log transformation is a kind of zeroth power:\nlim \\(p\\) → \\(0\\) \\(\\frac{X^p - 1}{p} = \\text{log}_eX\\)\nwhere \\(e \\approx 2.718\\) is the base of the natural logarithms. Thus, we will take \\(X^{(0)} \\equiv log(X)\\).\nIt is generally more convenient to use logs to the base 10 or base 2, which are more easily interpreted than logs to the base e.\nChanging bases is equivalent to multiplying your variable by a constant. No effect on significance tests.\n\n\n\nDescending the ladder of powers and roots from \\(p = 1\\) (i.e., no transformation) towards \\(p = -2\\) compresses the large values of X and spreads out the small ones.\nAscending the ladder of powers from \\(p = 1\\) towards \\(p = 3\\) has the opposite effect.\n\n\n\nPower transformations are sensible only when all of the values of \\(X\\) are positive:\n\nSome of the transformations, such as log (\\(p= 0\\)) and square root (\\(p= .5\\)), are undefined for negative or zero values of \\(X\\).\nPower transformations are not monotone (i.e., they change to order of scores) when there are both positive and negative values among the data.\nWe can add a positive constant (called a start) to each data value to make all of the values positive:\n\\(X\\) → \\((X + s)^{(p)}\\)\n\n\n\n\nPower transformations are effective only when the ratio of the biggest data values to the smallest ones is sufficiently large; if this ratio is close to 1, then power transformations are nearly linear. For example:\n\nPower transformations will work well if range of \\(X = 1 – 100\\).\nPower transformations will have little effect if range of \\(X = 1000 – 1100\\)\n\nUsing a negative start can often increase the ratio of highest/lowest score.\nUsing reasonable starts, if necessary, an adequate power transformation can usually be found in the range \\(−2 \\le p \\le 3\\).\n\n\n\nPower transformations of \\(Y\\) (or sometimes \\(X\\)) can correct problems with normality of errors.\nPower transformations of \\(Y\\) (or sometimes \\(X\\)) can stabilize the variance of the errors.\nPower transformations of \\(X\\) (or sometimes \\(Y\\)) can make many nonlinear relationships more nearly linear.\nYou can experiment with Box-Cox transformations of \\(X\\) or \\(Y\\) in R using bcPower() in the car package.\nIn many fields, \\(X^p\\) rather than \\(X^{(p)}\\) may be preferred. Particularly, if \\(p \\ge 0\\). This is simply done algebraically.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#transformations-dealing-with-skew",
    "href": "09_transformations.html#transformations-dealing-with-skew",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.2 Transformations: Dealing with Skew",
    "text": "9.2 Transformations: Dealing with Skew\n\nTransforming \\(Y\\) down the ladder can correct positive skew in the errors (most common problem).\nTransforming \\(Y\\) up the ladder corrects negative skew in the errors.\n\n\n\n\nCode\nset.seed(102030)\ny &lt;- tibble(y_raw = rchisq(n=500, df=1),\n            y_.5 = car::bcPower(y_raw, .5),\n            y_.25 = car::bcPower(y_raw, .25),\n            y_0 = car::bcPower(y_raw, 0))    \n\nplot_raw &lt;- y |&gt; \n  ggplot(aes(x = y_raw)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-.5, 12), breaks = c(0, 2, 4, 6, 8, 10)) +\n  labs(title = \"Raw Y\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_raw)$bw, 4)))\n\nplot_.5 &lt;- y |&gt; \n  ggplot(aes(x = y_.5)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-3, 5), breaks = c(-2, 0, 2, 4)) +\n  labs(title = \"p = .5; sqrt(Y)\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_.5)$bw, 4)))\n\nplot_.25 &lt;- y |&gt; \n  ggplot(aes(x = y_.25)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-4.5, 4), breaks = c(-4, -2, 0, 2, 4)) +\n  labs(title = \"p = .25\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_.25)$bw, 4)))\n\nplot_0 &lt;- y |&gt; \n  ggplot(aes(x = y_0)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-15, 4), breaks = c(-15, -10. -5, 0)) +\n  labs(title = \"p = 0; log(Y)\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_0)$bw, 4)))\n\n\n(plot_raw + plot_.5) / (plot_.25 + plot_0)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#transformations-dealing-with-heteroscedasticity",
    "href": "09_transformations.html#transformations-dealing-with-heteroscedasticity",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.3 Transformations: Dealing with Heteroscedasticity",
    "text": "9.3 Transformations: Dealing with Heteroscedasticity\n\nTransforming \\(Y\\) down the ladder can correct problems with increasing spread of errors as \\(Y\\) increases (most common problem).\nTransforming \\(Y\\) up the ladder corrects decreasing spread.\nThe problems of unequal spread and skewness commonly occur together and can be corrected together. Therefore, transforming \\(Y\\) down the ladder can correct both issues simultaneously.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(car) # needed for model_assumptions\nlibrary(psych) # needed for model_assumptions\nlibrary(gvlma) # needed for model_assumptions\n\npath_data &lt;- \"data_lecture\"\ndata &lt;- read_csv(here::here(path_data, \"9_transformations_fps.csv\"), \n                 show_col_types = FALSE) |&gt; \n   mutate(sex_c = if_else(sex == \"female\", -.5, .5)) |&gt; \n   # remove outliers to fit same model as last two units \n   filter(!subid %in% c(\"0125\", \"2112\"))\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/model_assumptions.R?raw=true\")\n\n\n\n\nCode\nm &lt;- lm(fps ~ bac + ta + sex_c, data = data)\n\n\n\n\nCode\nmodel_assumptions(m, Type = \"normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_assumptions(m, Type = \"constant\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.2370356 \n\n\n\n\nCode\ncar::ncvTest(m)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 13.2232, Df = 1, p = 0.00027651\n\n\n\n\n\nCode\ngvlma(m)\n\n\n\nCall:\nlm(formula = fps ~ bac + ta + sex_c, data = data)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n    28.7027    -254.5894       0.1231     -15.8550  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = m) \n\n                        Value p-value                   Decision\nGlobal Stat        9.39689053 0.05191    Assumptions acceptable.\nSkewness           3.60145982 0.05773    Assumptions acceptable.\nKurtosis           0.30834234 0.57870    Assumptions acceptable.\nLink Function      0.00002503 0.99601    Assumptions acceptable.\nHeteroscedasticity 5.48706334 0.01916 Assumptions NOT satisfied!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#box-cox-transformation-function",
    "href": "09_transformations.html#box-cox-transformation-function",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.4 Box Cox Transformation Function",
    "text": "9.4 Box Cox Transformation Function\n\n\nCode\ndata |&gt; \n  select(bac, ta, sex_c, fps) |&gt; \n  psych::describe() |&gt; \n  select(-c(trimmed, mad, se))\n\n\n      vars  n   mean     sd median    min    max  range  skew kurtosis\nbac      1 94   0.06   0.04   0.06   0.00   0.14   0.14 -0.09    -1.08\nta       2 94 145.91 104.80 119.00  10.00 445.00 435.00  0.91     0.03\nsex_c    3 94   0.01   0.50   0.50  -0.50   0.50   1.00 -0.04    -2.02\nfps      4 94  32.19  32.77  19.46 -26.07 136.82 162.89  0.95     0.33\n\n\nWe are going to first add a constant so all response values are &gt; 0\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(fps_27 = fps + 27)\n\nm_2 &lt;- lm(fps_27 ~ bac + ta + sex_c, data = data)\n\n\n\nNext we will pull the best lambda value from a plot of log-likelihood values by lambda power transformations of response variable.\n\n\nCode\ncar::boxCox(m_2)\n\nround(car::boxCox(m_2)$x[which.max(car::boxCox(m_2)$y)],2)\n\n\n\n\n\n\n\n\n\n[1] 0.55\n\n\n\nLastly, we will use the best lambda value to conduct our power transformation and re-evaluate our model assumptions.\n\n\nCode\ndata &lt;- data |&gt; \n  mutate(fps_bc = car::bcPower(fps_27, .55))\n\nm_3 &lt;- lm(fps_bc ~ bac + ta + sex_c, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  -0.5446104 \n\n\n\n\nCode\ncar::ncvTest(m_3)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 6.005664, Df = 1, p = 0.01426\n\n\n\n\n\nCode\ngvlma(m_3)\n\n\n\nCall:\nlm(formula = fps_bc ~ bac + ta + sex_c, data = data)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n   14.21548    -37.52098      0.01793     -2.70290  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = m_3) \n\n                       Value p-value                   Decision\nGlobal Stat        12.759981 0.01251 Assumptions NOT satisfied!\nSkewness            1.588199 0.20758    Assumptions acceptable.\nKurtosis            5.520612 0.01879 Assumptions NOT satisfied!\nLink Function       0.004907 0.94416    Assumptions acceptable.\nHeteroscedasticity  5.646263 0.01749 Assumptions NOT satisfied!\n\n\nTransformations often don’t help!\n\n\n\nCode\nm_3 |&gt; \n  broom::tidy()\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  14.2      1.03        13.8  6.26e-24\n2 bac         -37.5     11.5         -3.25 1.62e- 3\n3 ta            0.0179   0.00436      4.11 8.62e- 5\n4 sex_c        -2.70     0.910       -2.97 3.82e- 3\n\n\n\nWhite (1980) Heteroscedasticity-corrected SEs and Tests\n\n\nCode\ncorrected_ses &lt;- sqrt(diag(hccm(m)))\n\nbroom::tidy(m) |&gt; \n  select(term, estimate) |&gt; \n  add_column(std.error = corrected_ses) |&gt; \n  mutate(statistic = estimate/std.error,\n         p.value = 2*(pt(abs(statistic), df=m_2$df.residual, lower.tail=FALSE)))\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   28.7      6.91        4.15 0.0000744\n2 bac         -255.      71.3        -3.57 0.000575 \n3 ta             0.123    0.0320      3.85 0.000224 \n4 sex_c        -15.9      5.87       -2.70 0.00826  \n\n\nMaybe better choicer was untransformed Y (original model) with corrected SEs?\nMaybe the problem wasn’t bad enough to do anything?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#transformations-dealing-with-non-linearity",
    "href": "09_transformations.html#transformations-dealing-with-non-linearity",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.5 Transformations: Dealing with Non-linearity",
    "text": "9.5 Transformations: Dealing with Non-linearity\nPower transformations can make simple monotone relationships more linear (Fig A). Polynomial regression (or other transformations, e.g. logit) is often needed for more complex relationships (Figs, B & C).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#transformations-mosteller-and-tukeys-bulging-rule",
    "href": "09_transformations.html#transformations-mosteller-and-tukeys-bulging-rule",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.6 Transformations: Mosteller and Tukey’s bulging rule",
    "text": "9.6 Transformations: Mosteller and Tukey’s bulging rule\nSimple monotone relationships can be corrected by transforming \\(X\\) or \\(Y\\).\n\n\\(X\\) is typical if only one \\(XY\\) relationship is problematic.\nIf all \\(X\\)s have similar non-linear relationship, transform \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What 2 transformations would make this relationship linear?\n\n\n\n\n\n\n\n\n\n\nMove \\(X\\) down the ladder (e.g., \\(X^.5\\))\n\n\nCode\ndata_nonlinear  |&gt; \n  ggplot(aes(x = sqrt(X), y = Y)) +\n  geom_point(alpha = .4)\n\n\n\n\n\n\n\n\n\nMove \\(Y\\) up the ladder (e.g., \\(Y^2\\))\n\n\nCode\ndata_nonlinear  |&gt; \n  ggplot(aes(x = X, y = Y^2)) +\n  geom_point(alpha = .4)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#general-transformations",
    "href": "09_transformations.html#general-transformations",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.7 General Transformations",
    "text": "9.7 General Transformations\n\nIn some fields, certain power transformations (sqrt, log, inverse) are common.\nIf we have a choice between transformations that perform roughly equally well, we may prefer one transformation to another because of interpretability:\n\nThe log transformation has a convenient multiplicative interpretation (e.g., increasing log2 (X) by 1 doubles X; increasing log10 (X) by 1 multiples X by 10).\n\nTransformations are a big source of research dfs. Should be pre-registered. You may know what your DV typically needs from prior data. Worst case, report with and without transformations and justify.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#k-race-times",
    "href": "09_transformations.html#k-race-times",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.8 5K Race Times",
    "text": "9.8 5K Race Times\n\n\nCode\ndata_5k &lt;- read_csv(here::here(path_data, \"9_transformations_5k.csv\"),\n                    show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 80\nColumns: 4\n$ subid &lt;chr&gt; \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\"…\n$ time  &lt;dbl&gt; 24.91, 21.82, 21.54, 23.03, 25.35, 22.84, 30.16, 27.00, 16.42, 2…\n$ age   &lt;dbl&gt; 29, 25, 27, 25, 37, 31, 43, 44, 46, 53, 58, 30, 27, 36, 32, 45, …\n$ miles &lt;dbl&gt; 24.99984, 30.80905, 52.04042, 66.20958, 26.60005, 48.21255, 12.3…\n\n\n\n\nCode\nm_5k &lt;- lm(time ~ age + miles, data = data_5k)\n\nbroom::tidy(m_5k)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   25.0      1.80       13.9  1.07e-22\n2 age            0.172    0.0333      5.17 1.77e- 6\n3 miles         -0.210    0.0246     -8.53 9.54e-13\n\n\n\n\n\nCode\nmodel_assumptions(m_5k, Type = \"normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_assumptions(m_5k, Type = \"constant\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.4894249 \n\n\n\n\nCode\ncar::ncvTest(m_5k)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.8077899, Df = 1, p = 0.36877\n\n\n\n\n\nCode\nmodel_assumptions(m_5k, Type='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngvlma(m_5k)\n\n\n\nCall:\nlm(formula = time ~ age + miles, data = data_5k)\n\nCoefficients:\n(Intercept)          age        miles  \n    24.9804       0.1724      -0.2097  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = m_5k) \n\n                      Value   p-value                   Decision\nGlobal Stat        13.01039 0.0112252 Assumptions NOT satisfied!\nSkewness            0.01435 0.9046389    Assumptions acceptable.\nKurtosis            0.07603 0.7827462    Assumptions acceptable.\nLink Function      12.82570 0.0003419 Assumptions NOT satisfied!\nHeteroscedasticity  0.09430 0.7587759    Assumptions acceptable.\n\n\n\nLet’s try transforming miles using log2(), a binary logarithm (base 2).\n\n\nCode\ndata_5k &lt;- data_5k |&gt; \n  mutate(log_miles = log2(miles))\n\nm_5k_tran &lt;- lm(time ~ age + log_miles, data = data_5k)\n\nbroom::tidy(m_5k_tran)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   42.5      3.25       13.1  2.88e-21\n2 age            0.170    0.0320      5.32 9.97e- 7\n3 log_miles     -4.98     0.538      -9.26 3.73e-14\n\n\n\n\n\nCode\nmodel_assumptions(m_5k_tran, Type='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngvlma(m_5k_tran)\n\n\n\nCall:\nlm(formula = time ~ age + log_miles, data = data_5k)\n\nCoefficients:\n(Intercept)          age    log_miles  \n     42.519        0.170       -4.983  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = m_5k_tran) \n\n                     Value p-value                Decision\nGlobal Stat        1.83180  0.7667 Assumptions acceptable.\nSkewness           0.15584  0.6930 Assumptions acceptable.\nKurtosis           0.06402  0.8003 Assumptions acceptable.\nLink Function      1.58986  0.2073 Assumptions acceptable.\nHeteroscedasticity 0.02209  0.8819 Assumptions acceptable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "09_transformations.html#displaying-transformed-results-with-fake-data",
    "href": "09_transformations.html#displaying-transformed-results-with-fake-data",
    "title": "9  Dealing with Messy Data III: Transformations",
    "section": "9.9 Displaying Transformed Results With Fake Data",
    "text": "9.9 Displaying Transformed Results With Fake Data\n\n\nCode\ndata_fake &lt;- tibble(x = 3 * rchisq(200, df=3),\n                    x_sr = sqrt(x),\n                    y = 3 * x_sr + rnorm(200,mean=0,sd = 1))\n\nm_raw = lm(y ~ x, data = data_fake)\nbroom::tidy(m_raw)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    3.67     0.148       24.9 7.86e-63\n2 x              0.519    0.0147      35.3 2.66e-87\n\n\n\n\nCode\ndata_fake |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_abline(intercept = coef(m_raw)[1], slope = coef(m_raw)[2], color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nm_sr = lm(y ~ x_sr, data = data_fake)\nbroom::tidy(m_sr)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -0.151    0.175     -0.863 3.89e-  1\n2 x_sr           3.06     0.0624    49.1   9.18e-113\n\n\n\n\nCode\ndata_fake |&gt; \n  ggplot(aes(x = x_sr, y = y)) +\n  geom_point(alpha = .4) +\n  geom_abline(intercept = coef(m_sr)[1], slope = coef(m_sr)[2], color = \"red\")\n\n\n\n\n\n\n\n\n\n\nYou can display results from linear model but provide scale for raw \\(X\\) instead of Sqrt(\\(X\\)) or in addition to Sqrt(\\(X\\)) on another axis.\n\n\nCode\npreds &lt;- tibble(x_sr = seq(min(data_fake$x_sr),max(data_fake$x_sr), by=.01))\ny_pred = predict(m_sr, newdata = preds)\n\nggplot() +\n  geom_point(aes(x = data_fake$x_sr, y = data_fake$y), alpha = .4) +\n  geom_line(aes(x = preds$x_sr, y = y_pred)) +\n  ylab(\"Y\") +\n  scale_x_continuous(name = \"SQRT(X)\", sec.axis = sec_axis(~.^2, name = \"Raw X\"))\n\n\n\n\n\n\n\n\n\n\nYou can display results from linear model in Raw \\(X\\) Units and use predict() to get the \\(Y\\)s based on transformed \\(X\\).\n\n\nCode\npreds &lt;- tibble(x_sr = sqrt(seq(min(data_fake$x),max(data_fake$x), by=.1)))\ny_pred = predict(m_sr, newdata = preds)\n\nggplot() +\n  geom_point(aes(x = data_fake$x, y = data_fake$y), alpha = .4) +\n  geom_line(aes(x = seq(min(data_fake$x),max(data_fake$x), by=.1), y = y_pred)) +\n  ylab(\"Y\") +\n  xlab(\"X\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with Messy Data III: Transformations</span>"
    ]
  },
  {
    "objectID": "10_interactions_two_continuous.html",
    "href": "10_interactions_two_continuous.html",
    "title": "10  Inferences about Two Continuous Predictors and their Interaction",
    "section": "",
    "text": "10.1 Interactive Models: Two Quantitative Variables\nExample:\nEffect of positive attitudes (1-5) about birth control and peer pressure to not use birth control (1-5) on intention to use birth control (0-30) among sexually active female adolescents.\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\npath_data &lt;- \"data_lecture\"\n\ndata &lt;- read_csv(here::here(path_data, \"10_interactions_birth_control.csv\"),\n                 show_col_types = FALSE) |&gt; \n  glimpse()\n\n\nRows: 125\nColumns: 4\n$ subid &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ att   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ pp    &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5…\n$ bc    &lt;dbl&gt; 3, 4, 5, 6, 7, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7, 3, 4…\nCode\ndata |&gt; \n  pivot_longer(att:bc, names_to = \"var\") |&gt; \n  group_by(var) |&gt; \n  summarise(mean = mean(value), \n            sd = sd(value), \n            min = min(value), \n            max = max(value))\n\n\n# A tibble: 3 × 5\n  var    mean    sd   min   max\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 att       3  1.42     1     5\n2 bc       11  5.68     3    27\n3 pp        3  1.42     1     5\nQuestion: If you regressed BC on Att and PP in two separate linear models, what can you tell me about these two models based on the correlations below?\nCode\ndata |&gt; \n  select(-subid) |&gt; \n  cor()\n\n\n     att   pp    bc\natt 1.00  0.0  0.75\npp  0.00  1.0 -0.50\nbc  0.75 -0.5  1.00\nRegression coefficient for Att will be positive. Regression coefficient for PP will be negative. \\(R^2\\) will be bigger for Att model (\\(R^2\\) = .56) than for PP model (\\(R^2\\) = .25)\nCode\nm_att &lt;- lm (bc ~ att, data = data)\n\nbroom::tidy(m_att)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.00     0.791      2.53 1.27e- 2\n2 att             3.00     0.239     12.6  7.92e-24\n\n\nCode\nbroom::glance(m_att)$r.squared\n\n\n[1] 0.5625\nCode\nm_pp &lt;- lm (bc ~ pp, data = data)\n\nbroom::tidy(m_pp)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     17.0     1.04      16.4  9.08e-33\n2 pp              -2       0.312     -6.40 2.91e- 9\n\n\nCode\nbroom::glance(m_pp)$r.squared\n\n\n[1] 0.25\nQuestion: Based on the correlations, what can you tell me about the model including both Att and PP as regressors?\nCode\ndata |&gt; \n  select(-subid) |&gt; \n  cor()\n\n\n     att   pp    bc\natt 1.00  0.0  0.75\npp  0.00  1.0 -0.50\nbc  0.75 -0.5  1.00\nThe regression coefficients for Att and PP will match the coefficients from their respective bivariate models b/c Att and PP are fully orthogonal (uncorrelated). PP and Att each predict fully unique variance in BC.\nThe \\(R^2\\) for the additive model will be equal to the sum of the \\(R^2\\)s from the two bivariate models, again b/c Att and PP are orthogonal.\nQuestion: What about \\(\\eta_p^2\\) for Att (or PP) in the 1 predictor model vs. the 2 predictor model? Specify the augmented and compact models to test Att for 1 and 2 predictor approaches. Specify the formula for \\(\\eta_p^2\\).\nTest for Att in 1 predictor model:\n\\(\\text{BC}_a = b_0 + b_1*\\text{Att}\\)\n\\(\\text{BC}_c = b_0 + 0*\\text{Att}\\)\nTest for Att in 2 predictor model:\n\\(\\text{BC}_a = b_0 + b_1*\\text{Att} + b_2*\\text{PP}\\)\n\\(\\text{BC}_c = b_0 + 0*\\text{Att}+ b_2*\\text{PP}\\)\n\\(\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inferences about Two Continuous Predictors and their Interaction</span>"
    ]
  },
  {
    "objectID": "10_interactions_two_continuous.html#interactive-models-two-quantitative-variables",
    "href": "10_interactions_two_continuous.html#interactive-models-two-quantitative-variables",
    "title": "10  Inferences about Two Continuous Predictors and their Interaction",
    "section": "",
    "text": "Numerator is same for the both 1 and 2 predictor tests of Att.\n\nDenominator is smaller for 2 predictor test.\n\nTherefore, PRE for Att is bigger in 2 predictor model. Att produces a bigger proportional reduction in error.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inferences about Two Continuous Predictors and their Interaction</span>"
    ]
  }
]