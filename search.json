[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the General Linear Model",
    "section": "",
    "text": "Course Syllabus",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-syllabus",
    "href": "index.html#course-syllabus",
    "title": "Introduction to the General Linear Model",
    "section": "",
    "text": "Instructor\nJohn Curtin\n\nOffice hours: Thursdays, 1-2 pm or by appointment in Brogden 326\nEmail: jjcurtin@wisc.edu (but please use Slack DM or channel posts for all course communications during this semester)\n\n\n\nTeaching Assistants\nMichelle Marji\n\nOffice hours: Wednesdays, 10-11 am in Brogden 391 or by appointment\nEmail: michelle.marji@wisc.edu\n\nKendra Wyant\n\nOffice hours: Tuesdays, 12:30-1:30 pm in Brodgen 325 or by appointment\nEmail: kpaquette2@wisc.edu\n\n\n\nCourse Website\nhttps://jjcurtin.github.io/book_iaml/\n\n\nCommunications\nAll course communications will occur in the course’s Slack workspace (https://iaml-2024.slack.com/). You should have received an invitation to join the workspace. If you have difficulty joining, please contact me by my email above. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me. However, I may share the DM with the TAs unless you request otherwise. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.\n\n\nMeeting Times\nThe scheduled course meeting times are Tuesdays and Thursdays from 11:00 - 12:15 pm. Tuesdays are generally used by the TAs to discuss application issues from the homework or in the course more generally. Thursdays are generally led by John and used to discuss topics from the video lectures and readings.\nAll required videos, readings, and application assignments are described on the course website at the beginning of each unit.\n\n\nCourse Description\nThis course is designed to introduce students to a variety of computational approaches in machine learning. The course is designed with two key foci. First, students will focus on the application of common, “out-of-the-box” statistical learning algorithms that have good performance and are implemented in tidymodels in R. Second, students will focus on the application of these approaches in the context of common questions in behavioral science in academia and industry.\n\n\nRequisites\nStudents are required to have completed Psychology 610 with a grade of B or better or a comparable course with my consent.\n\n\nLearning Outcomes\n\nStudents will develop and refine best practices for data wrangling, general programming, and analysis in R.\nStudents will distinguish among a variety of machine learning settings: supervised learning vs. unsupervised learning, regression vs. classification\nStudents will be able to implement a broad toolbox of well-supported machine-learning methods: decision trees, nearest neighbor, general and generalized linear models, penalized models including ridge, lasso, and elastic-nets, neural nets, random forests.\nStudents will develop expertise with common feature extraction techniques for quantitative and categorical predictors.\nStudents will be able to use natural language processing approaches to extract meaningful features from text data.\nStudents will know how to characterize how well their regression and classification models perform and they will employ appropriate methodology for evaluating their: cross validation, ROC and PR curves, hypothesis testing.\nStudents will learn to apply their skills to common learning problems in psychology and behavioral sciences more generally.\n\n\n\nCourse Topics\n\nOverview of Machine Learning Concepts and Uses\nData wrangling in R using tidyverse and tidymodels\nIterations, functions, simulations in R\nRegression models\nClassification models\nModel performance metrics\nROCs\nCross validation and other resampling methods\nModel selection and regularization\nApproaches to parallel processing\nFeature engineering techniques\nNatural language processing\nTree based methods\nBagging and boosting\nNeural networks\nDimensionality reduction and feature selection\nExplanatory methods including variable importance, partial dependence plots, etc\nEthics and privacy issues\n\n\n\nSchedule\nThe course is organized around 14 weeks of academic instruction covering the following topics:\n\nIntroduction to course and machine learning\nExploratory data analysis\nRegression models\nClassification models\nResampling methods for model selection and evaluation\nRegularization and penalized models\nMidterm exam/project\nAdvanced performance metrics\nModel comparisons\nAdvanced models: Random forests and ensemble methods (bagging, boosting, stacking)\nAdvanced models: Neural networks\nNatural Language Processing I: Text processing and feature engineering\nApplications\nEthics\n\n\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\n\n\n\nRequired Textbooks and Software\nAll required textbooks are freely available online (though hard copies can also be purchased if desired). There are eight required textbooks for the course. The primary text for which we will read many chapters is:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. An Introduction to Statistical Learning: With Applications in R (2023; 2nd Edition). (website)\n\nWe will also read sections to chapters in each of the following texts:\n\nWickham, H. & Grolemund, G. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (1st ed.). Sebastopol, CA: O’Reilly Media, Inc. (website)\nHvitfeldt, E. & Silge, J. Supervised Machine Learning for Text Analysis in R (website)\nKuhn, M. & Johnson, K. Applied Predictive Modeling. New York, NYL Springer Science. (website)\nKuhn, M., & Johnson, K. Feature Engineering and Selection: A Practical Approach for Predictive Models (1st ed.). Boca Raton, FL: Chapman and Hall/CRC. (website)\nKuhn, M. & Silge, J. Tidy Modeling with R. (website)\nMolnar, C. Intepretable Machine Learning: A Guide for Makiong Black Box Models Explainable (2nd ed.). (website\nSilge, J., & Robinson, D. Text Mining with R: A Tidy Approach (1st ed.). Beijing; Boston: O’Reilly Media. (website)\nWickham, H. The Tidy Style Guide. (website)\nBoehmke, Brad and Greenwell, Brandon M. (2019). Hands-On Machine Learning with R. Chapman and Hall/CRC. (website)\nNg, Andrew (2018). Machine Learning Yearning: Technical Strategy for AI Engineers in the Age of Deep Learning. DeepLearning.AI. (website)\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC. (website)\n\nAdditional articles will be assigned and provided by pdf through the course website.\nAll data processing and analysis will be accomplished using R (and we recommend the RStudio IDE). R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\n\n\nGrading\n\nQuizzes (13 anticipated): 15%\nApplication assignments (11 anticipated): 25%\nMidterm application exam: 15%\nMidterm conceptual exam: 15%\nFinal application exam: 15%\nFinal conceptual exam: 15%\n\nFinal letter grades may be curved upward, but a minimum guarantee is made of an A for 93 or above, AB for 88 - 92, B for 83 - 87, BC for 78 - 82, C for 70 - 77, D for 60-69, and F for &lt; 60.\n\n\nExams, Application Assignments and Quizzes\n\nThe midterm application exam will be due during the 7th week of the course on Wednesday, March 6th at 8 pm.\nThe midterm conceptual exam will be administered during class on Thursday, March 7th.\nThe final exam is during exam week on Tuesday May 7th from 11 - 12:15 in our normal classroom.\nThe final project is due during exam week on Wednesday May 8th at 8 pm.\nApproximately weekly quizzes will be administered through Canvas and due each Wednesday at 8 pm\nApproximately weekly application assignments will be submitted via Canvas and due each Wednesday at 8 pm.\n\n\n\nApplication Assignments\nThe approximately weekly application assignments are due on Wednesdays at 8 pm through Canvas. These assignments are to be done individually. Please do not share answers or code. You are also encouraged to make use of online resources (e.g., stack overflow) for assistance. All assignments will be completed using R markdown to provide both the code and documentation as might be provided to your mentor or employer to fully describe your solution. Late assignments are not accepted because problem solutions are provided immediately after the due date. Application assignments are graded on a three-point scale (0 = not completed, 1 = partially completed and/or with many errors, 2 = fully completed and at least mostly correct). Grades for each assignment will be posted by the following Monday at the latest.\n\n\nChatGPT\nI suspect you have all seen discussions of all that ChatGPT can do by now and its impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it. Given that I expect AI like ChatGPT to become a useful tool in our workflow as professionals, now is the time to start to learn how it can help. Therefore, you are free to use it during any of our application assignments AND the application questions on the mid-term and final exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful. In contrast, you CANNOT use ChatGPT to answer the conceptual questions on the two exams or the weekly quizzes. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head and I want to be 100% clear that use of ChatGPT to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating. It will be reported to the Dean of Students on first offense.\n\n\nStudent Ethics\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester.\nIn the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.\n\n\nDiversity and Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/\n\n\nAcademic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to http://studentconduct.wiscweb.wisc.edu/academic-integrity\n\n\nAccommodations Polices\nMcBurney Disability Resource Center syllabus statement: “The University of Wisconsin-Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/\n\n\nComplaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawngreen@wisc.edu.\nIf you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system\n\n\nPrivacy of Student Information & Digital Tools\nThe privacy and security of faculty, staff and students’ personal information is a top priority for UW-Madison. The university carefully reviews and vets all campus-supported digital tools used to support teaching and learning, to help support success through learning analytics, and to enable proctoring capabilities. UW-Madison takes necessary steps to ensure that the providers of such tools prioritize proper handling of sensitive data in alignment with FERPA, industry standards and best practices. Under the Family Educational Rights and Privacy Act (FERPA which protects the privacy of student education records), student consent is not required for the university to share with school officials those student education records necessary for carrying out those university functions in which they have legitimate educational interest. 34 CFR 99.31(a)(1)(i)(B). FERPA specifically allows universities to designate vendors such as digital tool providers as school officials, and accordingly to share with them personally identifiable information from student education records if they perform appropriate services for the university and are subject to all applicable requirements governing the use, disclosure and protection of student data.\n\n\nPrivacy of Student Records & the Use of Audio Recorded Lectures\nSee information about privacy of student records and the usage of audio-recorded lectures.\nLecture materials and recordings for this course are protected intellectual property at UW-Madison. Students in this course may use the materials and recordings for their personal use related to participation in this class. Students may also take notes solely for their personal use. If a lecture is not already recorded, you are not authorized to record my lectures without my permission unless you are considered by the university to be a qualified student with a disability requiring accommodation. [Regent Policy Document 4-1] Students may not copy or have lecture materials and recordings outside of class, including posting on internet sites or selling to commercial entities. Students are also prohibited from providing or selling their personal notes to anyone else or being paid for taking notes by any person or commercial firm without the instructor’s express written permission. Unauthorized use of these copyrighted lecture materials and recordings constitutes copyright infringement and may be addressed under the university’s policies, UWS Chapters 14 and 17, governing student academic and non-academic misconduct.\n\n\nAcademic Calendar & Religious Observances\nStudents who wish to inquire about religious observance accommodations for exams or assignments should contact the instructor within the first two weeks of class, following the university’s policy on religious observance conflicts",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html",
    "href": "02_sampling_distributions.html",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "",
    "text": "2.1 Inferential Statistics\nInferential statistics are used to estimate parameters in the population from parameter estimates in a sample drawn from that population.\n\nIn inferential statistics, we use these parameter estimates to test hypotheses (predictions; null and alternative hypotheses) about the size of the population parameter.\n\nThese predictions about the size of population parameters typically map directly onto research questions about (causal) relationships between variables (IVs and DV).\n\nAnswers from inferential statistics are probabilistic. In other words, all answers have the potential to be wrong and you will provide an index of that probability along with your results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#populations",
    "href": "02_sampling_distributions.html#populations",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.2 Populations",
    "text": "2.2 Populations\nA population is any clearly defined set of objects or events (people, occurrences, animals, etc.). Populations usually represent all events in a particular class (e.g., all college students, all alcoholics, all depressed people, all people). It is often an abstract concept because in many/most instances you will never have access to the entire population.\n\nFor example, many of our studies may have the population of all people as its target.\n\nNonetheless, researchers usually want to describe or draw conclusions about populations (e.g., We don’t care if some new drug is an effective treatment for 100 people in your sample. Will it work, on average, for everyone we might treat?).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#population-parameters",
    "href": "02_sampling_distributions.html#population-parameters",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.3 (Population) Parameters",
    "text": "2.3 (Population) Parameters\nA parameter is a value used to describe a certain characteristic of a population. It is usually unknown and therefore has to be estimated.\n\nFor example, the population mean is a parameter that is often used to indicate the average/typical value of a variable in the population.\n\nWithin a population, a parameter is a fixed value which does not vary within the population at the time of measurement (e.g., the mean height of people in the US at the present moment).\n\nYou typically can’t calculate these parameters directly because you don’t have access to the entire population.\n\nWe use Greek letters to represent population parameters (\\(\\mu\\), \\(\\sigma\\), \\(\\sigma^2\\), \\(\\beta_0\\), \\(\\beta_j\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#samples-parameter-estimates",
    "href": "02_sampling_distributions.html#samples-parameter-estimates",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.4 Samples & Parameter Estimates",
    "text": "2.4 Samples & Parameter Estimates\nA sample is a finite group of units (e.g., participants) selected from the population of interest.\n\nA sample is generally selected for a study because the population is too large to study in its entirety. We typically have only one sample in a study.\n\nWe use the sample to estimate and test parameters in the population.\n\nThese estimates are called parameter estimates.\n\nWe use Roman letters to represent sample parameter estimates (\\(\\overline{X}\\), \\(s\\), \\(s^2\\), \\(b_0\\), \\(b_j\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-error",
    "href": "02_sampling_distributions.html#sampling-error",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.5 Sampling Error",
    "text": "2.5 Sampling Error\nSince a sample does not include all members of the population, parameter estimates generally differ from parameters on the entire population (e.g., use mean height of a sample of 1000 people to estimate mean height of US population).\n\nThe difference between the (sample) parameter estimate and the (population) parameter is sampling error.\n\nYou will not be able to calculate the sampling error of your parameter estimate directly because you don’t know the value of the population parameter. However, you can estimate it by probabilistic modeling of the hypothetical sampling distibution for that parameter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#hypothetical-sampling-distribution",
    "href": "02_sampling_distributions.html#hypothetical-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.6 Hypothetical Sampling Distribution",
    "text": "2.6 Hypothetical Sampling Distribution\nA sampling distribution is a probability distribution for a parameter estimate drawn from all possible samples of size \\(N\\) taken from a population.\n\nA sampling distribution can be formed for any population parameter.\n\nEach time you draw a sample of size \\(N\\) from a population you can calculate an estimate of that population parameter from that sample.\n\nBecause of sampling error, these parameter estimates will not exactly equal the population parameter. They will not equal each other either. They will form a distribution.\n\nA sampling distribution, like a population, is an abstract concept that represents the outcome of repeated (infinite) sampling. You will typically only have one sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#what-if-we-didnt-need-samples",
    "href": "02_sampling_distributions.html#what-if-we-didnt-need-samples",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.7 What if we didn’t need samples?",
    "text": "2.7 What if we didn’t need samples?\nResearch question: How do inhabitants of a remote pacific island feel about the ocean? Population size = 10,000.\n\nDependent measure: Ocean liking scale scores that range from -100 (strongly dislike) to 100 (strongly like). 0 represents neutral.\n\nHypotheses: \\(H_0: \\mu = 0; H_a: \\mu \\neq 0\\)\n\n\n\n\n\n\nQuestion: How would you answer this question if you had unlimited resources (e.g., time, money, and patience)?\n\n\n\n\n\nAdminister the Ocean liking scale to all 10,000 inhabitants in the population and calculate the population mean score. Is it 0? If not, the inhabitants are not neutral on average.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#ocean-liking-scale-scores-in-full-population",
    "href": "02_sampling_distributions.html#ocean-liking-scale-scores-in-full-population",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.8 Ocean Liking Scale Scores in Full Population",
    "text": "2.8 Ocean Liking Scale Scores in Full Population\n\nlibrary(tidyverse)\n1path_data &lt;- \"data_lecture\"\n\n2data &lt;- read_csv(here::here(path_data, \"2_sampling_distributions_like_clean.csv\"),\n                 show_col_types = FALSE) |&gt; \n3   glimpse()\n\n\n1\n\nThis path points to where your data is and should be a relative path from your R project.\n\n2\n\nwe use the here() function in the here package (here::here()) to define paths within a function. This approach (vs. file.path()) works well when using R Projects.\n\n3\n\nglimpse() is a useful function that you can pipe tibbles into when first reading them in. It shows you useful information, like number of rows and column, variable (column) names, a sample of what the data look like, and the class of each variable (e.g., double, character, factor).\n\n\n\n\nRows: 10,000\nColumns: 2\n$ subid      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ like_score &lt;dbl&gt; -23.608554, -9.011961, 30.536791, 5.886441, -9.164940, 23.6…\n\n\n\nSee also:\nview() allows you to open up the data frame in rstudio.\nskim() and other functions in the skimr package are helpful for quick summaries of your data (e.g., missingness, distribution, type of data)\n\ndata |&gt; \n  skimr::skim()\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsubid\n0\n1\n5000.5\n2886.90\n1.00\n2500.75\n5000.5\n7500.25\n10000.00\n▇▇▇▇▇\n\n\nlike_score\n0\n1\n0.0\n23.67\n-86.64\n-16.08\n0.1\n16.14\n84.46\n▁▃▇▃▁\n\n\n\n\n\n\nIf we want less information about the distribution we can use summarise and specify the descriptive statistics we want.\n\ndata |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n# A tibble: 1 × 3\n      n      mean    sd\n  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 10000 -6.27e-16  23.7\n\n\n\nYou can also pull out a few rows to look at your data. This can be done using slice_head() to pull out \\(n\\) top rows of the data set, slice_tail() to pull out \\(n\\) bottom rows of the data set, or slice_sample() to pull out a random \\(n\\) of rows from the data set.\n\ndata |&gt; \n  slice_head(n = 5) \n\ndata |&gt; \n  slice_tail(n = 5) \n\n1set.seed(101)\ndata |&gt; \n  slice_sample(n = 5)\n\n\n1\n\nWhenever you are using random numbers it is important to set a seed first (set.seed()). This ensures you can reproduce that randomness!\n\n\n\n\n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     1     -23.6 \n2     2      -9.01\n3     3      30.5 \n4     4       5.89\n5     5      -9.16\n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  9996     -16.6 \n2  9997      -8.37\n3  9998      18.6 \n4  9999     -16.2 \n5 10000     -17.0 \n# A tibble: 5 × 2\n  subid like_score\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  8009     -27.0 \n2  2873     -41.5 \n3  3281      17.8 \n4  5562      27.4 \n5  5471      -9.84\n\n\n\n\nplot_raw &lt;- data |&gt; \n  ggplot(aes(x = like_score)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(limits = c(-100, 100)) +\n1  theme_classic()\n\n\n1\n\nWe can use themes to customize the output of our figures. This can be piped into your ggplot() code or set globally at the top of your script using the code below:\n\n\n\n\n\ntheme_set(theme_classic()) \n\n\nplot_raw",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#parameter-estimation-and-testing",
    "href": "02_sampling_distributions.html#parameter-estimation-and-testing",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.9 Parameter Estimation and Testing",
    "text": "2.9 Parameter Estimation and Testing\n\n\n\n\n\n\nQuestion: What do you conclude?\n\n\n\n\n\nInhabitants of the island are neutral on average on the Ocean Liking Scale; \\(\\mu\\) = 0.\n\n\n\n\n\n\n\n\n\nQuestion: How confident are you about this conclusion?\n\n\n\n\n\nExcluding issues of measurement of the scale (i.e., reliability), you are 100% confident that the population mean score on this scale is 0 (\\(\\mu\\) = 0).\n\n\n\n\n\n\n\n\n\nQuestion: Of course, this approach to answering a research question is not typical. Why? And how would you normally answer this question?\n\n\n\n\n\nYou will very rarely have access to all scores in the population. Instead, you have to use inferential statistics to “infer” (estimate) the size of the population parameter from a sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#obtain-a-sample",
    "href": "02_sampling_distributions.html#obtain-a-sample",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.10 Obtain a Sample",
    "text": "2.10 Obtain a Sample\nYou are a poor graduate student. All you can afford is \\(N = 10\\).\n\nset.seed(2005) \ndata_sample_1 &lt;- data |&gt; \n   slice_sample(n = 10)\n\ndata_sample_1 |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n# A tibble: 1 × 3\n      n  mean    sd\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10  2.14  19.4\n\n\n\n\n\n\n\n\nQuestion: What do you conclude and why?\n\n\n\n\n\nA sample mean of 2.14 is not 0. However, you know that the sample mean will not match the population mean exactly. How likely is it to get a sample mean of 2.14 if the population mean is 0 (think about it!)?\n\n\n\n\nYour friend is a poor graduate student too. All she can afford is \\(N = 10\\) too.\n\ndata_sample_2 &lt;- data |&gt; \n   slice_sample(n = 10)\n\ndata_sample_2 |&gt; \n  summarise(n = n(),\n            mean = mean(like_score),\n            sd = sd(like_score))\n\n# A tibble: 1 × 3\n      n  mean    sd\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10  1.74  23.0\n\n\n\n\n\n\n\n\nQuestion: What does she conclude and why?\n\n\n\n\n\nA sample mean of 1.74 is not 0. However, she knows that the sample mean will not match the population mean exactly. It is more likely to get a sample mean of 1.74 than 2.14 if the population mean is 0 but she still doesn’t know how likely either outcome is. What if she obtained a sample with mean of 30?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-distribution-of-the-mean",
    "href": "02_sampling_distributions.html#sampling-distribution-of-the-mean",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.11 Sampling Distribution of the Mean",
    "text": "2.11 Sampling Distribution of the Mean\nYou can construct a sampling distribution for any parameter estimate (e.g., mean, \\(s\\), min, max, \\(r\\), \\(b_0\\), \\(b_1\\)).\n\nFor the mean, you can think of the sampling distribution conceptually as follows:\n\nImagine drawing many samples (lets say 1000 samples but in theory, the sampling distribution is infinite) of \\(N\\)=10 participants (10 participants in each sample) from your population.\nNext, calculate the mean for each of these samples of 10 participants.\nFinally, create a histogram (or density plot) of these sample means.\n\n\n\n2.11.1 1000 Samples of \\(N\\)=10\n\n1get_sample_mean &lt;- function(data, n_sub) {\n  data |&gt; \n  slice_sample(n = n_sub) |&gt; \n  summarise(across(everything(), list(mean = mean, sd = sd), .names = \"{fn}\")) |&gt; \n  mutate(n = n_sub) \n}\n\n\n1\n\nHere we are writing a function that we will use to generate descriptive statistics over 1000 samples. We are keeping this function generic because we are going to use this function again later in the chapter for more simulation examples!\n\n\n\n\n\nset.seed(101)\nsamples &lt;- 1:1000 |&gt; \n1  map(\\(i) get_sample_mean(data = data[, \"like_score\"], n_sub = 10)) |&gt;\n  list_rbind()\n\nsamples\n\n\n1\n\nWe use the map() function to repeat our function 1000 times.\n\n\n\n\n# A tibble: 1,000 × 3\n     mean    sd     n\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -6.89   22.4    10\n 2 11.9    20.2    10\n 3  0.147  15.9    10\n 4 -8.93   27.3    10\n 5  8.52   20.0    10\n 6  2.02   10.6    10\n 7 -0.198  16.9    10\n 8 -0.230  21.1    10\n 9  1.01   22.7    10\n10  6.26   28.4    10\n# ℹ 990 more rows\n\n\n\n\n\n2.11.2 Sampling Distribution of the Mean\n\nsamples |&gt; \n  summarise(n = n(), \n            mean_m = mean(mean), \n            sd = sd(mean))\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000 -0.238  7.69\n\nplot_samples &lt;- samples |&gt; \n  ggplot(aes(x = mean)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(\"sample means\", limits = c(-100, 100))\n\nplot_samples\n\n\n\n\n\n\n\n\nNote: In your research, you don’t form a sampling distribution by repeated sampling. You (typically) only have one sample.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#raw-score-distribution-vs.-sampling-distribution",
    "href": "02_sampling_distributions.html#raw-score-distribution-vs.-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.12 Raw Score Distribution vs. Sampling Distribution",
    "text": "2.12 Raw Score Distribution vs. Sampling Distribution\nThe distinction between the raw score distribution and your sample distribution is very important to keep clear in your mind!\n\n1library(patchwork)\n\n\n1\n\nThe patchwork package is an easy and customizable way to combine ggplot() objects. For more information see https://patchwork.data-imaginist.com/.\n\n\n\n\n\nplot_raw + plot_samples",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#sampling-distribution-of-the-mean-2",
    "href": "02_sampling_distributions.html#sampling-distribution-of-the-mean-2",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.13 Sampling Distribution of the Mean",
    "text": "2.13 Sampling Distribution of the Mean\n\n\n\n\n\n\nQuestion: What will the mean of the sample means be? In other words, what is the mean of the sampling distribution?\n\n\n\n\n\nThe mean of the sample means (i.e., the mean of the sampling distribution) will equal the population mean of raw scores on the dependent measure. This is important because it indicates that the sample mean is an unbiased estimator of the population mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean is an unbiased estimator: The mean of the sample means will equal the mean of the population. Therefore, individual sample means will neither systematically under or overestimate the population mean.\n\nRaw Ocean Liking Scores:\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 10000      0  23.7\n\n\n\nSample (N=10) Means:\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000  -0.24  7.69\n\n\nThe sample variance (\\(s^2\\); with n-1 denominator) is also an unbiased estimator of the population variance (\\(\\sigma^2\\)). In other words, the mean of the sample \\(s^2\\)’s will approximate the population variance. Sample \\(s\\) is negatively biased.\n\n\n\n\n\n\n\nQuestion: Will all of the sample means be the same?\n\n\n\n\n\nNo, there was a distribution of means that varied from each other. The mean of the sampling distribution was the population mean but the standard deviation was not zero.\n\n\n# A tibble: 1 × 3\n      n mean_m    sd\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  1000  -0.24  7.69",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#standard-errorse",
    "href": "02_sampling_distributions.html#standard-errorse",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.14 Standard Error(SE)",
    "text": "2.14 Standard Error(SE)\nThe standard deviation of the sampling distribution (i.e., standard deviation of the infinite sample means) is equal to:\n\n\\(\\frac{\\sigma}{\\sqrt{N_{sample}}}\\)\n\nWhere \\(\\sigma\\) is the standard deviation of the population raw scores.\n\n\nThis variability in the sampling distribution is due to sampling error.\n\n\nTherefore, because we use parameter estimates calculated in our sample to estimate population parameters, we would like to minimize sampling error.\n\nThe standard deviation of the sampling distribution for a parameter estimate has a technical name. It is called the standard error of the parameter estimate. Here, we are talking about the standard error of the mean.\n\n\n\n\n\n\n\nQuestion: What factors affect the size of the sampling error of the mean (i.e., the standard error)?\n\n\n\n\n\nThe standard deviation of the population raw scores and the sample size.\n\n\n\n\n\n\n\n\n\n\nQuestion: Variation among raw scores for a variable in the population is broadly caused by two factors. What are they?\n\n\n\n\n\n\nIndividual differences\nMeasurement error (the opposite of reliability)\n\n\n\n\n\n\n\n\n\n\nQuestion: What is the relationship between population variability (\\(\\sigma\\)) and SE?\n\n\n\n\n\nAs the variability of the variable increases in the population, the SE increases.\n\n\n\n\n\n\n\n\n\nQuestion: What would happen to SE if there was no variation in population scores?\n\n\n\n\n\nThe SE would equal 0 no matter which participants you sampled. They would all have the same scores!\n\n\n\n\n\n\n\n\n\n\nQuestion: What is the relationship between sample size and SE?\n\n\n\n\n\nAs the sample size increases, the SE for the statistic will decrease.\n\n\n\n\n\n\n\n\n\nQuestion: What would the SE be if the sample size equalled population size?\n\n\n\n\n\nIf the sample contained all participants from the population, the SE would be equal to 0 because each sample mean would have exactly the same value as the overall population mean (because all same scores).\n\n\n\n\n\n\n\n\n\nQuestion: What would happen if the samples contained only 1 participant?\n\n\n\n\n\nIf each sample contained only 1 participant, the SE would be equal to the variation (\\(\\sigma\\)) observed within the population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#shape-of-the-sampling-distribution",
    "href": "02_sampling_distributions.html#shape-of-the-sampling-distribution",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.15 Shape of the Sampling Distribution",
    "text": "2.15 Shape of the Sampling Distribution\nCentral Limit Theorem: The shape of the sampling distribution approaches normal as \\(N\\) increases.\n\nThe shape is roughly normal even for moderate sample sizes assuming that the original distribution isn’t really weird (i.e., non-normal).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#normal-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#normal-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.16 Normal Population and Various Sampling Distributions",
    "text": "2.16 Normal Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#uniform-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#uniform-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.17 Uniform Population and Various Sampling Distributions",
    "text": "2.17 Uniform Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#skewed-population-and-various-sampling-distributions",
    "href": "02_sampling_distributions.html#skewed-population-and-various-sampling-distributions",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.18 Skewed Population and Various Sampling Distributions",
    "text": "2.18 Skewed Population and Various Sampling Distributions\nPopulation size: 100,000; Simulated 10,000 samples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#an-important-normal-distribution-z-scores",
    "href": "02_sampling_distributions.html#an-important-normal-distribution-z-scores",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.19 An Important Normal Distribution: Z-scores",
    "text": "2.19 An Important Normal Distribution: Z-scores\nThe \\(z\\) distribution contains normally distributed scores with a mean of 0 and a standard deviation of 1.\n\nYou can therefore think of any specific z-score as telling you the position of the score in terms of standard deviations above the mean.\n\nThe probability distribution is known for the \\(z\\) distribution.\n\n# insert three figures of probability distributions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#probability-of-parameter-estimate-given-h_0",
    "href": "02_sampling_distributions.html#probability-of-parameter-estimate-given-h_0",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.20 Probability of Parameter Estimate Given \\(H_0\\)",
    "text": "2.20 Probability of Parameter Estimate Given \\(H_0\\)\nHow could you use the \\(z\\) distribution to determine the probability of obtaining a sample mean (parameter estimate) of 2.40 if you draw a sample of \\(N=10\\) from a population of Ocean Liking scores with a population mean (parameter) of 0?\n\nThink about it……",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#hypothetical-sampling-distribution-for-h_0",
    "href": "02_sampling_distributions.html#hypothetical-sampling-distribution-for-h_0",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.21 Hypothetical Sampling Distribution for \\(H_0\\)",
    "text": "2.21 Hypothetical Sampling Distribution for \\(H_0\\)\nIf \\(H_0\\) is true; the sampling distribution has a mean of 0 and standard deviation of \\(\\frac{\\sigma}{\\sqrt{N_{sample}}} =  \\frac{23.7}{\\sqrt{10}}  =  7.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: If \\(H_0\\) is true and this is the sampling distribution (in blue), how likely is it to get a sample mean of 2.4 or more extreme?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow Answer\nPretty likely…\nBut we can do better than that!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#our-first-inferential-test-the-z-test",
    "href": "02_sampling_distributions.html#our-first-inferential-test-the-z-test",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.22 Our First Inferential Test: The z-test",
    "text": "2.22 Our First Inferential Test: The z-test\n\\(z = \\frac{2.4 - 0}{7.5} = 0.32; p \\le .749\\)\n\npnorm(0.32, mean=0, sd=1, lower.tail=FALSE) * 2 \n\n[1] 0.7489683",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#t-vs.-z",
    "href": "02_sampling_distributions.html#t-vs.-z",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.23 \\(t\\) vs. \\(z\\)",
    "text": "2.23 \\(t\\) vs. \\(z\\)\n\\(z = \\frac{2.4 - 0}{7.5} = 0.32\\)\n\n\n\n\n\n\nQuestion: Where did we get the 2.4 from in our z-test?\n\n\n\n\n\nOur sample mean from our study. This is our parameter estimate of the population mean of OLS (like_score) scores.\n\n\n\n\n\n\n\n\n\nQuestion: Where did we get the 7.5 from in our z-test and what is the problem with this?\n\n\n\n\n\nThis was our estimate of the standard deviation of the sampling distribution. \\(\\frac{\\sigma}{\\sqrt{N_{sample}}}\\) We do not know \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\nQuestion: How can we estimate \\(\\sigma\\)?\n\n\n\n\n\nWe can use our sample standard deviation (\\(s\\)), but \\(s\\) is a negatively biased parameter estimate. On average, it will underestimate \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\nQuestion: So what do we do?\n\n\n\n\n\nWe account for this underestimation of \\(\\sigma\\) and therefore of the standard deviation (standard error) of the sampling distribution by using the t distribution rather than the \\(z\\) distribution to calculate the probability of our parameter estimate if \\(H_0\\) is true.The t distribution is slightly wider, particularly for small sample sizes to correct for our underestimate of the standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#our-second-inferential-test-one-sample-t-test",
    "href": "02_sampling_distributions.html#our-second-inferential-test-one-sample-t-test",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.24 Our Second Inferential Test: One Sample t-test",
    "text": "2.24 Our Second Inferential Test: One Sample t-test\n\\(t(df)\\) = \\(\\frac{\\text{Parameter estimate – Parameter:} H_0}{\\text{Standard error of parameter estimate}}\\)\n \nWhere \\(SE\\) is estimated using \\(s\\) from sample data.\n\\(df = N – P = 10 - 1 = 9\\)\n\n\n\n\n\n\n\n\n\n\nThe bias in \\(s\\) decreases with increasing \\(N\\). Therefore, \\(t\\) approaches \\(z\\) with larger sample sizes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  },
  {
    "objectID": "02_sampling_distributions.html#null-hypothesis-significance-testing-nhst",
    "href": "02_sampling_distributions.html#null-hypothesis-significance-testing-nhst",
    "title": "2  Sampling Distributions, Parameters, and Parameter Estimates",
    "section": "2.25 Null Hypothesis Significance Testing (NHST)",
    "text": "2.25 Null Hypothesis Significance Testing (NHST)\n\nDivide reality regarding the size of the population parameter into two non-overlapping possibilities: Null hypothesis (\\(H_0\\)) & Alternate hypothesis (\\(H_a\\)).\nAssume that \\(H_0\\) is true.\nCollect data.\nCalculate the probability (\\(p\\)-value) of obtaining your parameter estimate (or a more extreme estimate) given your assumption (i.e., \\(H_0\\) is true)\nCompare probability to some cut-off value (alpha level).\n\nIf this parameter estimate is less probable than cut-off value, reject \\(H_0\\) in favor of \\(H_a\\).\nIf data is not less probable, fail to reject \\(H_0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling Distributions, Parameters, and Parameter Estimates</span>"
    ]
  }
]