[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to the General Linear Model",
    "section": "",
    "text": "Course Syllabus",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-designations-and-attributes",
    "href": "index.html#course-designations-and-attributes",
    "title": "Introduction to the General Linear Model",
    "section": "Course Designations And Attributes",
    "text": "Course Designations And Attributes\n\nCourse Website: https://jjcurtin.github.io/book_glm/\nCredits: 4\nLevel: Advanced\nBreadth: Social Science\nL&S Credit Type: Counts as LAS credit (L&S)\nInstructional Mode: All face-to-face\nHow Credit Hours are met by the Course: Four and ½ hours of classroom or direct faculty/instructor instruction and a minimum of eight hours of out of class student work each week over approximately 14 weeks.\nRequisites: None",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#meeting-time-and-location",
    "href": "index.html#meeting-time-and-location",
    "title": "Introduction to the General Linear Model",
    "section": "Meeting Time And Location",
    "text": "Meeting Time And Location\n\nLecture: Monday and Wednesday, 2:30-3:45 pm; Psychology 103\nLabs:\n\nFriday 9:00-11:00 am (section 301, room 228; Punturieri)\nFriday 1:30-3:30 pm (section 302, room 228; Yu)\nFriday 9:00-11:00 am (section 303, room 311i; Dong)",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Introduction to the General Linear Model",
    "section": "Instructor",
    "text": "Instructor\n\nJohn J. Curtin, Ph.D.\nOffice hours: Room 326, Wednesdays, 3:45 - 4:45 pm or by appt.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#teaching-assistants",
    "href": "index.html#teaching-assistants",
    "title": "Introduction to the General Linear Model",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\nLiChen Dong; Office hours: Room 626, Mondays, 11 - 12 pm\nClaire Punturieri; Office hours: Room 325, Thursdays, 8:30 - 9:30 am\nCoco Yu; Office hours: Room 325, Tuesdays, 1 - 2 pm",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#communications",
    "href": "index.html#communications",
    "title": "Introduction to the General Linear Model",
    "section": "Communications",
    "text": "Communications\nAll course communications will occur in the course’s Slack workspace. You should have received an invitation to join the workspace. If you have difficulty joining, please let me know immediately. The TAs and I will respond to all Slack messages within 1 business day (and often much quicker). Please plan accordingly (e.g., weekend messages may not receive a response until Monday). For general questions about class, coding assignments, etc., please post the question to the appropriate public channel. If you have the question, you are probably not alone. For issues relevant only to you (e.g., class absences, accommodations, etc.), you can send a direct message in Slack to me and the TAs. If you DM only me, I will share the DM with the TAs unless you request otherwise. Therefore, it is generally best if you include all three TAs on the DM when you start the thread. In general, we prefer that all course communication occur within Slack rather than by email so that it is centralized in one location.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Introduction to the General Linear Model",
    "section": "Course Description",
    "text": "Course Description\nOne-sample t-test, independent-samples t-test, simple and multiple regression, effect size indicators, analysis of variance (ANOVA), analysis of covariance (ANCOVA), case analysis, model assumptions, transformations, and the generalized linear model",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Introduction to the General Linear Model",
    "section": "Course Goals",
    "text": "Course Goals\nThe goal of this course is to familiarize you with a statistical data analysis procedure called the general linear model. We will spend most of the semester on the use of the general linear model as a tool for analyzing data from psychological experiments. We will give special attention to the interpretation of model parameter estimates, models with quantitative and categorical predictors, and the interpretation of interaction effects in the general linear model. We will be using the statistics software R (http://www.r-project.org/). Please know that extensive work outside the classroom is required in order to succeed in this class. You are encouraged to participate actively in the class, both the lecture and the lab session.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "Introduction to the General Linear Model",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\nBy the end of the course, the students should master the following data-analytic techniques and skills:\n\nInferences about a single mean (t-test)\nThe analysis of single and multiple dichotomous/categorical predictors\nThe analysis of single and multiple quantitative predictors\nThe analysis of interactions among predictors\nThe analysis of contrasts among levels of categorical predictors\nThe use of centering predictors in interactive models\nAssessment and remediation techniques for case analysis and model assumptions\nLogistic regression as exemplar of the generalized linear model\nGenerating publication-level graphs",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-requirements-and-grades",
    "href": "index.html#course-requirements-and-grades",
    "title": "Introduction to the General Linear Model",
    "section": "Course Requirements And Grades",
    "text": "Course Requirements And Grades\nCourse requirements include regular attendance, active participation in class discussion, and completion of all exams and application assignments.\nThere will be two closed-book concepts exams completed in class to assess conceptual knowledge and three application exams completed outside of class to assess your ability to implement your conceptual knowledge with real data (each of these 5 exams counts for 15% of your total grade). The first concepts exam is scheduled for Wednesday, 10/23. The second concepts exam is scheduled during the exam period on Wednesday, December 18 from 8:15 - 9:45 am (room TBD). Please plan your end of semester travel accordingly. The application exams will be assigned at the end of weeks 5 (Friday 10/4), 10 (Friday 11/8), and 15 (Wednesday 12/11).\nThere will also be approximately weekly application assignments, which will involve hands-on application of the material similar to (but shorter than) the application exams. They are assigned Fridays at 5 pm and are due the following Friday at 8:30 am. These application assignments are graded on a pass/fail basis, and together constitute 20% of your total grade.\nFinally, 5% of your grade will be determined by your attendance and participation in lecture and lab.\nFinal letter grades are based on total course percentages as follows:\n\nA: 93 or above\nAB: 88 - 92\nB: 83 - 87\nBC: 78 - 82\nC: 70 - 77\nD: 60-69\nF: &lt; 60",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#required-texts",
    "href": "index.html#required-texts",
    "title": "Introduction to the General Linear Model",
    "section": "Required Texts",
    "text": "Required Texts\nJudd, C.M., McClelland, G. H., & Ryan, C. (2017). Data Analysis: A Model- Comparison Approach. 3rd Edition. New York, US: Routledge. ISBN: 9780805833881.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#additional-required-readings",
    "href": "index.html#additional-required-readings",
    "title": "Introduction to the General Linear Model",
    "section": "Additional Required Readings",
    "text": "Additional Required Readings\nAdditional required readings will be provided as pdfs on the course website. The readings are pulled from various texts and primary sources. Supplemental readings and recommended reference texts are also provided on the course website and the end of this document. You are expected to read only the required readings. You will not be tested on the other readings.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#required-software",
    "href": "index.html#required-software",
    "title": "Introduction to the General Linear Model",
    "section": "Required Software",
    "text": "Required Software\nThis course will contain a significant applied component. As such, access to statistical analysis software is required. In the context of this course, we will rely heavily on R. R is freely available and is rapidly becoming the standard for statistical analysis in many disciplines.\nA secondary goal of the course will be to provide you with introductory data wrangling skills in R within the Tidyverse ecosystem of packages. The Tidyverse is “an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” It is arguably the dominant approach for data science in R today.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Introduction to the General Linear Model",
    "section": "Course Schedule",
    "text": "Course Schedule\nThis schedule is provisional so that we may adjust our rate of progress as necessary to ensure maximal mastery of the material. See course website for the most up to date version of the assigned readings and topics.\n\nIntroduction to inferential statistics (1 day)\n\n\nintroduction to the course\nthe GLM framework\ndata exploration in R (descriptive statistics, visual displays) (lab only)\n\n\nSampling Distributions (1 day)\n\n\nstandard deviation, standard error of the mean\ntheory of null hypothesis significance testing\n\n\nInferences about a single mean (one-sample t test) (1 day)\n\n\nthe null model (\\(Y = b_{0}\\))\nsum of squares, number of estimated parameters, residuals, etc.\nthe basic model (\\(Y = b_{0}\\))\nstatistical inference (comparison of basic model with null model, computation of t, interpretation of p)\nwriting up the results (text, graphs, tables) of a one-sample t test\n\n\nInferences about a single quantitative predictor (simple regression) (2 days)\n\n\nthe model : \\(Y = b_0 + b_{1}X_{1}\\) when \\(X_{1}\\) is quantitative\ncomputation of residuals, meaning of residuals\ngraphic representation: intercept, slope, residuals\nstatistical inference (comparison of the new model with the basic model, computation of t and F, interpretation of p)\nproportion of variance explained, computation of \\(R^{2}\\), interpretation of \\(R^{2}\\), effect sizes\nrunning a simple regression in R and interpreting the R output\nwriting up the results (text, graphs, tables) of a simple regression analysis\n\n\nInferences about a single dichotomous predictor (independent-samples t test) (1 day)\n\n\nthe model: \\(Y = b_{0} + b_{1}X_{1}\\) when \\(X_{1}\\) is dichotomous\ncomputation of residuals, meaning of residuals (= within-group variance)\ngraphic representation: intercept, slope, residuals; comparison with bar graph\nstatistical inference (comparison of the new model with the basic model, computation of t and F, interpretation of p)\nrunning an independent-samples t test in R (using the lm() function in R) and interpreting the R output\nwriting up the results (text, graphs, tables) of an independent-samples t test\n\n\nInferences about two (or more) predictors (multiple regression without interaction) (4 days)\n\n\nthe model: \\(Y = b_{0} + b_{1}X_{1} + b_{2}X_{2}\\) when \\(X_{1}\\) is dichotomous and \\(X_{2}\\) is quantitative\nthe model: \\(Y = b_{0} + b_{1}X_{1} + b_{2}X_{2}\\) when \\(X_{1}\\) and \\(X_{2}\\) are both quantitative\ncomputation of residuals, meaning of residuals\ngraphic representation: two lines, intercepts, slopes, residuals\nstatistical inference (model comparison, interpretation of the effect of one variable on DV while controlling for the effects of another variable)\ncomputation of partial r, interpretation of partial r\ndifferent theoretical predictions that can be answered by multiple regression analyses that do not contain interactions\nmodels with 3, 4, 5, etc. predictors\nissues of collinearity, variance inflation, tolerance\ndata fishing, overfitting, hierarchical vs. stepwise vs. simultaneous models\nraw vs. standardized coefficients, partial r\nwriting up the results of a multiple regression analysis\n\n\nDealing with messy data I – case analysis (1 day)\n\n\nthe different ways of being an outlier\noutlier statistics: levers \\(h_{ij}\\), studentized deleted residuals, Cook’s D\ndealing with outliers\n\n\nDealing with messy data II – model assumptions (1 day)\n\n\nthe 5 assumptions of the GLM: exact X, independence, normality, constant variance, and linearity\ndata exploration in R (visual displays: residual plots, normal quantile plots, density plots, spread-level plots, etc.)\nstatistical indicators: ncv test, gvlma test\nfirst remedies: heteroscedasticity-corrected standard errors, weighted least squares\n\n\nDealing with messy data III – transformations (1 day)\n\n\nHow to address violations of GLM model assumptions: power transformations, root transformations, how to find the best transformations\nhow to analyze proportions and correlations as data\n\n\nInferences about two predictors and their interaction (= moderation) (1 day)\n\n\ncentering variables: mean deviation form, contrast codes\nthe model: \\(Y = b_{0} + b_{1}X_{1}c + b_{2}X_{2}c + b_{3}(X_{1}c * X_{2}c\\)) when \\(X_{1}\\) is dichotomous, \\(X_{2}\\) is quantitative and both predictors are centered]\ngraphic representation: different slopes for different folks, \\(b_{3}\\) tests the difference between the two slopes\nwhat happens if variables are not centered?\ninterpretation of an interaction\nwriting up the results of a multiple regression analysis with an interaction\n\n\nInferences about two quantitative predictors and their interaction (1 day)\n\n\nthe model: \\(Y = b_{0} + b_{1}X_{1}c + b_{2}X_{2}c + b_{3}(X_{1}c * X_{2}c\\)) when \\(X_{1}\\) and \\(X_{2}\\) are both quantitative\ninterpretation of an interaction between two quantitative predictors\nthe pitfalls of dichotomization II: imaginary interaction effects\n\n\nInferences about two dichotomous predictors and their interaction (= 2 x 2 ANOVA) (1 day)\n\n\nthe model: \\(Y = b_{0} + b_{1}X_{1}c + b_{2}X_{2}c + b_{3}(X_{1}c * X_{2}c\\)) when \\(X_{1}\\) and \\(X_{2}\\) are both dichotomous\ndifference between main effects and simple effects\ninterpretation of interactions in 2 x 2 ANOVAs (Rosnow & Rosenthal)\ncomparison of the GLM terminology and the ANOVA terminology\nthe pitfalls of dichotomization I: loss of power, biased estimates\nwriting up the results of a 2 x 2 ANOVA\n\n\nInferences about three predictors and one interaction (= ANCOVA) (1 day)\n\n\nthe model: \\(Y = b_{0} + b_{1}X_{1}c + b_{2}X_{2}c + b_{3}(X_{1}c * X_{2}c) + b_{4}X_{3}\\) when \\(X_{1}\\) and \\(X_{2}\\) are both dichotomous and \\(X_{3}\\) is quantitative\ninterpretation of \\(b_{3}\\)\ngeneralization to other models (e.g., the covariate is dichotomous, one of the predictors is quantitative)\nappropriate and “inappropriate” uses of ANCOVA\nwriting up the results of an ANCOVA\n\n\nStatistical power and power analysis (2 day)\n\n\ntype I and type II errors\nfactors determining statistical power\nhow to compute power\npositive predictive value\nReadings: Button et al. (2013). Cohen (1992).\n\n\nInferences about categorical predictors with three or more levels (2 days)\n\n\northogonal and non-orthogonal contrasts\ncomparing several experimental groups to one reference group (dummy codes)\ntest-wise error rate vs. family-wise error rate\nFisher LSD Protected Testing for two or three planned comparisons\nHolm-Bonferroni Adjustment for four or more planned comparisons\nScheffé Approach for unplanned comparisons\nReadings: Abelson & Prentice (1997). Guggenmos et al. (2018).",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#recommended-general-texts-for-data-analysis-and-research-methodology",
    "href": "index.html#recommended-general-texts-for-data-analysis-and-research-methodology",
    "title": "Introduction to the General Linear Model",
    "section": "Recommended General Texts For Data Analysis And Research Methodology",
    "text": "Recommended General Texts For Data Analysis And Research Methodology\n\nAbelson, R. P. (1995). Statistics as Principled Argument. Hillsdale, NJ: Lawrence Erlbaum Associates.\nAiken, L. S., & West, S. G. (1991). Multiple Regression: Testing and Interpreting Interactions. Newbury Park, CA.: Sage.\nChambers, J (2008). Software for Data Analysis: Programming with R. New York: Springer Science Business Media.\nCook, T. D., & Campbell, D. T. (1979). Quasi-Experimentation - Design and Analysis Issues for Field Settings. Boston, MA: Houghton Mifflin Company.\nCohen, J., Cohen, P., West, S. G., & Aiken,, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3rd. Ed.). Mahwah, NJ: Lawrence Erlbaum Associates.\nDalgaard, P. (2008) Introductory Statistics with R (2nd edition). New York: Springer Science Business Media.\nFox, J. (2015). Applied Regression, Generalized Linear Models, and Related Methods, Third Edition. Sage Publications.\nFox, J., & Weisberg, S. (2010). An R Companion to Applied Regression (3rd Edition). Sage Publications.\nHayes, A. F. (2013). Introduction to mediation, moderation, and conditional process analysis; A regression-based approach (3rd edition). NY, US: Guilford Press.\nHealy, K. (2019). Data Visualization A Practical Introduction. Princeton, NJ: Princeton University Press\nHoyle, R. H., Harris, M. J., & Judd, C. M. (2006). Research Methods in Social Relations (8th edition). Belmont, CA, US: Allyn & Bacon.\nJudd, C. M., & Kenny, D. A. (1981). Estimating the Effects of Social Interventions. New York, NY: Cambridge University Press.\nKline, R. B. (2016). Principles and practice of structural equation modeling (5th edition). New York, US: The Guilford Press\nKutner, M., Nachtscheim, C., & Neter, J (2004). Applied Linear Regression Models, Fourth edition, McGraw-Hill.\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical Linear Models. Applications and Data Analysis Methods (2nd ed.). Newbury Park, CA: Sage.\nReis, H. T., & Judd, C. M. (2014). Handbook of Research Methods in Social and Personality Social Psychology (2nd ed.). New York, NY: Cambridge University Press.\nSnijders, T. A. B., & Bosker, R. J. (2012). Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling (2nd ed.). London, UK: Sage Publishers.\nTabachnick, B. G., & Fidell, L. S. (2018). Using Multivariate Statistics (7th edition). New York, NY: Pearson.\nWickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data (2nd edition). Sebastopol, CA: O’Reilly Media",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#ethics-of-being-a-student-in-the-department-of-psychology",
    "href": "index.html#ethics-of-being-a-student-in-the-department-of-psychology",
    "title": "Introduction to the General Linear Model",
    "section": "Ethics of Being a Student in the Department of Psychology",
    "text": "Ethics of Being a Student in the Department of Psychology\nThe members of the faculty of the Department of Psychology at UW-Madison uphold the highest ethical standards of teaching and research. They expect their students to uphold the same standards of ethical conduct. By registering for this course, you are implicitly agreeing to conduct yourself with the utmost integrity throughout the semester. In the Department of Psychology, acts of academic misconduct are taken very seriously. Such acts diminish the educational experience for all involved – students who commit the acts, classmates who would never consider engaging in such behaviors, and instructors. Academic misconduct includes, but is not limited to, cheating on assignments and exams, stealing exams, sabotaging the work of classmates, submitting fraudulent data, plagiarizing the work of classmates or published and/or online sources, acquiring previously written papers and submitting them (altered or unaltered) for course assignments, collaborating with classmates when such collaboration is not authorized, and assisting fellow students in acts of misconduct. Students who have knowledge that classmates have engaged in academic misconduct should report this to the instructor.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Introduction to the General Linear Model",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBy enrolling in this course, each student assumes the responsibilities of an active participant in UW-Madison’s community of scholars in which everyone’s academic work and behavior are held to the highest academic integrity standards. Academic misconduct compromises the integrity of the university. Cheating, fabrication, plagiarism, unauthorized collaboration, and helping others commit these acts are examples of academic misconduct, which can result in disciplinary action. This includes but is not limited to failure on the assignment/course, disciplinary probation, or suspension. Substantial or repeated cases of misconduct will be forwarded to the Office of Student Conduct & Community Standards for additional review. For more information, refer to https://conduct.students.wisc.edu/academic-misconduct/",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#chatgpt-and-other-llms",
    "href": "index.html#chatgpt-and-other-llms",
    "title": "Introduction to the General Linear Model",
    "section": "ChatGPT and other LLMs",
    "text": "ChatGPT and other LLMs\nI suspect you have all seen discussions of all that ChatGPT and other LLMs can do by now and their impact on teaching and assessment. I believe that AI like ChatGPT will eventually become an incredible tool for data scientists and programmers. As such, I view these advances with excitement. Of course, I don’t plan to assign a grade to ChatGPT so I want to make sure that we are clear on when you can and when you cannot use it.\nGiven that I expect AI like ChatGPT and other LLMs to become a useful tool in our workflow as professionals, now is the time to start to learn how they can help. Therefore, you are free to use them for coding assistance during any of our application assignments AND the application exams. Code from ChatGPT is unlikely to be sufficient in either context (and my testing suggests it can be flat out wrong in some instances!) but I suspect that it will still be useful.\nIn contrast, you cannot use ChatGPT or other LLMs/AI to answer the conceptual questions on the conceptual exams. Those questions are designed to assess your working knowledge about concepts and best practices. That information must be in YOUR head and I want to be 100% clear that use of ChatGPT/AI (or any other sources other than what is in your head) to answer those questions will be considered cheating and handled as such if detected. There will be a zero tolerance policy for such cheating and it will be considered academic misconduct and disciplined as such. ## Accommodations Policies McBurney Disability Resource Center syllabus statement: “The University of Wisconsin- Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW-Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility. Students are expected to inform faculty [me] of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized. Faculty [I], will work either directly with the student [you] or in coordination with the McBurney Center to identify and provide reasonable instructional accommodations. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA.” http://mcburney.wisc.edu/facstaffother/faculty/syllabus.php\nUW-Madison students who have experienced sexual misconduct (which can include sexual harassment, sexual assault, dating violence and/or stalking) also have the right to request academic accommodations. This right is afforded them under Federal legislation (Title IX). Information about services and resources (including information about how to request accommodations) is available through Survivor Services, a part of University Health Services: https://www.uhs.wisc.edu/survivor-services/.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#diversity-inclusion",
    "href": "index.html#diversity-inclusion",
    "title": "Introduction to the General Linear Model",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nInstitutional statement on diversity: “Diversity is a source of strength, creativity, and innovation for UW-Madison. We value the contributions of each person and respect the profound ways their identity, culture, background, experience, status, abilities, and opinion enrich the university community. We commit ourselves to the pursuit of excellence in teaching, research, outreach, and diversity as inextricably linked goals.\nThe University of Wisconsin-Madison fulfills its public mission by creating a welcoming and inclusive community for people from every background – people who as students, faculty, and staff serve Wisconsin and the world.” https://diversity.wisc.edu/",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#complaints",
    "href": "index.html#complaints",
    "title": "Introduction to the General Linear Model",
    "section": "Complaints",
    "text": "Complaints\nOccasionally, a student may have a complaint about a TA or course instructor. If that happens, you should feel free to discuss the matter directly with the TA or instructor. If the complaint is about the TA and you do not feel comfortable discussing it with the individual, you should discuss it with the course instructor. Complaints about mistakes in grading should be resolved with the TA and/or instructor in the great majority of cases. If the complaint is about the instructor (other than ordinary grading questions) and you do not feel comfortable discussing it with the individual, make an appointment to speak to the Associate Chair for Graduate Studies, Professor Shawn Green, cshawn.green@wisc.edu. If you have concerns about climate or bias in this class, or if you wish to report an incident of bias or hate that has occurred in class, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Chair of the Psychology Department Climate & Diversity Committee, Martha Alibali (martha.alibali@wisc.edu). You may also use the University’s bias incident reporting system, which you can reach at the following link: https://doso.students.wisc.edu/services/bias-reporting-process/.",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "index.html#concerns-about-sexual-misconduct",
    "href": "index.html#concerns-about-sexual-misconduct",
    "title": "Introduction to the General Linear Model",
    "section": "Concerns About Sexual Misconduct",
    "text": "Concerns About Sexual Misconduct\nAll students deserve to be safe and respected at UW-Madison. Unfortunately, we know that sexual and relationship violence do happen here. Free, confidential resources are available on and off campus for students impacted by sexual assault, sexual harassment, dating violence, and stalking (regardless of when the violence occurred). You don’t have to label your experience to seek help. Friends of survivors can reach out for support too. A list of resources can be found at https://www.uhs.wisc.edu/survivor-resources/ If you wish to speak to someone in the Department of Psychology about your concerns, you may contact the Chair of the Department, Professor Allyson Bennett (allyson.j.bennett@wisc.edu) or the Associate Chair of Graduate Studies, Professor Shawn Green (cshawn.green@wisc.edu). Please note that all of these individuals are Responsible Employees (https://compliance.wisc.edu/titleix/mandatory-reporting/#responsible-employees).",
    "crumbs": [
      "Course Syllabus"
    ]
  },
  {
    "objectID": "web2_slide_decks.html",
    "href": "web2_slide_decks.html",
    "title": "Slide Decks",
    "section": "",
    "text": "Unit 1: Overview\nUnit 2: Sampling Distributions, Parameters, and Parameter Estimates\nUnit 3: Inferences about a Single Mean (1 Parameter Models)\nUnit 4: Inferences about a Single Quantitative Predictor\nUnit 5: Inferences about a Single Dichotomous Predictor\nUnit 6: Inferences about two predictors\nUnit 7: Case analysis\nUnit 8: Model assumptions\nUnit 9: Transformations\nUnit 10: Interactive models with two quantitative predictors\nUnit 11: Interactive models with quantitative and dichotomous predictors\nUnit 12: Interactive models with two dichotomous predictors\nUnit 13: Categorical Predictors w/ &gt; 2 levels",
    "crumbs": [
      "Slide Decks"
    ]
  },
  {
    "objectID": "web3_required_readings.html",
    "href": "web3_required_readings.html",
    "title": "Readings",
    "section": "",
    "text": "Unit 1: Overview",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-1-overview",
    "href": "web3_required_readings.html#unit-1-overview",
    "title": "Readings",
    "section": "",
    "text": "Required\n\nJudd et a., Chapter 1, Introduction to Data Analysis\n\n\n\nSupplemental\n\nNone",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-2-sampling-distributions",
    "href": "web3_required_readings.html#unit-2-sampling-distributions",
    "title": "Readings",
    "section": "Unit 2: Sampling Distributions",
    "text": "Unit 2: Sampling Distributions\n\nRequired\n\nJudd et al. Chapter 2, Definitions of Error and Parameter Estimates\nJudd et al. Chapter 3, Models of Error and Sampling Distributions\n\n\n\nSupplemental\n\nToothacker, L. E., & Miller, L. (1986) Sampling Distributions. In Introductory Statistics for the Behavioral Sciences.",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-3-one-parameter-model",
    "href": "web3_required_readings.html#unit-3-one-parameter-model",
    "title": "Readings",
    "section": "Unit 3: One Parameter Model",
    "text": "Unit 3: One Parameter Model\n\nRequired\n\nJudd et al. Chapter 4. Simple Models: Statistical Inferences about Parameter Values\n\n\n\nSupplemental\n\nNone",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-4-one-quantitative-predictor",
    "href": "web3_required_readings.html#unit-4-one-quantitative-predictor",
    "title": "Readings",
    "section": "Unit 4: One Quantitative Predictor",
    "text": "Unit 4: One Quantitative Predictor\n\nRequired\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Chapter 5. Simple regression: Estimating models with a single continuous predictor. In Data Analysis: A Model Comparison Approach\n\n\n\nSupplemental\n\nNamboodiri, K. (1984). Matrix algebra: An introduction (Quantitative Applications in the Social Sciences). Sage Publications.",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-5-one-dichotomous-predictor",
    "href": "web3_required_readings.html#unit-5-one-dichotomous-predictor",
    "title": "Readings",
    "section": "Unit 5: One Dichotomous Predictor",
    "text": "Unit 5: One Dichotomous Predictor\n\nRequired\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Chapter 8. One-Way ANOVA (pp. 168-178). In Data Analysis: A Model Comparison Approach.\n\n\n\nSupplemental\n\nNone",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-6-two-predictors",
    "href": "web3_required_readings.html#unit-6-two-predictors",
    "title": "Readings",
    "section": "Unit 6: Two Predictors",
    "text": "Unit 6: Two Predictors\n\nRequired\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Chapter 6. Multiple regression: Models with multiple continuous predictors. In Data Analysis: A Model Comparison Approach. pp. 103-116.\n\n\n\nSupplemental\n\nFritz, C. O., Morris, P. E., & Richler, J. J. (2012). Effect size estimates: Current use, calculations, and interpretation. Journal of Exp. Psychology: General, 141, 2-18.",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-7-case-analysis",
    "href": "web3_required_readings.html#unit-7-case-analysis",
    "title": "Readings",
    "section": "Unit 7: Case Analysis",
    "text": "Unit 7: Case Analysis\n\nRequired\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Chapter 13. Outliers and Ill-Mannered Error. In Data Analysis: A Model Comparison Approach. pp. 314-327.\n\n\n\nSupplemental\n\nFox, J. (1991). Regression diagnostics. SAGE Series (#79): Quantitative Applications in the Social Science.\nTabachnick, B. G., & Fidel, L. S. (2018). Cleaning up your act: Screening data prior to analysis. In Using Multivariate Statistics.",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-8-model-assumptions",
    "href": "web3_required_readings.html#unit-8-model-assumptions",
    "title": "Readings",
    "section": "Unit 8: Model Assumptions",
    "text": "Unit 8: Model Assumptions\n\nRequired\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Chapter 13. Outliers and Ill-Mannered Error. In Data Analysis: A Model Comparison Approach. pp. 327-338.\n\n\n\nSupplemental\n\nFox, J. (1991). Regression diagnostics. SAGE Series (#79): Quantitative Applications in the Social Science.",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web3_required_readings.html#unit-9-transformations",
    "href": "web3_required_readings.html#unit-9-transformations",
    "title": "Readings",
    "section": "Unit 9: Transformations",
    "text": "Unit 9: Transformations\n\nRequired\n\nFox, J. (2008). Transforming Data (Chapter 4).\n\n\n\nSupplemental",
    "crumbs": [
      "Readings"
    ]
  },
  {
    "objectID": "web4_application_assignments.html",
    "href": "web4_application_assignments.html",
    "title": "Application Assignments",
    "section": "",
    "text": "Week 01: Due Wednesday, 9/11 at 1:30 pm",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-01-due-wednesday-911-at-130-pm",
    "href": "web4_application_assignments.html#week-01-due-wednesday-911-at-130-pm",
    "title": "Application Assignments",
    "section": "",
    "text": "instructions\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-02-due-wednesday-0918-at-130-pm",
    "href": "web4_application_assignments.html#week-02-due-wednesday-0918-at-130-pm",
    "title": "Application Assignments",
    "section": "Week 02: Due Wednesday, 09/18 at 1:30 pm",
    "text": "Week 02: Due Wednesday, 09/18 at 1:30 pm\n\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-03-due-friday-0927-at-830-am",
    "href": "web4_application_assignments.html#week-03-due-friday-0927-at-830-am",
    "title": "Application Assignments",
    "section": "Week 03: Due Friday, 09/27 at 8:30 am",
    "text": "Week 03: Due Friday, 09/27 at 8:30 am\n\nggplot cheatsheet\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-04-due-friday-1004-at-830-am",
    "href": "web4_application_assignments.html#week-04-due-friday-1004-at-830-am",
    "title": "Application Assignments",
    "section": "Week 04: Due Friday, 10/04 at 8:30 am",
    "text": "Week 04: Due Friday, 10/04 at 8:30 am\n\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-05-no-application-assignment.-application-exam-week",
    "href": "web4_application_assignments.html#week-05-no-application-assignment.-application-exam-week",
    "title": "Application Assignments",
    "section": "Week 05: No application assignment. Application exam week!",
    "text": "Week 05: No application assignment. Application exam week!",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-06-due-friday-1018-at-830-am",
    "href": "web4_application_assignments.html#week-06-due-friday-1018-at-830-am",
    "title": "Application Assignments",
    "section": "Week 06: Due Friday, 10/18 at 8:30 am",
    "text": "Week 06: Due Friday, 10/18 at 8:30 am\n\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-07-due-friday-1025-at-830-am",
    "href": "web4_application_assignments.html#week-07-due-friday-1025-at-830-am",
    "title": "Application Assignments",
    "section": "Week 07: Due Friday, 10/25 at 8:30 am",
    "text": "Week 07: Due Friday, 10/25 at 8:30 am\n\nshell\niq data\nbrain data\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-08-due-friday-1101-at-830-am",
    "href": "web4_application_assignments.html#week-08-due-friday-1101-at-830-am",
    "title": "Application Assignments",
    "section": "Week 08: Due Friday, 11/01 at 8:30 am",
    "text": "Week 08: Due Friday, 11/01 at 8:30 am\n\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-09-due-friday-1108-at-830-am",
    "href": "web4_application_assignments.html#week-09-due-friday-1108-at-830-am",
    "title": "Application Assignments",
    "section": "Week 09: Due Friday, 11/08 at 8:30 am",
    "text": "Week 09: Due Friday, 11/08 at 8:30 am\n\nshell\ndata\nkey: html; qmd",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-10-no-application-assignnment.-application-exam-week",
    "href": "web4_application_assignments.html#week-10-no-application-assignnment.-application-exam-week",
    "title": "Application Assignments",
    "section": "Week 10: No application assignnment. Application exam week!",
    "text": "Week 10: No application assignnment. Application exam week!",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-11-due-friday-1122-at-830-am",
    "href": "web4_application_assignments.html#week-11-due-friday-1122-at-830-am",
    "title": "Application Assignments",
    "section": "Week 11: Due Friday, 11/22 at 8:30 am",
    "text": "Week 11: Due Friday, 11/22 at 8:30 am\n\nshell\ndata",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-12-due-friday-1129-at-830-am",
    "href": "web4_application_assignments.html#week-12-due-friday-1129-at-830-am",
    "title": "Application Assignments",
    "section": "Week 12: Due Friday 11/29 at 8:30 am",
    "text": "Week 12: Due Friday 11/29 at 8:30 am\n\nshell\ndata",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-13-no-application-assignment.-thanksgiving-break",
    "href": "web4_application_assignments.html#week-13-no-application-assignment.-thanksgiving-break",
    "title": "Application Assignments",
    "section": "Week 13: No application assignment. Thanksgiving Break",
    "text": "Week 13: No application assignment. Thanksgiving Break",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-14-due-friday-1213-at-830-am",
    "href": "web4_application_assignments.html#week-14-due-friday-1213-at-830-am",
    "title": "Application Assignments",
    "section": "Week 14: Due Friday 12/13 at 8:30 am",
    "text": "Week 14: Due Friday 12/13 at 8:30 am",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web4_application_assignments.html#week-15-no-application-assignment.-application-exam-week",
    "href": "web4_application_assignments.html#week-15-no-application-assignment.-application-exam-week",
    "title": "Application Assignments",
    "section": "Week 15: No application assignment. Application exam week!",
    "text": "Week 15: No application assignment. Application exam week!",
    "crumbs": [
      "Application Assignments"
    ]
  },
  {
    "objectID": "web5_labs.html",
    "href": "web5_labs.html",
    "title": "Lab Materials",
    "section": "",
    "text": "Week 1: 09/06",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-1-0906",
    "href": "web5_labs.html#week-1-0906",
    "title": "Lab Materials",
    "section": "",
    "text": "Installing R, R Studio, & Quarto\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-2-0913",
    "href": "web5_labs.html#week-2-0913",
    "title": "Lab Materials",
    "section": "Week 2: 09/13",
    "text": "Week 2: 09/13\n\nLab 02 data\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-3-0920",
    "href": "web5_labs.html#week-3-0920",
    "title": "Lab Materials",
    "section": "Week 3: 09/20",
    "text": "Week 3: 09/20\n\nLab 03 data\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-4-0927",
    "href": "web5_labs.html#week-4-0927",
    "title": "Lab Materials",
    "section": "Week 4: 09/27",
    "text": "Week 4: 09/27\n\nLab 04 quantitative data\nLab 04 dichotomous data\nStudent shell\nTA notebook: qmd; html\nVideo recording of lab",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-5-1004",
    "href": "web5_labs.html#week-5-1004",
    "title": "Lab Materials",
    "section": "Week 5: 10/04",
    "text": "Week 5: 10/04\n\nNo data file (using built-in data from R)\nStudent shell\nTA notebook: qmd; html\nVideo recording of lab",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-6-1011",
    "href": "web5_labs.html#week-6-1011",
    "title": "Lab Materials",
    "section": "Week 6: 10/11",
    "text": "Week 6: 10/11\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-7-1018",
    "href": "web5_labs.html#week-7-1018",
    "title": "Lab Materials",
    "section": "Week 7: 10/18",
    "text": "Week 7: 10/18\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-8-1025",
    "href": "web5_labs.html#week-8-1025",
    "title": "Lab Materials",
    "section": "Week 8: 10/25",
    "text": "Week 8: 10/25\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-9-1101",
    "href": "web5_labs.html#week-9-1101",
    "title": "Lab Materials",
    "section": "Week 9: 11/01",
    "text": "Week 9: 11/01\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-10-1108",
    "href": "web5_labs.html#week-10-1108",
    "title": "Lab Materials",
    "section": "Week 10: 11/08",
    "text": "Week 10: 11/08\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-11-1115",
    "href": "web5_labs.html#week-11-1115",
    "title": "Lab Materials",
    "section": "Week 11: 11/15",
    "text": "Week 11: 11/15\n\ndata\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web5_labs.html#week-12-1122",
    "href": "web5_labs.html#week-12-1122",
    "title": "Lab Materials",
    "section": "Week 12: 11/22",
    "text": "Week 12: 11/22\n\nStudent shell\nTA notebook: qmd; html",
    "crumbs": [
      "Lab Materials"
    ]
  },
  {
    "objectID": "web6_exams.html",
    "href": "web6_exams.html",
    "title": "Exams",
    "section": "",
    "text": "Application Exam 1",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web6_exams.html#application-exam-1",
    "href": "web6_exams.html#application-exam-1",
    "title": "Exams",
    "section": "",
    "text": "data\nexam shell",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web6_exams.html#application-exam-2",
    "href": "web6_exams.html#application-exam-2",
    "title": "Exams",
    "section": "Application Exam 2",
    "text": "Application Exam 2\n\ndata\nexam shell",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web6_exams.html#application-exam-3",
    "href": "web6_exams.html#application-exam-3",
    "title": "Exams",
    "section": "Application Exam 3",
    "text": "Application Exam 3",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web6_exams.html#concepts-exam-1",
    "href": "web6_exams.html#concepts-exam-1",
    "title": "Exams",
    "section": "Concepts Exam 1",
    "text": "Concepts Exam 1",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web6_exams.html#concepts-exam-2",
    "href": "web6_exams.html#concepts-exam-2",
    "title": "Exams",
    "section": "Concepts Exam 2",
    "text": "Concepts Exam 2",
    "crumbs": [
      "Exams"
    ]
  },
  {
    "objectID": "web7_videos.html",
    "href": "web7_videos.html",
    "title": "Video Recordings from Lab and Lecture",
    "section": "",
    "text": "Lab",
    "crumbs": [
      "Video Recordings from Lab and Lecture"
    ]
  },
  {
    "objectID": "web7_videos.html#lab",
    "href": "web7_videos.html#lab",
    "title": "Video Recordings from Lab and Lecture",
    "section": "",
    "text": "Week 4\nWeek 5\nWeek 6\nWeek 7 - part I\nWeek 7 - part II\nWeek 8\nWeek 9\nWeek 10\nWeek 11",
    "crumbs": [
      "Video Recordings from Lab and Lecture"
    ]
  },
  {
    "objectID": "app1_exam_review.html",
    "href": "app1_exam_review.html",
    "title": "Exam Concepts",
    "section": "",
    "text": "Unit 1 - Overview",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-1---overview",
    "href": "app1_exam_review.html#unit-1---overview",
    "title": "Exam Concepts",
    "section": "",
    "text": "What are the four levels of the General Linear model\nHow do the number of \\(X\\) variables differ across these levels?\nNumber of \\(Y\\) variables differ across these levels?\nExamples of special case analyses across these levels?",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-2---sampling-distributions",
    "href": "app1_exam_review.html#unit-2---sampling-distributions",
    "title": "Exam Concepts",
    "section": "Unit 2 - Sampling Distributions",
    "text": "Unit 2 - Sampling Distributions\n\npopulations vs. samples\nparameters vs. parameter estimates\nWhat is sampling error?\nThe logic and procedure for null hypothesis signficance testing\nConnecting NHST to sampling distributions\nRaw score distribitions vs. sampling distributions\nThe Central Limit Theorem\nWhat is an unbiased estimator?\nWhat is the standard error for a parameter estimate and how does that connect to sampling distributions\nWhat factors affect the standard error of the mean\nHow does the one-sample z-test connect sampling distributions and standard errors.\nWhat is the difference between the one-sample z-test and the one sample t-test",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-3---one-parameter-models",
    "href": "app1_exam_review.html#unit-3---one-parameter-models",
    "title": "Exam Concepts",
    "section": "Unit 3 - One parameter models",
    "text": "Unit 3 - One parameter models\n\nWhat are the three uses of models\nWhat is the equation for a one parameter model for \\(Y_i\\) and for \\(\\hat{Y}_i\\)\nWhat is an error or residual in the context of a model. Define it in terms of \\(Y_i\\) and \\(\\hat{Y}_i\\)\nWhat are two reasonable methods for estimating total model error\nWhat are three important properties of parameter estimates? Define each\nWhy do we prefer the sum of squared errors over the sum of absolute errors?\nWhat is the interpretation of \\(b_0\\) if we estimate it by minimizing the sum of squared errors vs. the sum of absolute errors?\nParameter estimates in our models miminize what?\nHow do we use a t-test the null hypothesis that \\(b_0\\) is zero in a one parameter model? Connect this to the sampling distribution of \\(b_0\\)\nHow do we use an F-test and model comparison to test the null hypothesis that \\(b_0\\) is zero in a one parameter model? How do we use sum of squared errors in this comparison? What is the compact and augmented model used to test the null about \\(b_0\\) = 0. How would you change these models if you had a null hypothesis about some value other than 0?\nWhat is a confidence interval for a parameter estimate? Connect the confidence interval to the standard error of that parameter estimate\nHow can we use a confidence interval to test the null hypothesis that \\(b_0\\) (or any parameter estimate) is zero?\nThe test of \\(b_0\\) in a one parameter model is equivalent to what special case analysis?\nWhat is the formula for the t-test to test the null hypothesis that \\(b_0\\) (or any parameter estimate) is zero? How would you modify this formula to test a null hypothesis about a value other than 0.",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-4---two-parameter-models",
    "href": "app1_exam_review.html#unit-4---two-parameter-models",
    "title": "Exam Concepts",
    "section": "Unit 4 - Two parameter models",
    "text": "Unit 4 - Two parameter models\n\nWhat is the equation for a two parameter model for \\(Y_i\\) and for \\(\\hat{Y}_i\\)\nWhat is the interpretation of \\(b_0\\) and \\(b_1\\) in a two parameter model?\nIdentify \\(b_0\\) and \\(b_1\\) on a line plot of the data\nHow are \\(b_0\\) and \\(b_1\\) estimated in a two parameter model?\nCompare the interpretation of \\(b_0\\) in a one vs. two parameter model\nHow do we test null hypotheses about \\(b_0\\) and \\(b_1\\) in a two parameter model using a t-test? Connect this to the sampling distribution of \\(b_0\\) and \\(b_1\\)A\nHow do we test null hypotheses about \\(b_0\\) and \\(b_1\\) in a two parameter model using an F-test and model comparison? What are the associated compact and augmented models for each test?\nWhat are the degrees of freedom equal to for the t-test and F-test\nWhat would the SSE be for two parameter model that perfectly predicts \\(Y\\). What would it be if there was no relationship between \\(X\\) and \\(Y\\)?\nWhat is the impact of mean-centering \\(X\\) on the interpretation of \\(b_0\\) and \\(b_1\\) in a two parameter model?\nWhat is the formula for the confidence interval for a parameter estimate in a two parameter model? Connect the confidence interval to the standard error of that parameter estimate\nWhat is \\(R^2\\) and how is it calculated in a two parameter model using SSE or using variances of \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(e_i\\)?\nWhat this the relationship between \\(R^2\\) and \\(r_xy\\) in the two parameter model?",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-5---one-dichotomous-predictor",
    "href": "app1_exam_review.html#unit-5---one-dichotomous-predictor",
    "title": "Exam Concepts",
    "section": "Unit 5 - One Dichotomous Predictor",
    "text": "Unit 5 - One Dichotomous Predictor\n\nHow do we handle dichotomous categorical variables in linear models?\nWhat are the two coding schemes we learned for coding regressors (\\(X\\)s) for dichtomous categorical predictors\nHow do we interpret the parameter estimates for each coding scheme?\nHow would values or interpretations of \\(b_0\\) and \\(b_1\\) change if we used other coding schemes (e.g., -1 vs. 1, 2 vs. 0)?\nWhat are the typical consequences of dichotomizing a quantitative predictor? Should we generally do this or avoid it?\nHow does the t-test or model comparison approach work for testing parameter estimates in a model with a dichotomous predictor?\nIdentify \\(b_0\\) and \\(b_1\\) on a line plot of the data",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app1_exam_review.html#unit-6---models-with-two-or-more-predictors",
    "href": "app1_exam_review.html#unit-6---models-with-two-or-more-predictors",
    "title": "Exam Concepts",
    "section": "Unit 6 - Models with Two or More Predictors",
    "text": "Unit 6 - Models with Two or More Predictors\n\nWhat is the equation for a model with two predictors for \\(Y_i\\) and for \\(\\hat{Y}_i\\)? What is the general form for \\(k\\) predictors?\nWhat is the interpretation of \\(b_0\\), \\(b_1\\), and \\(b_2\\) in a model with two predictors?\nWhat are the five benefits of using multiple predictors in a model?\nWhat factors affect the standard error of a parameter estimate in a model with multiple predictors? (You do not need to know the exact formula for this but you do need to know all the components and the direction of their impact on the SE)\nWhat is \\(R^2_j\\)?\nWhat is the problem of multicolinearity?\nHow do you identify if multicolinarity is a problem in a model?\nWhat are the appropriate compact and augmented models for testing any parameter estimate in a model with multiple predictors?\nWhat is the relationship between the SE for a parameter estimate and power to test the null hypothesis about that parameter estimate? Connect this to your understanding of the sampling distribution for a parameter estimate\nWhat is the relationship between the SE for a parameter estimate and the precision of the parameter estimate? Connect this to your understanding of the confidence interval and the sampling distribution for a parameter estimate.\nWhat are the total, direct, and indirect/spurious effects of \\(X\\)s in a two predictor model? How do you calculate each? What models do you use to calculate each?\nWhat are the three variance based effect size estimates for models with two or more predictors?\n\nDefine them in terms of the variances of \\(Y_i\\), \\(\\hat{Y}_i\\), and \\(e_i\\)\nDefine them in terms of sums of squares and models/model comparisons\nWhat is the interpreation of each?\nLink them to the areas of the Venn diagrams\nCalculate them from sums of squares in Venn diagrams\nHow are they similar and different from each other?\nWhen are each generally used?\n\nIdentify \\(b_0\\), \\(b_1\\), and \\(b_2\\) on a line plot of the data from a two predictor model with a dichotomous and quantitative predictor",
    "crumbs": [
      "Appendices",
      "Exam Concepts"
    ]
  },
  {
    "objectID": "app3_contrasts.html",
    "href": "app3_contrasts.html",
    "title": "Monte Carlo Simulation of Contrast Approaches",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n3 Groups with No Group Differences (Type I Errors)\nSet up simulation characteristics for Null Findings.\nThis will allow us to determine Type I error rates because any signficant effect is a type I error given we have set the population effect to 0\n\n\nCode\n# simulate N experiments\nn_experiments &lt;- 20000\n\n# group means\nm_1 &lt;- 10\nm_2 &lt;- 10\nm_3 &lt;- 10\n\nsd &lt;- 20 # sd for y\nn &lt;- 50 # group size\n\n# set up x as factor\nx &lt;-  factor(c(rep(\"a\", n), rep(\"b\", n), rep(\"c\", n)))  \n\nset.seed(1234567)\n\n\n\n1. POCs - all focal (separate research questions)\n\n\nCode\nsimulate_poc &lt;- function(i) {\n  # vector of y for three groups\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)&lt;- matrix(c(2, -1, -1,\n                         0,  1, -1), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] &lt; 0.05,\n         sig_c2 = results$p.value[3] &lt; 0.05,\n         sig_any = any(results$p.value[2:3] &lt; 0.05))\n}\n\ntype1_poc &lt;- map(1:n_experiments, simulate_poc) |&gt; \n  list_rbind()\n\n\nResults (to make clear what function returns)\n\n\nCode\ntype1_poc |&gt; head()\n\n\n# A tibble: 6 × 4\n    sim sig_c1 sig_c2 sig_any\n  &lt;int&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt;  \n1     1 FALSE  FALSE  FALSE  \n2     2 FALSE  FALSE  FALSE  \n3     3 TRUE   FALSE  TRUE   \n4     4 FALSE  FALSE  FALSE  \n5     5 FALSE  FALSE  FALSE  \n6     6 FALSE  FALSE  FALSE  \n\n\nTest wise type I error for each contrast is 5%\n\n\nCode\nmean(type1_poc$sig_c1)\n\n\n[1] 0.0491\n\n\nCode\nmean(type1_poc$sig_c2)\n\n\n[1] 0.0545\n\n\nThe results across contrasts are independent because they come from different families\n\n\nCode\ncor(type1_poc$sig_c1, type1_poc$sig_c2) |&gt; round(2)\n\n\n[1] 0.01\n\n\nTo be clear, the family-wise type I error across the set is 10% BUT often not considered in same family so not important?\n\n\nCode\nmean(type1_poc$sig_any)\n\n\n[1] 0.1002\n\n\n\n2. Dummy contrasts from one model (3 levels; no protection).\n\n\nCode\nsimulate_dummy &lt;- function(i) {\n  # vector of y for three groups\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results &lt;- lm(y ~ x) |&gt; \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] &lt; 0.05,\n         sig_d2 = results$p.value[3] &lt; 0.05,\n         sig_any = any(c(results$p.value[2:3]) &lt; 0.05))\n}\n\ntype1_dummy &lt;- map(1:n_experiments, simulate_dummy) |&gt; \n  list_rbind()\n\n\nTest-wise Type I for each contrast is 5%\n\n\nCode\nmean(type1_dummy$sig_d1)\n\n\n[1] 0.0508\n\n\nCode\nmean(type1_dummy$sig_d2)\n\n\n[1] 0.051\n\n\nBut these are from same family (results of contrasts are related)\n\n\nCode\ncor(type1_dummy$sig_d1, type1_dummy$sig_d2) |&gt; round(2)\n\n\n[1] 0.15\n\n\nTherefore, family-wise error rate is higher (but not 10% because contrasts are dependent/related)\n\n\nCode\nmean(type1_dummy$sig_any)\n\n\n[1] 0.0919\n\n\n\n3. All (3) pairwise contrasts (no protection).\n\n\nCode\nsimulate_pair &lt;- function(i) {\n  # vector of y for three groups\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n \n  # fit second model \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results_3$p.value[2] &lt; 0.05,\n         sig_d2 = results_3$p.value[3] &lt; 0.05,\n         sig_d3 = results_1$p.value[2] &lt; 0.05,\n         sig_any = any(c(results_3$p.value[2:3], results_1$p.value[2]) &lt; 0.05))\n}\n\ntype1_pair &lt;- map(1:n_experiments, simulate_pair) |&gt; \n  list_rbind()\n\n\nTest-wise Type I for each contrast is 5%\n\n\nCode\nmean(type1_pair$sig_d1)\n\n\n[1] 0.0501\n\n\nCode\nmean(type1_pair$sig_d2)\n\n\n[1] 0.0498\n\n\nCode\nmean(type1_pair$sig_d3)\n\n\n[1] 0.0497\n\n\nBut these are from same family (results of contrasts are related)\n\n\nCode\ncor(type1_pair$sig_d1, type1_pair$sig_d2) |&gt; round(2)\n\n\n[1] 0.14\n\n\nCode\ncor(type1_pair$sig_d1, type1_pair$sig_d3) |&gt; round(2)\n\n\n[1] 0.14\n\n\nCode\ncor(type1_pair$sig_d2, type1_pair$sig_d3) |&gt; round(2)\n\n\n[1] 0.12\n\n\nFamily-wise error rate is higher (but not 15% because contrasts are dependent/related)\n\n\nCode\nmean(type1_pair$sig_any)\n\n\n[1] 0.12315\n\n\n\n4. Fisher LSD with 3 pairwise comparisons\n\n\nCode\nsimulate_fisher &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  sig_omnibus &lt;- anova(lm(y ~ x))$`Pr(&gt;F)`[1] &lt; 0.05\n  \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] &lt; 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] &lt; 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] &lt; 0.05,\n         sig_any = sig_omnibus && any(c(results_3$p.value[2:3], results_1$p.value[2]) &lt; 0.05))\n}\n\ntype1_fish &lt;- map(1:n_experiments, simulate_fisher) |&gt; \n  list_rbind()\n\n\nTest-wise Type I for each contrast is &lt; 5% (too conservative!)\n\n\nCode\nmean(type1_fish$sig_d1)\n\n\n[1] 0.02485\n\n\nCode\nmean(type1_fish$sig_d2)\n\n\n[1] 0.02475\n\n\nCode\nmean(type1_fish$sig_d3)\n\n\n[1] 0.02555\n\n\nThese are from same family (results of contrasts are even more related)\n\n\nCode\ncor(type1_fish$sig_d1, type1_fish$sig_d2) |&gt; round(2)\n\n\n[1] 0.3\n\n\nCode\ncor(type1_fish$sig_d1, type1_fish$sig_d3) |&gt; round(2)\n\n\n[1] 0.31\n\n\nCode\ncor(type1_fish$sig_d2, type1_fish$sig_d3) |&gt; round(2)\n\n\n[1] 0.3\n\n\nFamily-wise error rate is controlled at 5%\n\n\nCode\nmean(type1_fish$sig_any)\n\n\n[1] 0.0509\n\n\n\n5. Holm-Bonferroni correction with 3 pairwise comparisons\n\n\nCode\nsimulate_hb &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  p_contrasts &lt;- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] &lt; 0.05,\n         sig_d2 = p_contrasts[2] &lt; 0.05,\n         sig_d3 = p_contrasts[3] &lt; 0.05,\n         sig_any = any(p_contrasts &lt; 0.05))\n}\n\ntype1_hb &lt;- map(1:n_experiments, simulate_hb) |&gt; \n  list_rbind()\n\n\nTest-wise Type I is well under 5% (too conservative!)\n\n\nCode\nmean(type1_hb$sig_d1)\n\n\n[1] 0.01865\n\n\nCode\nmean(type1_hb$sig_d2)\n\n\n[1] 0.01765\n\n\nCode\nmean(type1_hb$sig_d3)\n\n\n[1] 0.01845\n\n\nThese are from same family (results of contrasts are related)\n\n\nCode\ncor(type1_hb$sig_d1, type1_hb$sig_d2) |&gt; round(2)\n\n\n[1] 0.19\n\n\nCode\ncor(type1_hb$sig_d1, type1_hb$sig_d3) |&gt; round(2)\n\n\n[1] 0.18\n\n\nCode\ncor(type1_hb$sig_d2, type1_hb$sig_d3) |&gt; round(2)\n\n\n[1] 0.2\n\n\nFamily-wise error rate is controlled at 5%\n\n\nCode\nmean(type1_hb$sig_any)\n\n\n[1] 0.04365\n\n\n\n\n\n3 Groups with One Group Difference (Type II Errors)\nNow lets consider Type II errors. This is too often neglected in these discussions. However it is also complicated because there are LOTS of different ways that the population effects could be set up and its not necessarily true that the same method would be more powerful across these settings. You should consider these simulations as only a start to comparing the power of these methods.\nHere we update the pattern of means such that one group is different from the other two but the other two group means are equal\n\n\nCode\nm_1 &lt;- 10\nm_2 &lt;- 10\nm_3 &lt;- 20\n\n\n\n1. Dummy contrasts from one model (3 levels; no protection).\nThis pattern of means is well-suited to using dummy codes with the third group as reference.\nThat said, if we only tested these two contrasts, we couldnt conclude anything about differences between groups 1 and 2.\n\n\nCode\nsimulate_dummy_2 &lt;- function(i) {\n  # vector of y for three groups\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results &lt;- lm(y ~ x) |&gt; \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] &lt; 0.05,\n         sig_d2 = results$p.value[3] &lt; 0.05,\n         sig_both = all(c(results$p.value[2:3]) &lt; 0.05))\n}\n\ntype2_dummy &lt;- map(1:n_experiments, simulate_dummy_2) |&gt; \n  list_rbind()\n\n\nHere is power for the two contrasts that should be significant and for finding both significant.\nPower is low (70%) for the individual tests but what we would expect given the effect size and sample size. What we care about is relative power across the approaches.\nBut again, we should also note that this method doesnt inform us about differences between group 1 and 2\n\n\nCode\nmean(type2_dummy$sig_d1)\n\n\n[1] 0.70465\n\n\nCode\nmean(type2_dummy$sig_d2)\n\n\n[1] 0.7037\n\n\nCode\nmean(type2_dummy$sig_both)\n\n\n[1] 0.56145\n\n\n2. Fisher LSD with 3 pairwise comparisons\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n\nCode\nsimulate_fish_2 &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  sig_omnibus &lt;- anova(lm(y ~ x))$`Pr(&gt;F)`[1] &lt; 0.05\n\n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] &lt; 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] &lt; 0.05,\n         sig_both = sig_omnibus && all(c(results_3$p.value[2:3]) &lt; 0.05))\n}\n\ntype2_fish &lt;- map(1:n_experiments, simulate_fish_2) |&gt; \n  list_rbind()\n\n\nPower is lower for the two individual contrasts (63-64%) but we get the third contrasts to show that G1 and G2 are not difference (with 5% false alarm rate).\n\n\nCode\nmean(type2_fish$sig_d1)\n\n\n[1] 0.6385\n\n\nCode\nmean(type2_fish$sig_d2)\n\n\n[1] 0.6322\n\n\nCode\nmean(type2_fish$sig_both)\n\n\n[1] 0.5464\n\n\n\n2. Holm-Bonferroni correction with 3 pairwise comparisons\n\n\nCode\nsimulate_hb_2 &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n\n  p_contrasts &lt;- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] &lt; 0.05,\n         sig_d2 = p_contrasts[2] &lt; 0.05,\n         sig_both = all(p_contrasts[1:2] &lt; 0.05))\n} \n\ntype2_hb &lt;- map(1:n_experiments, simulate_hb_2) |&gt; \n  list_rbind()\n\n\nHB is worse still on power for the individual contrasts (55-56%)\n\n\nCode\nmean(type2_hb$sig_d1)\n\n\n[1] 0.56085\n\n\nCode\nmean(type2_hb$sig_d2)\n\n\n[1] 0.5684\n\n\nCode\nmean(type2_hb$sig_both)\n\n\n[1] 0.4326\n\n\n\n4. POCs - Assuming we were right about the pattern of means\nPOCs don’t fit perfectly to this setting. However, if in this instance our theory predicts only group three different from groups 1 and 2, we could test only that contrast or we might test the second contrast to demonstrate it was NOT significant.\n\n\nCode\nsimulate_poc_2 &lt;- function(i) {\n  # vector of y for three groups\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)&lt;- matrix(c(-1, -1, 2,\n                          1, -1, 0), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] &lt; 0.05)\n}\n\ntype2_poc &lt;- map(1:n_experiments, simulate_poc_2) |&gt; \n  list_rbind()\n\n\nHere we only care about the power for the first effect. Clearly, the best power (~81%) if this is sufficient. We would likely want the second contrast to be non-significant to demonstrate that the effect is specific to group 3. This would false alarm at 5%.\n\n\nCode\nmean(type2_poc$sig_c1)\n\n\n[1] 0.81675\n\n\n\n\n\n3 Groups with All Groups Different (Type II Errors)\n\n\nCode\nm_1 &lt;- 10\nm_2 &lt;- 20\nm_3 &lt;- 30\n\n\n1. Dummy contrasts from one model (3 levels; no protection).\nThis approach doesn’t make sense because we want to test all three contrasts (assuming our theory correctly predicted all groups different)\n2. Fisher LSD with 3 pairwise comparisons\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n\nCode\nsimulate_fish_3 &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  sig_omnibus &lt;- anova(lm(y ~ x))$`Pr(&gt;F)`[1] &lt; 0.05\n  \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] &lt; 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] &lt; 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] &lt; 0.05,\n         sig_all = sig_omnibus && all(c(results_3$p.value[2:3], results_1$p.value[2]) &lt; 0.05))\n}\n\ntype2_fish_3 &lt;- map(1:n_experiments, simulate_fish_3) |&gt; \n  list_rbind()\n\n\nPower is much better for G1 vs G3 (~99%) because its a bigger mean difference than for the other two (w/ ~ 70% power). But again, its about relative power now. We need to compare to other methods.\n\n\nCode\nmean(type2_fish_3$sig_d1)\n\n\n[1] 0.99605\n\n\nCode\nmean(type2_fish_3$sig_d2)\n\n\n[1] 0.69935\n\n\nCode\nmean(type2_fish_3$sig_d3)\n\n\n[1] 0.69805\n\n\nCode\nmean(type2_fish_3$sig_all)\n\n\n[1] 0.4333\n\n\n\n2. Holm-Bonferroni correction with 3 pairwise comparisons\n\n\nCode\nsimulate_hb_3 &lt;- function(i) {\n  y &lt;- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) &lt;- contr.treatment(levels(x), base = 3) \n  results_3 &lt;- lm(y ~ x) |&gt; \n    tidy()\n  \n  contrasts(x) &lt;- contr.treatment(levels(x), base = 1) \n  results_1 &lt;- lm(y ~ x) |&gt; \n    tidy()\n\n  p_contrasts &lt;- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] &lt; 0.05,\n         sig_d2 = p_contrasts[2] &lt; 0.05,\n         sig_d3 = p_contrasts[3] &lt; 0.05,\n         sig_all = all(p_contrasts[1:3] &lt; 0.05))\n} \n\ntype2_hb_3 &lt;- map(1:n_experiments, simulate_hb_3) |&gt; \n  list_rbind()\n\n\nHB is a bit worse power for the the smaller individual contrasts (66-67%)\n\n\nCode\nmean(type2_hb_3$sig_d1)\n\n\n[1] 0.99555\n\n\nCode\nmean(type2_hb_3$sig_d2)\n\n\n[1] 0.6619\n\n\nCode\nmean(type2_hb_3$sig_d3)\n\n\n[1] 0.66925\n\n\nCode\nmean(type2_hb_3$sig_all)\n\n\n[1] 0.4247\n\n\n\n4. POCs - Assuming we were right about the pattern of means\nAgain, not clear exactly how to use POCs in this setting if we expect all groups to be different\n\n\nWhat about with 4 levels?\nHavent done this year but we will see that Type 1 control falls apart for all pairwise with fisher LSD because there are two many pairwise contrasts. Without, that most wouldnt tolerate it. HB will still handle 4 levels with go type 1 control. Power will be lower still though because there will be bigger adjustments to all p-values. POCs can really shine here but ONLY if the pattern of means works. Otherwise, HB is the best bet.",
    "crumbs": [
      "Appendices",
      "Monte Carlo Simulation of Contrast Approaches"
    ]
  }
]