--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 15: The Generalized Linear Model

::: {.callout-important}
# Question

What are the assumptions of **general** linear models?
:::


:::{.fragment}
The general linear model makes the 5 assumptions below. When these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). 


1. **Exact X**: The IVs are assumed to be known exactly (i.e., without measurement error).
2. **Independence**: Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).
3. **Normality**: All residual distributions are normally distributed.
4. **Constant variance**: All residual distributions have a constant variance, $SEE^2$.
5. **Linearity**: All residual distributions (i.e., for each $Y'$) are assumed to have means equal to zero.
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question

What are the consequences of violating each of these assumptions? What options exist when these assumptions are violated?
:::


:::{.fragment}
1. **Exact X**: Biased parameters (to the degree that measurement error exists). Use reliable measures.
2. **Independence**: Inaccurate standard errors, degrees of freedom and significance tests. Use repeated measures or linear mixed effects models or ANCOVA.
3. **Normality**: Inefficient (with large N). Use power transformations, **generalized linear models**.
4. **Constant variance**: Inefficient and inaccurate standard errors. Use power transformations, SE corrections, weighted least squares, **generalized linear models**.
5. **Linearity**: Biased parameter estimates. Use power transformations, polynomial regression, **generalized linear models**.
:::

--------------------------------------------------------------------------------

## Generalized Linear Models

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(broom)
library(skimr)
library(patchwork)
theme_set(theme_classic()) 
```

`glm(formula, family = familytype(link = linkfunction), data = data)`

```{r}
#| echo: false

tibble(Family = c("binomial", "gaussian", "Gamma", "inverse.gaussian", 
                  "poisson", "quasi", "quasibinomial", "quasipoisson"),
       `Default Link Function` = c('(link = "logit")', '(link = "identity")', 
                                   '(link = "inverse")', '(link = "1/mu^2")', 
                                   '(link = "log")', 
                                   '(link = "indentity", variance = "constant")', 
                                   '(link = "logit")', '(link = "log")')) |> 
  kableExtra::kable() |> 
  kableExtra::kable_styling()
```

--------------------------------------------------------------------------------

## An Example

Predicting admission to a grad program in engineering based on quantitative GRE, GPA, and Undergraduate Institution Rank.   

```{r}
#| echo: false

data <- read_csv(here::here("data_lecture/15_grad_admit.csv"),
                 show_col_types = FALSE) |> 
  glimpse()
```

```{r}
data |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

--------------------------------------------------------------------------------

```{r}
m_lm <- lm(admit ~ gpa, data = data)

tidy(m_lm)
```


::: {.callout-important}
# Question

What is the effect of GPA on admission to grad school?
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question

What are the problems with using a general linear model to assess the effects of these predictors on admission outcomes?
:::

:::{.fragment}

1. Residuals will not be normal (not efficient).

2. Residual variance often will not be constant (not efficient, SEs are inaccurate).

3. Relationship will not be linear (parameter estimates biased).

4. Y is not constrained between 0 â€“ 1 (model may make nonsensical predictions).
:::

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(-2, 1.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```


--------------------------------------------------------------------------------


```{r}
gvlma::gvlma(m_lm)
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_lm), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-4, 4, length.out = 100),
                          y = dnorm(seq(-4, 4, length.out = 100), 
                                    mean = 0, sd = sd(rstudent(m_lm)))), 
            linetype = "dashed", color = "blue")
```

```{r}
#| code-fold: true

car::qqPlot(m_lm, id = FALSE, simulate = TRUE,
            main = "QQ Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```


--------------------------------------------------------------------------------

```{r}
#| code-fold: true

tibble(x = predict(m_lm),
       y = rstudent(m_lm)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs.  Predicted Values",
       x = "Predicted Values",
       y = "Studentized Residuals")
```

```{r}
#| cold-fold: true
#| warning: false

car::spreadLevelPlot(m_lm)
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

car::crPlots(m_lm, ask = FALSE)
```

--------------------------------------------------------------------------------

```{r}
m_glm <- glm(admit ~ gpa, data = data, family = binomial(logit))

tidy(m_glm)
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

x <- tibble(gpa = seq(0, 6, length.out = 500))

preds <- predict(m_glm, newdata = x) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(x)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 6)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")

```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| warning: false

ex_data <- tibble(x = c(rep(1:4, 10), rep(5, 10), rep(6:10, 10)),
                 y = c(rep(0, 40), rep(0:1, 5), rep(1, 50))) 

m_ex <- glm(y ~ x, data = ex_data, family = binomial(logit))

preds_ex <- predict(m_ex, newdata = tibble(x = seq(1, 10, length.out = 100))) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(tibble(x = seq(1, 10, length.out = 100)))

ex_data |> 
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = 0, height = .04) +
  scale_x_continuous(limits = c(1, 10), 
                     breaks = c(2, 4, 6, 8, 10)) +
  geom_line(data = preds_ex, aes(x = x, y = value), color = "blue") 
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question

What other non-linear shapes do you know how to model in the general linear model?
:::

:::{.fragment}

1. Simple monotone relationships with power transforms.

2. Quadratic, cubic, etc relationships with polynomial regression. <!--KW: I don't think this was introduced?-->
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false

tibble(x = 1:100,
       y = sqrt(x)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = sqrt(x)")

tibble(x = -100:100,
       y = x + x^2) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2")

tibble(x = -100:100,
       y = x + x^2 + x^3) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2 + x^3")
```

--------------------------------------------------------------------------------

```{r}
#| echo: false

tidy(m_glm)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

**Linear Regression:**   

- Residuals are gaussian.   
- Link function is identity.   

$Y = 1 * (b_0 + b_1X_1 + ... + b_kX_k)$    

**Logistic Regression:**   

- Residuals are binomial   
- Link function is logit (a transformation of the logistic function).   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$\pi$ = probability of $Y = 1$    
$e$ = 2.718 (approx)

--------------------------------------------------------------------------------

## Logs and Exponentials

You are likely familiar with logs using base 10.

```{r}
log10(10)

log10(100)

log10(1000)

log10(1)

log10(15)

log10(0)

log10(.1)
```

--------------------------------------------------------------------------------

The natural log (often abbreviated ln) is similar but uses base $e$ (approx 2.718) rather than base 10.  

```{r}
log(2.718282)

log(10)

log(1)

log(0)
```

--------------------------------------------------------------------------------

The inverse of the natural log is the exponential function: `exp()`.  This function simply raises $e$ to the power of $X$ (whatever value you provide).   

```{r}
exp(1)

exp(2)

exp(0)

exp(-1)
```

Logistic regression uses natural logs and exponentials for the transformations of $Y$ and $X$s.

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

```{r}
#| echo: false

tidy(m_glm)
```

$\pi = \frac{e^{-14.5 + 4.1}}{1 + e^{-14.5 + 4.1X_1}}$   

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = 0$

```{r}
#| echo: false

tibble(x = -10:10,
       b0 = 0,
       b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x))) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line(color = "red") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = 0 \text{ to }1$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, .25, .5, 1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = -1 \text{ to }0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, -.25, -.5, -1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = -5 \text{ to } 5$    

$b_1 = 0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = -5 \text{ to } 5$    

$b_1 = 1$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 1) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

## Odds

Odds = $\frac{\pi}{1-\pi}$   

::: {.callout-important}
# Question

What are the odds of obtaining a head on a fair coin toss?
:::

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = \frac{0.5}{0.5} = 1 [1:1]$ 
:::

::::{.fragment}
::: {.callout-important}
# Question

What would the odds of obtaining a head be if I altered the coin to have a probability of heads = 0.67?
:::
::::

:::{.fragment}
Odds = $\frac{0.67}{1-0.67} = \frac{0.67}{0.33} = 2 [2:1]$ 
:::

--------------------------------------------------------------------------------

$\pi = \frac{\text{odds}}{\text{odds} + 1}$    

::: {.callout-important}
# Question

What is the probability of an event that has an odds of 3?
:::

:::{.fragment}
$\pi = \frac{3}{3 + 1} = .75$ 
:::

:::{.fragment}
**Odds can range from 0 - infinity.**
:::

--------------------------------------------------------------------------------

Odds = $\frac{\pi}{1-\pi}$  

::: {.callout-important}
# Question

What are the approximate odds of getting into grad school with a GPA of 3.5?
:::   


```{r}
#| echo: false

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue") +
  labs(y = "p(admit)") +
  geom_vline(xintercept = 3.5, color = "dark green", linewidth = .75) +
  geom_hline(yintercept = .49, color = "dark green", linewidth = .75)
```


:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = 1[1:1]$  
:::

--------------------------------------------------------------------------------

**Logistic function (probability Y = 1):**   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

$0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1)**    

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$0 \le \text{odds} \le\text{INF}$  


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What are the predicted odds of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$= e^{-14.5 + 4.1 * 3.5}$   

$= e^{-0.15}$  

$= 0.86 [0.86:1]$   

**Note: Probability of 0.50 occurs with odds of 1.**
:::

--------------------------------------------------------------------------------

**Logistic function (probability Y = 1):**   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

$0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1):**    

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$0 \le \text{odds} \le\text{INF}$  

**Convert odds to log-odds(logit function; log-odds of Y = 1):**   

$ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   

$-\text{INF} \le logit \le \text{INF}$  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What are the log-odds (logit) of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$logit= b_0 + b_1X_1 + ... + b_kX_k$ 

$= b_0 + b_1*gpa$

$= -14.5 + 4.1 * 3.5$   

$= -0.15$  

**Note: Odds of 1 occur when logit = 0.**
:::

--------------------------------------------------------------------------------

Log-odds are not very intuitive.    

However, they are a linear function of our regressors. GLM estimates the parameters in the logit function.   

```{r}
#| code-fold: true

tidy(m_glm)
```

$logit(Y) = -14.5 + 4.1*gpa$    

We transform the logit function to either odds or probability functions to convey relationships in meaningful units.   

--------------------------------------------------------------------------------

```{r}
#| echo: false

plot_logit <- preds |> 
  mutate(value = log(value/(1-value))) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(-15, 2)) +
  geom_line(color = "blue") +
  labs(title = "Logit(Y)",
       y = "logit(admit)")

plot_odds <- preds |> 
  mutate(value = value/(1-value)) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(0, 8)) +
  geom_line(color = "blue") +
  labs(title = "Odds(Y)",
       y = "odds(admit)")

plot_prob <- ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  geom_line(data = preds |> 
              filter(!gpa > 4), 
            aes(y = value, x = gpa), color = "blue") +
  labs(title = "Probability(Y)",
       y = "p(admit)")
```

```{r}
#| echo: false

plot_logit + plot_odds + plot_prob
```


--------------------------------------------------------------------------------

Log-odds are a linear function of $X$s, but these parameter estimates are not very descriptive.   

$logit(admission)= -14.5 + 4.1*gpa$    

Not clear how to interpret a parameter estimate of 4.1 for GPA.    

The log-odds of admission increases by 4.1 units for every 1 point increase on GPA??   

```{r}
#| echo: false

plot_logit +
  labs(title = NULL)
```

--------------------------------------------------------------------------------

Odds and probability are more descriptive but they are not linear functions of the $X$s so their parameter estimates aren't very useful to describe effect of $X$s.    

Can't make simple statement about unit change in odds or probability as a result of unit change in GPA.  

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

```{r}
#| echo: false

plot_odds + plot_prob
```

--------------------------------------------------------------------------------

## Odds Ratios

This is where the odds ratio comes in!   

Odds are defined at a specific point for $X$.   

**Odds ratio ($\Psi$)** = the change in odds for a change in $X$ of some magnitude $c$.   

$\Psi = \frac{odds(X+c)}{odds(X)}$   

$= \frac{e^{b_0 +b_1(X+c)}}{e^{b_0+b_1(X)}}$   

$= e^{c*b_1}$  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What is the odds ratio for a change in GPA of 1.0?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$\Psi = e^{c*b_1}$   

$= e^{1.0*4.1}$   

$= 60.3$     

**The odds of getting into grad school increase by a factor of 60.3 for every 1 point increase in GPA.**    

This is the preferred descriptor for the effect of $X$. For a quantitative variable, choose a meaningful value for $c$ (though often 1 is most appropriate).
:::

--------------------------------------------------------------------------------

$\Psi = e^{c*b_1}$    

- When $b_1 = 0$, $\Psi = 1$, indicating no change in odds for change in $X$.   

- When $b_1 > 0$, $\Psi > 1$, indicating an increase in odds with increasing $X$.   

- When $b_1 < 0$, $\Psi < 1$, indicting a decrease in odds with increasing $X$.   

- Odds ratio ($\Psi$) is never negative.   

--------------------------------------------------------------------------------

## Parameter Significance Tests

There are three common statistical tests for a parameter in logistic regression:   

1. **Z test:** Reported in the `tidy()` output.  

    $z = \frac{b_j}{SE_j}$


2. ***Wald test:** Reported by SPSS. Wald is asymptotically distributed as Chi-square with 1 df.   

    $wald = \frac{b_j^2}{SE_j^2}$   

**Both 1 and 2 are not preferred as they have higher type II error rates than the third option.**   

3. **Likelihood ratio test:** Reported in the `anova` output.   

--------------------------------------------------------------------------------

## Likelihood Ratio Test

Deviance is the maximum likelihood generalization of SSE from OLS.   

The likelihood ratio test involves a comparison of two models' deviances.   

To test the effect of GPA, compare deviances.    
**Compact**: Intercept only (null model)   
**Augmented**: Model with GPA    

LR test = compact - augmented    

Distributed as chi-square with df = df(augmented) - df(compact).    

--------------------------------------------------------------------------------

```{r}
compact <- glm(admit ~ 1, data = data, family = binomial(logit))
augmented <- glm(admit ~ gpa, data = data, family = binomial(logit))

anova(compact, augmented)
```

--------------------------------------------------------------------------------

## Maximum Likelihood Estimation (MLE)

The regression coefficients are estimated using maximum likelihood estimation.   

It is iterative (Newton's method).   

Model may fail to converge if:   

- Ratio of predictors to infrequent cases (10 *events* per predictor).   
- High multicollinearity.
- Sparseness (cells with zero events). Bigger problem for categorical predictor.   
- Complete separation: Predictors perfectly predict the criterion.   

MLE can yield biased parameters with small samples. Definitions of what is not small vary, but $\ge 200$ is a reasonable minimum.   

--------------------------------------------------------------------------------

## Model Assumptions

1. Exact X.
2. Independence.
3. Logistic function and logit correctly specify the form of the relationship (equivalent to the correct fit in linear regression). Logistic function is almost always correct for dichotomous data. You can examine the logit function directly to assess the shape of the relationship.


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What about categorical variables?
:::   


:::{.fragment}
- Handled exactly as in linear regression.  

- Contrast codes, dummy codes.  

- Issues of family-wise error rates apply as before for non-orthogonal and unplanned contrasts. Holm-bonferroni is available.

- Odds ratio is for contrast (assuming unit weighted).
:::

--------------------------------------------------------------------------------

