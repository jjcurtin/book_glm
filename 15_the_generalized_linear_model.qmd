--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 15: The Generalized Linear Model

::: {.callout-important}
# Question

What are the assumptions of **general** linear models?
:::


:::{.fragment}
The general linear model makes the 5 assumptions below. When these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). 


1. **Exact X**: The IVs are assumed to be known exactly (i.e., without measurement error).
2. **Independence**: Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).
3. **Normality**: All residual distributions are normally distributed.
4. **Constant variance**: All residual distributions have a constant variance, $SEE^2$.
5. **Linearity**: All residual distributions (i.e., for each $Y'$) are assumed to have means equal to zero.
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question

What are the consequences of violating each of these assumptions? What options exist when these assumptions are violated?
:::


:::{.fragment}
1. **Exact X**: Biased parameters (to the degree that measurement error exists). Use reliable measures.
2. **Independence**: Inaccurate standard errors, degrees of freedom and significance tests. Use repeated measures or linear mixed effects models or ANCOVA.
3. **Normality**: Inefficient (with large N). Use power transformations, **generalized linear models**.
4. **Constant variance**: Inefficient and inaccurate standard errors. Use power transformations, SE corrections, weighted least squares, **generalized linear models**.
5. **Linearity**: Biased parameter estimates. Use power transformations, polynomial regression, **generalized linear models**.
:::

--------------------------------------------------------------------------------

## Generalized Linear Models

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(broom)
library(skimr)
theme_set(theme_classic()) 
```

`glm(formula, family = familytype(link = linkfunction), data = data)`

```{r}
#| echo: false

tibble(Family = c("binomial", "gaussian", "Gamma", "inverse.gaussian", 
                  "poisson", "quasi", "quasibinomial", "quasipoisson"),
       `Default Link Function` = c('(link = "logit")', '(link = "identity")', 
                                   '(link = "inverse")', '(link = "1/mu^2")', 
                                   '(link = "log")', 
                                   '(link = "indentity", variance = "constant")', 
                                   '(link = "logit")', '(link = "log")')) |> 
  kableExtra::kable() |> 
  kableExtra::kable_styling()
```

--------------------------------------------------------------------------------

## An Example

Predicting admission to a grad program in engineering based on quantitative GRE, GPA, and Undergraduate Institution Rank.   

```{r}
#| echo: false

data <- read_csv(here::here("data_lecture/15_grad_admit.csv"),
                 show_col_types = FALSE) |> 
  glimpse()
```

```{r}
data |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

--------------------------------------------------------------------------------

```{r}
m_lm <- lm(admit ~ gpa, data = data)

tidy(m_lm)
```


::: {.callout-important}
# Question

What is the effect of GPA on admission to grad school?
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question

What are the problems with using a general linear model to assess the effects of these predictors on admission outcomes?
:::

:::{.fragment}

1. Residuals will not be normal (not efficient).

2. Residual variance often will not be constant (not efficient, SEs are inaccurate).

3. Relationship will not be linear (parameter estimates biased).

4. Y is not constrained between 0 â€“ 1 (model may make nonsensical predictions).
:::

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4))
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4))
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(-2, 1.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)
```


--------------------------------------------------------------------------------


```{r}
gvlma::gvlma(m_lm)
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_lm), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-4, 4, length.out = 100),
                          y = dnorm(seq(-4, 4, length.out = 100), 
                                    mean = 0, sd = sd(rstudent(m_lm)))), 
            linetype = "dashed", color = "blue")
```

```{r}
#| code-fold: true

car::qqPlot(m_lm, id = FALSE, simulate = TRUE,
            main = "QQ Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```


--------------------------------------------------------------------------------

```{r}
#| code-fold: true

tibble(x = predict(m_lm),
       y = rstudent(m_lm)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs.  Predicted Values",
       x = "Predicted Values",
       y = "Studentized Residuals")
```

```{r}
#| cold-fold: true
#| warning: false

car::spreadLevelPlot(m_lm)
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

car::crPlots(m_lm, ask = FALSE)
```

--------------------------------------------------------------------------------

```{r}
m_glm <- glm(admit ~ gpa, data = data, family = binomial(logit))

tidy(m_glm)
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

x <- tibble(gpa = seq(0, 6, length.out = 500))

preds <- predict(m_glm, newdata = x) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(x)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 6)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue") 
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| warning: false

ex_data <- tibble(x = c(rep(1:4, 10), rep(5, 10), rep(6:10, 10)),
                 y = c(rep(0, 40), rep(0:1, 5), rep(1, 50))) 

m_ex <- glm(y ~ x, data = ex_data, family = binomial(logit))

preds_ex <- predict(m_ex, newdata = tibble(x = seq(1, 10, length.out = 100))) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(tibble(x = seq(1, 10, length.out = 100)))

ex_data |> 
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = 0, height = .04) +
  scale_x_continuous(limits = c(1, 10), 
                     breaks = c(2, 4, 6, 8, 10)) +
  geom_line(data = preds_ex, aes(x = x, y = value), color = "blue") 
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question

What other non-linear shapes do you know how to model in the general linear model?
:::

:::{.fragment}

1. Simple monotone relationships with power transforms.

2. Quadratic, cubic, etc relationships with polynomial regression. <!--KW: I don't think this was introduced?-->
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false

tibble(x = 1:100,
       y = sqrt(x)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = sqrt(x)")

tibble(x = -100:100,
       y = x + x^2) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2")

tibble(x = -100:100,
       y = x + x^2 + x^3) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2 + x^3")
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| warning: false

tidy(m_glm)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue") 
```

--------------------------------------------------------------------------------

**Linear Regression:**   

- Residuals are gaussian.   
- Link function is identity.   

$Y = 1 * (b_0 + b_1X_1 + ... + b_kX_k)$    

**Logistic Regression:**   

- Residuals are binomial   
- Link function is logit (a transformation of the logistic function).   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$\pi$ = probability of $Y = 1$    
$e$ = 2.718 (approx)

--------------------------------------------------------------------------------

## Logs and Exponentials

You are likely familiar with logs using base 10.

```{r}
log10(10)

log10(100)

log10(1000)

log10(1)

log10(15)

log10(0)

log10(.1)
```

--------------------------------------------------------------------------------

The natural log (often abbreviated ln) is similar but uses base $e$ (approx 2.718) rather than base 10.  

```{r}
log(2.718282)

log(10)

log(1)

log(0)
```

--------------------------------------------------------------------------------

The inverse of the natural log is the exponential function: `exp()`.  This function simply raises $e$ to the power of $X$ (whatever value you provide).   

```{r}
exp(1)

exp(2)

exp(0)

exp(-1)
```

Logistic regression uses natural logs and exponentials for the transformations of $Y$ and $X$s.

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

```{r}
#| echo: false

tidy(m_glm)
```

$\pi = \frac{e^{-14.5 + 4.1}}{1 + e^{-14.5 + 4.1X_1}}$   

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = 0$

```{r}
#| echo: false

tibble(x = -10:10,
       b0 = 0,
       b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x))) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line(color = "red") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = 0 \text{ to }1$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, .25, .5, 1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = 0$    

$b_1 = -1 \text{ to }0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, -.25, -.5, -1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = -5 \text{ to } 5$    

$b_1 = 0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

$b_0 = -5 \text{ to } 5$    

$b_1 = 1$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 1) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

## Odds

Odds = $\frac{\pi}{1-\pi}$   

::: {.callout-important}
# Question

What are the odds of obtaining a head on a fair coin toss?
:::

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = \frac{0.5}{0.5} = 1 [1:1]$ 
:::

::::{.fragment}
::: {.callout-important}
# Question

What would the odds of obtaining a head be if I altered the coin to have a probability of heads = 0.67?
:::
::::

:::{.fragment}
Odds = $\frac{0.67}{1-0.67} = \frac{0.67}{0.33} = 2 [2:1]$ 
:::

--------------------------------------------------------------------------------

$\pi = \frac{\text{odds}}{\text{odds} + 1}$    

::: {.callout-important}
# Question

What is the probability of an event that has an odds of 3?
:::

:::{.fragment}
$\pi = \frac{3}{3 + 1} = .75$ 
:::

:::{.fragment}
**Odds can range from 0 - infinity.**
:::

--------------------------------------------------------------------------------

Odds = $\frac{\pi}{1-\pi}$  

::: {.callout-important}
# Question

What are the approximate odds of getting into grad school with a GPA of 3.5?
:::   


```{r}
#| echo: false
#| warning: false

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue", linewidth = .75) +
  geom_vline(xintercept = 3.5, color = "dark green", linewidth = .75) +
  geom_hline(yintercept = .49, color = "dark green", linewidth = .75)
```


:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = 1[1:1]$  
:::

--------------------------------------------------------------------------------

**Logistic function (probability Y = 1):**   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

$0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1)**    

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$0 \le \text{odds} \le\text{INF}$  


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What are the predicted odds of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$= e^{-14.5 + 4.1 * 3.5}$   

$= e^{-0.15}$  

$= 0.86 [0.86:1]$   

**Note: Probability of 0.50 occurs with odds of 1.**
:::

--------------------------------------------------------------------------------

**Logistic function (probability Y = 1):**   

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

$0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1):**    

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$0 \le \text{odds} \le\text{INF}$  

**Convert odds to log-odds(logit function; log-odds of Y = 1):**   

$ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   

$-\text{INF} \le logit \le \text{INF}$  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What are the log-odds (logit) of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$logit= b_0 + b_1X_1 + ... + b_kX_k$ 

$= b_0 + b_1*gpa$

$= -14.5 + 4.1 * 3.5$   

$= -0.15$  

**Note: Odds of 1 occur when logit = 0.**
:::

--------------------------------------------------------------------------------

