# Unit 6: Inferences about two predictors (multiple regression without interaction)

```{r}
#| echo: false

options(scipen = 999) # turns off scientific notation
```


## Multiple Regression 2+ Predictors

In this unit, we will consider the case of multiple predictors.

- Consider how the concepts we have discussed so far generalize to the 2 predictor (3 parameter) model.  
- We will start with 2 quantitative predictors example. Will continue with 1 quantitative and 1 dichotomous predictor example.  
- Learn how to quantify, test, and interpret ‘partial’ effects:
    - $b_j$
    - $\Delta R^2, \eta_p^2$
- Multicollinearity
- Text, table and figure descriptions of results
- Generalization to >2 predictors is straightforward.

-----

## Benefits of Multiple Predictors

1. **Statistical power**:  Goal is to increase power to test focal predictor’s effect on DV by adding it to model that contains additional known predictors of DV.

2. **Additional explanatory power**: Goal is to demonstrate that focal predictor adds explanatory power above and beyond other predictor(s) [Unique effect controlling for other predictors].

3. **Efficiency**:  Can test focal effects of two predictors in one study (each benefiting from increased power per point 1).

4. **Mediation**: We have identified a known cause of a DV.  We add a new focal predictor to test if the effect of our known causal IV on the DV is mediated by our focal predictor (i.e., identify “mechanism” of IV effect).

5. **Better prediction**: If we are using the model to predict the DV for individuals, all real life DVs are the result of multiple causes.  Including them will improve prediction.

-----

## Alcohol and Stress Response Dampening (SRD)

Test for Alcohol *Stress response dampening*   

Manipulate BAC (0.00% - 0.15%)  

Stressor Task (threat of unpredictable shock)  

Measure Stress Response (Fear potentiated startle)  

-----

## Two Parameter (1 Predictor) Model

Set up environment and load data

```{r}
#| message: false
#| code-fold: true

library(tidyverse) 
library(broom)
library(patchwork)
theme_set(theme_classic()) 

path_data <- "data_lecture" 

data <- read_csv(here::here(path_data, "06_two_predictors_fps.csv"),
                 show_col_types = FALSE) 
```

Define `my_skim()` function
```{r}
#| code-fold: true

library(skimr)
my_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),
                                n_missing = ~sum(is.na(.), na.rm = TRUE)),
                     numeric = sfl(p25 = NULL,
                                   p75 = NULL,
                                   hist = NULL),
                     character = sfl(min = NULL, max = NULL),
                     factor = sfl(ordered = NULL))
```

-----

Skim the data
```{r}
data |> my_skim()
```

-----

We can also write some functions to help us with calculations and effect sizes.

```{r}
# calculate sum of squared errors for a model
sse <- function(model) {
  sum(residuals(model)^2)
}

# calculate pre for a model comparison
pre <- function(compact, augmented) {
  (sse(compact) - sse(augmented)) / sse(compact)
}
```

-----

Now, we fit the two parameter model to our data again (same as previous unit)

```{r}
m_2 <- lm(fps ~ 1 + bac, data = data)

m_2 |> tidy()
```

:::{.callout-important}
# Question

Describe the interpretation of $b_1$ (coefficient for BAC) and its significance test.
:::

:::{.fragment}
[$b_1$ *describes* the relationship between BAC and FPS in the units of each measure. FPS will decrease by 184 µV for every 1% increase in BAC (It will decrease by 1.84µV for every .01 increase in BAC).]{style="color:blue;"}

[The significance test for $\beta_1$ tests the null hypothesis that the population relationship between BAC and FPS is 0 (i.e., $\beta_1$ = 0, no relationship). We fail to reject this $H_0$. Conclude that alcohol does not affect FPS.]{style="color:blue;"}
:::

-----

## Testing Inferences about $\beta_1$

```{r}
m_2 |> tidy()
```

-----

:::{.callout-important}
# Question
$H_0: \beta_1 = 0$  
$H_a: \beta_1 \neq 0$  

What could we change about the sampling distribution that would make this $b_1$ be less probable given $H_0$ so that we reject the Null?
:::

:::{.fragment}
```{r}
#| code-fold: true

tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_2), term == "bac")$std.error, m_2$df.residual)) |> 
  ggplot(aes(x = b1, y = probability)) +
  geom_line() +
  geom_vline(xintercept = subset(broom::tidy(m_2), term == "bac")$estimate, 
             color = "red") +
  labs(title = "Sampling Distribution for b1")
```

[If the standard deviation of the sampling distribution (its standard error) was smaller so that the distribution was narrower, $b_1$ would be less probable given $H_0$.]{style="color:blue;"}`
:::

## Standard Errors of GLM Coefficients

The formula for the standard error for a coefficient $b_j$ in **multiple** regression (i.e., GLM with more than one regressor) is:  

$SE_{bj} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R^2_y)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R^2_j)}}$  

\

- $R^2_j$ = variance in $X_j$ accounted for by all other regressors ($X$s) in the model (i.e., how redundant is $X_{a_j}$ in model?).

- This is literally: predict $X_j$ as $Y$ with all other regressors as $X$s.  

\

:::{.callout-note}
# Note 
Formula for the standard error for $b_0$ is different than for $b_j$.  For the two parameter model $SE_{b0}$ is (need matrix notation for $P > 2$):  

$\sqrt {\frac{SSE}{N-P}}*\sqrt{\frac{1}{N}+\frac{X^2}{(N-1)s_x^2}}$
:::

-----

## SE for $b_j$ and $R^2$

$SE_{b_j}= \frac{s_y}{s_j}*\frac{\sqrt{(1-R_y^2)}}{(N-P)}*\frac{1}{\sqrt{(1-R_j^2)}}$  

\

If we increase $R_y^2$, we would decrease the SE for our regression coefficient.  

-----

## Model Comparison: Testing Inferences about $\beta_1$

$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$  

:::{.callout-important}
# Question
What two models are you comparing when you test hypotheses about $\beta_1$ for BAC? Describe the logic.
:::

:::{.fragment}
[Compact model:]{style="color:blue;"}

```{r}
m_c <- lm(fps ~ 1, data = data)
```

- $\hat{FPS}_i = \beta_0+0*BAC_i$
- $SSE_c=$ `r round(sse(m_c), 1)`, $P_c=1$     

\

[Augmented model:]{style="color:blue;"}
```{r}
m_a <- lm(fps ~ 1 + bac, data = data)
```

- $\hat{FPS}_i = \beta_0+\beta_1*BAC_i$    
- $SSE_a=$`r round(sse(m_a), 1)`, $P_a=2$   

\

$F(P_a-P_c, N-P_a) = \frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$

```{r}
anova(m_c, m_a)
```
:::

-----

$F(P_a-P_c, N-P_a) = \frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$  

:::{.callout-important}
# Question
What could you change from this model comparison perspective to increase $F$ and probability to reject the $H_0$ about $\beta_1$?
:::

:::{.fragment}
[Make $SSE_a$ smaller by explaining more variance in $Y_i$.]{style="color:blue;"}

[Of course, $R^2 = \frac{SSE_{mean-only} - SSE_a}{SSE_{mean-only}}$]{style="color:blue;"}

[If you decrease $SSE_a$ or increase model $R^2$, you will have more power to reject $H_0$ regarding parameter estimates.]{style="color:blue;"}
:::

-----

## Two Parameter (1 Regressor) Model (Continued)

```{r}
#| code-fold: true

m_2 |> tidy()
```

:::{.callout-important}
# Question
What can we do analytically to decrease SSE (increase model $R^2$) in any model?
:::

:::{.fragment}
[Include another regressor ($X$) in the model that accounts for additional variance in $Y$ (reduces SSE).]{style="color:blue;"}

- [When we add $X$s to the model to reduce error or increase $R^2$, rather than to explicitly test questions about these $X$s, we call these $X$s covariates.]{style="color:blue;"}

- [Ideally, covariates should be orthogonal (uncorrelated) with the other $X$s.]{style="color:blue;"}

- [In this experiment, I could have measured another predictor of stress response, Trait Anxiety (TA). TA might be expected to be a robust predictor of FPS. It also should be uncorrelated with BAC because I manipulated BAC.]{style="color:blue;"}
:::

-----

Here are the distributions of $Y$ and our two potential $X$s

```{r}
#| code-fold: true
 
plot_fps <- data |> 
  ggplot(aes(x = fps)) +
  geom_histogram(aes(y = after_stat(density)),
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +
  geom_rug(color = "red")

plot_bac <- data |> 
  ggplot(aes(x = bac)) +
  geom_histogram(aes(y = after_stat(density)), boundary = 0,
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  geom_rug(color = "red")

plot_ta <- data |> 
  ggplot(aes(x = ta)) +
  geom_histogram(aes(y = after_stat(density)), boundary = 0, 
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  geom_rug(color = "red") 

plot_fps + plot_bac + plot_ta
```

-----

We can look at correlations among these variables using `corr.test()` from the `psych` package (you may need to install that package)

\

```{r}
r <- data |> 
  select(where(is.numeric)) |> 
           psych::corr.test()

# corr.test() is not tidy and does not return a dataframe.  Annoying!!
r$r
```

-----

We can also visualize these bivariate relationships using scatterplots

```{r}
data |> 
  ggplot(aes(x = bac, y = fps)) +
  geom_point(alpha = .6)
```

-----

```{r}
data |> 
  ggplot(aes(x = ta, y = fps)) +
  geom_point(alpha = .6)
```

-----

```{r}
data |> 
  ggplot(aes(x = bac, y = ta)) +
  geom_point(alpha = .6)
```


## The Two Predictor and General Linear Models

The two (and $k$) are simple generalizations of the models you have already learned

- Simply add more $X$s to the model 
- Estimate parameters for these $X$s

\

DATA = MODEL + ERROR

**Two Predictor Model for Sample Data**  

$Y_i=b_0+b_1X_1+b_2X_2+e_i$  

$\hat{Y_i}=b_0+b_1X_1+b_2X_2$  

\

**$k$ Predictor Model for Sample Data**   

$Y_i=b_0+b_1X_1+...+b_kX_k+e_i$  

$\hat{Y_i}=b_0+b_1X_1+...+b_kX_k$  

-----

## Testing BAC in a Three Parameter Model (2 Predictors)

```{r}
m_3 <- lm(fps ~ 1 + bac + ta, data = data)

m_3 |> tidy()
```

$\hat{FPS} = `r round(coef(m_3)[1], 1)` + `r round(coef(m_3)[2], 1)` * BAC + `r round(coef(m_3)[3], 1)` * TA$

:::{.callout-important}
# Question
What parameter estimate is used to test our research question about the effect of BAC? What are our $H_0$ and $H_a$ for the associated population parameter?
:::

:::{.fragment}
[$H_0:\beta_1=0; H_a: \beta_1 \neq 0$]{style="color:blue;"}

[We use $b_1$ (`r round(coef(m_3)[2], 1)`) to test our hypothesis about the population effect of BAC ($\beta_1$).]{style="color:blue;"}
:::

-----

```{r}
#| code-fold: true

m_3 |> tidy()
```

:::{.callout-important}
# Question
Describe conclusion and logic of the test of $H_0:\beta_1=0$ from sampling distribution perspective.
:::

:::{.fragment}
[If $H_0$ is true, we expect a sampling distribution for $b_1$ to have a mean of 0 and an SE of 86.6 (red curve below).]{style="color:blue;"}


```{r}
#| code-fold: true

tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(tidy(m_3), term == "bac")$std.error, m_3$df.residual)) |> 
  ggplot(aes(x = b1, y = probability)) +
  geom_line(color = "red") +
  geom_vline(xintercept = subset(tidy(m_3), term == "bac")$estimate, 
             color = "blue") +
  geom_vline(xintercept = -1 * subset(tidy(m_3), term == "bac")$estimate, 
             color = "green") 
```

[A sample $b_1$ = -177.0 is unlikely (about 2 standard deviations below mean; p = .0437). Therefore, we reject our $H_0$ and conclude that $\beta_1 \neq 0$.]{style="color:blue;"}

[Conclusion is that BAC affects FPS.]{style="color:blue;"}
:::

-----

:::{.callout-important}
# Question
Describe conclusion and logic of the test of $H_0:\beta_1=0$ from model comparison perspective.
:::

:::{.fragment}
[$H_0: \beta_1 = 0; H_a: \beta_1 \neq 0$]{style="color:blue;"}

[**Compact Model:**]{style="color:blue;"}

```{r}
m_c <- lm (fps ~ 1 + ta, data = data)
```

- $\hat{FPS}=\beta_0 + 0*BAC+\beta_2*TA$    
- $SSE_c =$ `r round(sse(m_c), 1)`, $P_c = 2$   

\

[**Augmented Model:**]{style="color:blue;"}

```{r}
m_a <- lm(fps ~ 1 + bac + ta, data = data)
```

- $\hat{FPS}=\beta_0 + \beta_1*BAC+\beta_2*TA$    
- $SSE_a =$ `r round(sse(m_a), 1)`, $P_a = 3$ 

\

$F(P_a - P_c, N - P_a) = \frac{(SSE_c - SSE_a)/(P_a-P_c)}{SSE_a/(N-P_a)}$   
$F (1, 93) = $ `r round((sse(m_c) - sse(m_a)) / (sse(m_a) / 93), 2)`, $p =$ `r round(pt(-2.04, 93, lower.tail = TRUE)*2, 4)`
:::

-----

**Two parameter model test of BAC**   

```{r}
#| code-fold: true
m_2 <- lm(fps ~ 1 + bac, data = data)

tidy(m_2)
```

$SSE =$ `r round(sse(m_2), 1)`

\

**Three parameter model test of BAC**   

```{r}
#| code-fold: true
m_3 <- lm(fps ~ 1 + bac + ta, data = data)
tidy(m_3)
```

$SSE =$ `r round(sse(m_3), 1)`

:::{.callout-important}
# Question
What changed about test of $\beta_1$ (BAC effect) and why?
:::

:::{.fragment}
**Two parameter model test of BAC**    
```{r}
glance(m_2) |> pull(r.squared)
```

\

**Three parameter model test of BAC**   
```{r}
glance(m_3) |> pull(r.squared)
```

[SSE decreased and $R^2$ increased.]{style="color:blue;"}

[As a result, SE for BAC decreased leading to a larger t-statistic]{style="color:blue;"}
:::

-----

## Standard Error of Partial Regression Coefficient ($b_j$)

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

:::{.callout-important}
# Question
What happens to $SE_{b_j}$ as model $R^2$ ($R^2_y$) increases (holding other factors constant)?
:::

:::{.fragment}
[$SE_{b_j}$ decreases as model $R^2$ increases. In other words, the sampling distribution get narrower.]{style="color:blue;"}
:::

-----

:::{.callout-important}
# Question
What happens to significance test of $b_j$ as $\text{SE}_{b_j}$ decreases (holding other factors constant)?
:::

:::{.fragment}
[$t$ increases and associated p-value deceases (More Power!).]{style="color:blue;"}
:::

-----

## Sampling Distributions and Power

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  


**Two parameter model test of BAC**   

- $t(96-2) = \frac{-184 - 0}{95.9}$    
- $t(94) = -1.92$  
- $p = .0579$
```{r}
#| code-fold: true
tidy(m_2)
```

\


**Three parameter model test of BAC**   

- $t(96-3) = \frac{-177 - 0}{86.6}$    
- $t(93) = -2.04$  
-$p = .0437$
```{r}
#| code-fold: true
tidy(m_3)
```

-----

```{r}
#| code-fold: true

distr_data <- tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_3), term == "bac")$std.error, m_3$df.residual),
       group = "3 parameter model") |> 
  bind_rows(tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_2), term == "bac")$std.error, m_2$df.residual),
       group = "2 parameter model"))

distr_data |> 
  ggplot(aes(x = b1, y = probability, color = group)) +
  geom_line() +
  scale_color_manual(values = c("red", "blue")) +
  geom_vline(xintercept = subset(broom::tidy(m_2), term == "bac")$estimate, 
             color = "red") +
  geom_vline(xintercept = subset(broom::tidy(m_3), term == "bac")$estimate, 
             color = "blue") 
```

-----

## Sampling Distributions and Precision

$CI_b=b \pm t(\alpha; N-P)*SE_b$


```{r}
confint(m_2)
```

$\Delta$ `r round(as_tibble(confint(m_2))[[2]][2]-as_tibble(confint(m_2))[[1]][2],3)`


```{r}
confint(m_3)
```

$\Delta$ `r round(as_tibble(confint(m_3))[[2]][2]-as_tibble(confint(m_3))[[1]][2],3)`

## Standard Error of Partial Regression Coefficient ($b_j$; Continued)

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

$R^2_j$ = variance in $X_j$ accounted for by all other predictors in model (i.e., how redundant is $X_j$ in model?).  


-----

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

:::{.callout-important}
# Question
What other factors affect SE for regression coefficients and how?
:::

:::{.fragment}
- [Increasing $N$ decreases SE (increases power).]{style="color:blue;"}
- [Increasing $P$ increases SE (decreases power)]{style="color:blue;"}
- [Increasing $s_y$ increases SE (decreases power).]{style="color:blue;"}
- [Increasing $s_j$ decreases SE (increases power).]{style="color:blue;"}
- [Increasing $R_j^2$ increases SE (decreases power).]{style="color:blue;"}
:::

## Power and SSE in Two and Three Parameter Models

**Two parameter model test of BAC**   

Compact Model: 

- $\hat{FPS}= `r round(coef(lm(fps~1, data = data)), 1)` + 0*{BAC}$   
- $SSE_c =$ `r round(sum(residuals(lm(fps~1, data = data))^2), 1)`, $P_c$ = 1  

Augmented Model: 

- $\hat{FPS}= `r round(coef(m_2)[1],1)` + `r round(coef(m_2)[2],1)` *BAC$   
- $SSE_a =$ `r round(sum(residuals(m_2)^2), 1)`, $P_a$ = 2  

\

**Three parameter model test of BAC**   

Compact Model: 

- $\hat{FPS}= `r round(coef(lm(fps~ta, data = data))[1],1)` + 0*BAC + `r round(coef(lm(fps~ta, data = data))[2],1)` * TA$   
- $SSE_c =$ `r round(sum(residuals(lm(fps~ta, data = data))^2),1)`, $P_c$ = 2  

Augmented Model: 

- $\hat{FPS}= `r round(coef(m_3)[1],1)` + `r round(coef(m_3)[2],1)` *BAC + `r round(coef(m_3)[3], 1)` *TA$   
- $SSE_a =$ `r round(sum(residuals(m_3)^2), 1)`, $P_a$ = 3  

\

$F(P_a - P_c, N - P_a) = \frac{(SSE_c - SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$ 

-----

:::{.callout-important}
# Question
How can you see the increase in power from the model comparison perspective (look back at last slide)?
:::

:::{.fragment}
**Two parameter model test of BAC** 

- $F(2-1, 96-2) = \frac{(`r round(sum(residuals(lm(fps~1, data = data))^2),1)` - `r round(sum(residuals(m_2)^2),1)`)/(2-1)}{`r round(sum(residuals(m_2)^2),1)` / (96-2)}$

- $F(1, 94) = \frac{(`r round(sum(residuals(lm(fps~1, data = data))^2) -  sum(residuals(m_2)^2),1)`)/(1)}{`r round(sum(residuals(m_2)^2)/94, 1)`}$

- $F (1, 94) = `r round((sum(residuals(lm(fps ~ 1, data=data))^2) - sum(residuals(m_2)^2))/(sum(residuals(m_2)^2)/94), 2)`, p = `r round(pt(-1.92, 94, lower.tail = TRUE)*2, 4)`$

\

**Three parameter model test of BAC** 

- $F(3-2, 96-3) = \frac{(`r round(sum(residuals(lm(fps~ta, data = data))^2),1)` - `r round(sum(residuals(m_3)^2),1)`)/(3-2)}{`r round(sum(residuals(m_3)^2),1)` / (99-3)}$

- $F(1, 93) = \frac{(`r round(sum(residuals(lm(fps~ta, data = data))^2) -  sum(residuals(m_3)^2),1)`)/(1)}{`r round(sum(residuals(m_3)^2)/93, 1)`}$

- $F (1, 93) = `r round((sum(residuals(lm(fps ~ ta, data=data))^2) - sum(residuals(m_3)^2))/(sum(residuals(m_3)^2)/93), 2)`, p = `r round(pt(-2.04, 93, lower.tail = TRUE)*2, 4)`$


Decreased $SSE_a$ in three parameter model. Flip side of increased model $R^2$.]{style="color:blue;"}
:::

-----

$F(P_a - P_c, N - P_a) = \frac{(SSE_c = SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$   

- Impact of $N$ on $P_a$ also clear.

- Impact of $s_y$ and $s_{x_j}$ and multicollinearity less clear in formula.

- Connection to precision of parameter estimation less clear in formula.

-----

## $R^2_j$ and Multicollinearity

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

- $t(N-P) = \frac{b-0}{SE_b}$   
- $CI_b= b \pm t(\alpha; N-P)*SE_b$   

\

This decrease in **power and precision** for model parameter estimates (regression coefficients) associated with redundancy among the predictors is called the **problem of Multicollinearity**.  

-----

When there are more than two `X`s in the model, it is **not** sufficient to examine only bivariate correlations among $X$s. To determine if a problem exists, calculate Variance Inflation Factors (VIF) for each $X$.  

$VIF_j = \frac{1}{(1-R^2_j)}$   

\

VIF tells you how much $SE_{b_j}$ is increased because of redundancy. VIFs $\ge$ 5 are considered problematic (SE increased by factor of 2.2).  

-----

We can use `car::vif()` to calculate VIFs in R.

```{r}
car::vif(m_3)
```

\
\

A related concept is **tolerance** ($X_i= 1- R^2_j$). 

Tolerance decreases toward 0 as multicollinearity increases.

-----

Solutions for Multicollinearity include:  

1. Drop redundant variable.

2. Factor analysis (e.g., PCA) to produce factors that reflect major sources of variance among the redundant predictors.

3. This is only a problem for the `X`s in the model with high VIFs. If you don't care about testing them, this is not a problem. Generally, you only care about VIFs for your focal `X`(s).

## Interpretation of Multiple Regression Coefficients

:::{.callout-important}
# Question
What did the value of $b_1$ tell us in a regression model with one $X$?
:::

:::{.fragment}
[The change in $Y$ associated with a one unit increase in $X_1$.]{style="color:blue;"}

[For every 1 unit increase in $X_1$, there will be a $b_1$ unit increase in $Y$.]{style="color:blue;"}
:::

-----

:::{.callout-important}
# Question
What about $b_j$ with multiple (e.g., 2) $X$s?
:::

:::{.fragment}
[The change in $Y$ associated with a one unit increase in $X_j$ **controlling for all other $X$s in the model**. "Controlling for" means holding constant.]{style="color:blue;"}

[For every 1 unit increase in $X_j$, there will be a $b_j$ unit increase in $Y$ holding all other $X$s in the model constant.]{style="color:blue;"}
:::


## A Second Example

Now lets switch gears to a new example with new data.

- Do hours of studying per week affect exam performance in 610?
- How might we test this question if we also knew students IQs?

-----

Here are some data to test our question

- Lets make centered `X`s for both of our quantitative predictors
```{r}
data_exam <- read_csv(here::here(path_data, "06_two_predictors_exam.csv"), 
                 show_col_types = FALSE) |> 
  mutate(study_hours_c = study_hours - mean(study_hours),
         iq_c = iq - mean(iq))
```

- And take a quick look at the dataframe
```{r}
data_exam |> head()
```

-----

- And descriptives
```{r}
data_exam |> my_skim()
```

-----

Start by fitting the two parameter model (testing just `study_hours`) 

\

```{r}
m_exam_2 <- lm(exam ~ 1 + study_hours_c, data = data_exam)

tidy(m_exam_2)
```

-----

```{r}
tidy(m_exam_2)
```

:::{.callout-important}
# Question
What is the interpretation of $b_1$ in this model?
:::

:::{.fragment}
[There is a significant effect of study hours on exam scores.  For every one hour of studying per week, students' exam scores increase by 2.3 points]{style="color:blue;"}
:::


-----

```{r}
tidy(m_exam_2)
```

\

:::{.callout-important}
# Question
What is the interpretation of $b_0$?
:::

:::{.fragment}
[The expected exam score for a student who studies the mean number of hours per week (from the sample) is 62.1.  This is significantly different from zero, but that test is probably not that meaningful.]{style="color:blue;"}
:::

-----

Maybe study hours are related to exam scores only because intelligent students study more (learned early good study habits) and intelligent students do better on exams.  

\

:::{.callout-important}
# Question
What would you expect about the relationships (i.e., correlations) among study hours, iq, and exam scores if this were true?
:::

:::{.fragment}
[They would all be positively correlated with each other.  This is, in fact, what we see in these data (because they are fake!)]{style="color:blue;"}

```{r}
data_exam |> 
  select(study_hours, iq, exam) |> 
  cor() |> 
  round(2)
```
:::

-----

:::{.callout-important}
# Question
How could you assess the unique effect of `study_hours`, controlling for `iq`?
:::

:::{.fragment}
[Model exam scores as a function of both `study_hours` and `iq`. $b_1$ (effect of study hours) in this model is the unique effect of study_hours, controlling for IQ.]{style="color:blue;"}

```{r}
m_exam_3 <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)
```
:::

-----

```{r}
tidy(m_exam_3)
```

:::{.callout-important}
# Question
Do hours of studying per week affect performance in 610 after controlling for student IQ?</span>   
:::

:::{.fragment}
[Yes, for every one hour of studying per week, students' exam scores increase by 1 point, controlling for IQ.]{style="color:blue;"}
:::

-----

```{r}
tidy(m_exam_3)
```

:::{.callout-important}
# Question
What is the interpretation of $b_0$ in this model?
:::

:::{.fragment}
[It is the predicted value for exam scores for someone of mean IQ who studies the mean number of hours per week.  This is descriptively useful but the statistical test against 0 is not very meaningful.]{style="color:blue;"}
:::

-----

:::{.callout-important}
# Question
What would change in this model if we did not center `study_hours` and `iq`?
:::

:::{.fragment}
[The intercept would be the expected exam score for a student who studies 0 hours per week and has an IQ of 0.]{style="color:blue;"}

\

[There would be no change in either `b_1` or `b_2`.  These are still the unique effects of each of their associated `X`s, holding the other `X` constant in the model.]{style="color:blue;"}
:::

-----

```{r}
tidy(m_exam_2)
```

```{r}
tidy(m_exam_3)
```

\

Look at the effect of `study_hours` across the two models

-----

:::{.callout-important}
# Question
Why did the effect of `study_hours` get smaller after controlling for IQ (see last slide and think about pattern of correlations among predictors)?
:::

:::{.fragment}
```{r}
data_exam |> 
  select(study_hours, iq, exam) |> 
  cor() |> 
  round(2)
```

[1. When study_hours increases, IQ increases.]{style="color:blue;"}

[2. When IQ increases, exam scores increase.]{style="color:blue;"}

[3. The partial effect of study_hours on exam scores is smaller if IQ is not allowed to increase.]{style="color:blue;"}
:::

-----

## Causal Models?

Lets think about a causal model for the relationship between study hours and exam scores

:::{.callout-important}
# Question
How do we get the total (overall) effect of study hours on exam scores, not controlling for any other variables?
:::

:::{.fragment}
[To get the total effect of study_hours on exam score, predict exam scores from study hours, without controlling for any other variables.  This is just the two parameter model we fit earlier.]{style="color:blue;"}
```{r}
lm(exam ~ 1 + study_hours_c, data = data_exam) |> 
  tidy()
```
:::

-----

This is our causal model at this point

```{r}
#| echo: false 

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    Y [label = 'exam']
    A->Y [label = '2.34', fontsize = 5]
}
")
```

------

:::{.callout-important}
# Question
But what if we had a more complicated model that acknowledged that IQ may also affect exam scores.  How do we get the direct (partial, unique) effect of study hours on exam scores, controlling for IQ?  And how do we get the direct effect of IQ on exam scores, controlling for study hours?
:::

:::{.fragment}
```{r}
lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam) |> 
  tidy()
```

[These are the parameter estimates for these two `X`s, from the three parameter model]{style="color:blue;"}
:::

-----

And here is our updated causal model

\

```{r}
#| code-fold: true

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '???', fontsize = 5]
{ rank = same; A; B }
}
")
```

:::{.callout-tip}
# Programming Tip
We left the code for this figure above (folded). You can unfold to see how we used the `Diagrammer` package to create this figure.
:::

-----

The total effect of study hours on exam scores is the sum of the direct effect of study hours on exam scores and the indirect or spurious* effect of study hours on exam scores through IQ.  That is currently missing from our model

- This would be considered an indirect effect if we believe that study hours causes IQ
- This would be a spurious effect if we believe that IQ causes study hours or a third variable (not displayed) causes both study hours and IQ
- The decision about which is the most plausible explanation is a theoretical one and/or a function of your research design
```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '???', fontsize = 5]
{ rank = same; A; B }
}
")
```

-----

To calculate this indirect/spurious effect, we first need to calculate the direct effect of study hours on IQ.

```{r}
m_iq <- lm(iq_c ~ 1 + study_hours_c, data = data_exam)

m_iq |> tidy()
```

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '3.75', fontsize = 5]
{ rank = same; A; B }
}
")
```

-----

**Total effect = Direct + Indirect/Spurious**

`r round(coef(m_exam_2)[2], 2)` = `r round(coef(m_exam_3)[2], 2)` +  (`r round(coef(m_iq)[2], 2)` * `r round(coef(m_exam_3)[3], 2)`)

\

`r round(coef(m_exam_2)[2], 2)` = `r round(coef(m_exam_3)[2], 2)` +  `r round(coef(m_iq)[2] * coef(m_exam_3)[3], 2)`

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    Y [label = 'exam']
    A->Y [label = '2.34', fontsize = 5]
}
")
```

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '3.75', fontsize = 5]
{ rank = same; A; B }
}
")
```

-----

:::{.callout-important}
# Question
In what situation would $b_j$ for a focal predictor (e.g., `X1 below) not change when you added an additional `X` (e.g., `X2`)? 

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
}
")
```

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    B [label = 'X2']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
    B->Y [label = 'b2', fontsize = 5]
    A->B [label = '?', fontsize = 5]
{ rank = same; A; B }
}
")
```
:::

-----

[1. If X2 was completely uncorrelated (orthogonal) with the focal predictor (X1) in those sample data, there would be no change in the parameter estimate when you added X2. The direct effect of X1 would equal its total effect because the indirect/suprious effect through X2 would be zero.]{style="color:blue;"}

[2. This is why uncorrelated predictors/covariates are considered easier to interpret when trying to increase power. If they are related to the DV, they will increase power (< SE) to test your focal variable but they will not change your estimate of the magnitude of the focal variables parameter estimate.]{style="color:blue;"}

[3. Completely orthogonal variables are typically only observed in experimental designs. However, small/trivial, nonsystematic sample $r$ will occur when population $r=0$.]{style="color:blue;"}

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
}
")
```

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    B [label = 'X2']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
    B->Y [label = 'b2', fontsize = 5]
    A->B [label = '0', fontsize = 5]
{ rank = same; A; B }
}
")
```

-----

## Effect Sizes for Overall Model and Individual Predictors (`X`s)

Once you have more than one `X` in your model, you have more than one effect size to consider.

All of these effect sizes can be defined in terms of differences in SSE between various models.  Lets fit all the relevant models here

- Mean only model
```{r}
m_mo <- lm(exam ~ 1, data = data_exam)
sse(m_mo)
```

- Study hours two parameter model
```{r}
m_sh <- lm(exam ~ 1 + study_hours_c, data = data_exam)
sse(m_sh)
```

- IQ two parameter model
```{r}
m_iq <- lm(exam ~ 1 + iq_c, data = data_exam)
sse(m_iq)
```

- Full three parameter model
```{r}
m_full <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)
sse(m_full)
```

-----

The SSE of the mean only model informs us about the total variability in Y (numerator of variance)

- $SSE_{mean-only} = \Sigma((Y_i - \hat{Y}_i)^2)$


This is equivalent to the numerator of the variance in $Y$ because $\hat{Y}_i = \bar{Y}$ in the mean only model.

- $SSE_{mean-only} = \frac{\Sigma((Y_i - \bar{Y}_i)^2)}{N-1}$

-----

We can represent this total (currently unexplained) variability (SSE) of $Y$ with a pie chart.  

- We will use this visualization to show how the various effect size estimate partition variance in $Y$ to unique and shared variance among the predictors.
- More on this in a moment

```{r}
#| echo: false

sses <- tibble(
  source = c("unexplained"),
  value = c(round(sse(m_mo))))

sses |> 
  ggplot(aes(x = "", y=value, fill = source)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("red")) +
  theme_void() # remove background, grid, numeric labels
```

## Model Effect Size: Coefficient of Determination ($R^2$)

You can quantify and test the overall performance of the model (with all $X$s) using the coefficient of determination ($R^2$).

\

$R^$ is the proportion of variance in $Y$ explained by the set of all model predictors (i.e., proportion of variance in $Y$ predicted by all $X$s in model).    

- In our example, $R^2$ describes the combined effect of `study_hours` and `iq`. 
- In more complex models, $R^2$ will always be predictive strength of the set of all $X$s derived from the predictors. 

\

There are several ways to calculate $R^2$ in R that can give you different insights into what it quantifies

-----

## $R^2$: Option 1

The easiest way to get $R^2$ for any model is the `glance()` function from the `broom` package.  

- Pass in the full model
- `glance()` provides a variety of full model statistics
- You can either `pull()`, `select()` or use `$r.squared` to get the numeric value or a tibble with the value

\

```{r}
glance(m_full) |> pull(r.squared)
```

\

```{r}
glance(m_full) |> select(r.squared)
```

\

```{r}
glance(m_full)$r.squared
```

-----

## $R^2$: Option 2

$R^2$ is the proportion of explained (predicted) variance in $Y$ to the total variance in $Y$.

\

$variance(Y_i) = variance(\hat{Y_i}) + variance(e_i)$  

- $variance(Y_i) is total variance in $Y$
- $variance(\hat{Y_i})$ is variance in predicted values
- $variance(e_i)$ is variance in residuals

Therefore, we can get $R^2$ by dividing the variance of the predicted values by the variance of the observed values

```{r}
var(predict(m_full)) / var(data_exam$exam)
```

\

Or we can get $R^2$ by dividing the difference between the variance of the observed values and the variance of the residuals (i.e., still just how much variance in $Y$ our model explains) by the variance of the observed values

```{r}
(var(data_exam$exam) - var(residuals(m_full))) / var(data_exam$exam)
```

-----

## $R^2$: Option 3

And that last formula can be re-written focusing on SSE rather than variance.  This is attractive because we can define all of our effect sizes in terms of differences in SSE between various models.

$R^2 = \frac{SSE_{mean-only} - SSE_full}{SSE_{mean-only} }$   

\

```{r}
(sse(m_mo) - sse(m_full)) / sse(m_mo)
```

-----

## $R^2$: Pie Chart

Visually, we can represent this with a pie chart

- Green area is explained variance 
- Red areas is unexplained variance
- $R^2 = \text{green} / \text{full pie (green + red)}$

```{r}
#| echo: false

sses <- tibble(
  source = fct(c("unexplained", "explained"), levels = c("unexplained", "explained")),
  value = c(sse(m_full), sse(m_mo) - sse(m_full)))

sses |> 
  ggplot(aes(x = "", y=value, fill = source)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("red", "green")) +
  theme_void() # remove background, grid, numeric labels
```

-----

## Predictor Effect Size Options

- As in the one predictor model, the parameter estimates in the two predictor model (and the $k$ (or $P-1$) predictor model) are attractive effect size estimates.   

- In addition, there are variance based effect size estimates that are also attractive.

- You have already learned about Partial eta-squared ($\eta_p^2$), which Judd et al refer to as PRE.  We will return to this in a moment

- You will first learn about Delta $R^2$ ($\Delta R^2$).

-----

## $\Delta R^2$

$\Delta R^2$ is the increase in model $R^2$ for the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.

\

$\Delta R^2 = R^2_a - R^2_c$

- We can calculate $\Delta R^2$ for each $X$ (or later set of $X$s) in the model
- We need to specify the appropriate compact and augmented model

-----

## $\Delta R^2$: Option 1

We need: $\Delta R^2 = R^2_a - R^2_c$

We can get it using `glance()` across two models

\

For `study_hours`

- Compact Model: $\hat{Y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_iq) |> pull(r.squared))
```

-----

## $\Delta R^2$: Option 1

For `iq`

- Compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_sh) |> pull(r.squared))
```

-----

## $\Delta R^2$: Option 1

We need: $\Delta R^2 = R^2_a - R^2_c$

- We can get it using `glance()`

\

For `study_hours`

- Compact Model: $\hat{Y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_iq) |> pull(r.squared))
```

-----

## $\Delta R^2$: Option 1

For `iq`

- Compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_sh) |> pull(r.squared))
```

-----

## $\Delta R^2$: Option 2

$\Delta R^2$ can also be defined with respect to SSE

- $\Delta R^2 = \frac{SSE_c - SSE_a}{SSE_{\text{mean-only}}}$

\

For `study_hours`

- Compact Model: $\hat{Y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_iq) - sse(m_full)) / sse(m_mo)
```

-----

## $\Delta R^2$: Option 2

For `iq`

- Compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_sh) - sse(m_full)) / sse(m_mo)
```

-----

## $R^2$: Pie Chart

Visually, we can represent these $\Delta R^2$s with a pie chart

- Green area is unique variance explained by `study_hours`
- Blue area is unique variance explained by `iq`
- Yellow area is shared variance explained by both `study_hours` and `iq`
- Red areas is unexplained variance


```{r}
#| echo: false

unexplained <- sse(m_full)
sh <- sse(m_iq) - sse(m_full)
iq <- sse(m_sh) - sse(m_full)
shared <- (sse(m_mo) - sse(m_full)) - sh - iq

sses <- tibble(
  source = fct(c("unexplained", "study_hours", "shared", "iq"), levels = c("unexplained", "study_hours", "shared", "iq")),
  value = c(unexplained, sh, shared, iq))

pie_4 <- sses |> 
  ggplot(aes(x = "", y=value, fill = source)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("red", "green", "blue", "yellow")) +
  theme_void() # remove background, grid, numeric labels

pie_4
```

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
How is the $R^2$ for the study hours only model represented in this pie chart?
:::

:::{.fragment}
[(Green + Blue) / Full Pie]{style="color:blue;"}

```{r}
glance(m_sh) |> pull(r.squared)
```

[This is the total/overall effect of `study_hours` on exam scores]{style="color:blue;"}
:::

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
How is the $R^2$ for the iq only model represented in this pie chart?
:::

:::{.fragment}
[(Yellow + Blue) / Full Pie]{style="color:blue;"}

```{r}
glance(m_iq) |> pull(r.squared)
```

[This is the total/overall effect of `iq` on exam scores]{style="color:blue;"}
:::

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
Do the individual $R^2$ for the study hours only and iq only models add up to the $R^2$ for the two predictor model?
:::

:::{.fragment}
[No, that would count the shared variance (blue) twice.]{style="color:blue;"}
```{r}
glance(m_sh) |> pull(r.squared) + glance(m_iq) |> pull(r.squared)
```

```{r}
glance(m_full) |> pull(r.squared)
```
:::

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
How is the $\Delta R^2$ for the study hours only model represented in this pie chart?
:::

:::{.fragment}
[(Green) / Full Pie]{style="color:blue;"}

[This is the unique effect of `study_hours` on exam scores, controlling for`iq`.  This unique variancce is relative to total variance in $Y$ ]{style="color:blue;"}
```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_iq) |> pull(r.squared))
```

```{r}
(sse(m_iq) - sse(m_full)) / sse(m_mo)
```
:::

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
How is the $\Delta R^2$ for the iq only model represented in this pie chart?
:::

:::{.fragment}
[Yellow / Full Pie]{style="color:blue;"}

[This is the unique effect of `iq` on exam scores, controlling for`study hours`.  This unique variance is relative to total variance in $Y$ ]{style="color:blue;"}
```{r}
(glance(m_full) |> pull(r.squared)) - (glance(m_sh) |> pull(r.squared))
```

```{r}
(sse(m_sh) - sse(m_full)) / sse(m_mo)
```
:::

-----

## $R^2$: Pie Chart

```{r}
#| echo: false

pie_4
```

:::{.callout-important}
# Question
Do the $\Delta R^2$ values for `study_hours` and `iq` sum to the $R^2$ value for the full model?
:::

:::{.fragment}
[No, they do not.  The $\Delta R^2$ values are the unique variance explained by each predictor.  They do not include the shared variance in $Y$ that is explained by both predictors.]{style="color:blue;"}

```{r}
(glance(m_full)$r.squared - glance(m_iq)$r.squared) + (glance(m_full)$r.squared - glance(m_sh)$r.squared)
```

```{r}
glance(m_full)$r.squared
```
:::

-----

## $R^2$: Pie Chart

:::{.callout-important}
# Question
Is there any situation where the $\Delta R^2$ for the individual $X$s **will**  sum to the $R^2$ of the full model?
:::

:::{.fragment}
[yes, if the $x$s do not predict shared variance in $y$.  this will happen if the $x$s are uncorrelated in the sample, which typically only occurs in balanced experimental designs.]{style="color:blue;"}

[In this situation the overall effect of each predictor (i.e., its $R^2$ from its respective one predictor model) will also equal the unique effect for each predictor (its $\Delta R^2$)]{style="color:blue;"}

```{r}
#| echo: false

tibble(source = fct(c("unexplained", "X1", "X2"), 
                    levels = c("unexplained", "X1", "X2")),
       value = c(50, 20, 10)) |> 
  ggplot(aes(x = "", y=value, fill = source)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    scale_fill_manual(values = c("red", "green", "yellow")) +
    theme_void() # remove background, grid, numeric labels
```
:::

-----

## $\eta_p^2$ or PRE

$\eta_p^2$ or PRE describes how much SSE was reduced (proportionally) in the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.  
\

$PRE = \frac{SSE_c - SSE_a}{SSE_c}$

\

- We can calculate PRE for each $X$ (or later set of $X$s) in the model
- We need to specify the appropriate compact and augmented model

-----

## PRE for `study_hours` and `iq`

$PRE = \frac{SSE_c - SSE_a}{SSE_c}$

\

For `study_hours`

- Compact Model: $\hat{Y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_iq) - sse(m_full)) / sse(m_iq)
```

\

For `iq`

- Compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$


```{r}
(sse(m_sh) - sse(m_full)) / sse(m_sh)
```

-----

-----

## Comparing Variance Based Effect Sizes

$R^2 = \frac{\text{SSE}_{\text{mean-only}} - \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$


$\Delta R^2 = \frac{\text{SSE}_c - \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$


$\eta_p^2 = \frac{\text{SSE}_c - \text{SSE}_a}{\text{SSE}_c}$

-----

$R^2$  

- Describes proportion of explained variance in $Y$ explained by full model relative to total variance in $Y$.   
- Can **not** be used for a specific predictor.   
- Not used frequently in Psychology but useful in other fields.   


$\Delta R^2$  

- Describes proportion of unique variance in $Y$ explained by $X_j$ relative to total variance in $Y$.  
- If $X$s are orthogonal $\Delta R^2$ will sum to $R^2$.   
- Anchored to total $Y$ variance.  
- Same denominator for all $X$s.  


$\eta_p^2$  

- Describes proportion of reduction of unexplained variance (SSE) by adding $X_j$.  
- Stable in experimental designs when additional IVs are added.  


-----


## Visualizing the Model

$\hat{\text{FPS}}= `r round(coef(m_3)[1],1)` + `r round(coef(m_3)[2],1)` *\text{BAC} + `r round(coef(m_3)[3],1)` *\text{TA}$ 


```{r}
#| code-fold: true
#| warning: false

library(plot3D)

preds_bac <- seq(min(data$bac), max(data$bac), length.out = 100)
preds_ta <- seq(min(data$ta), max(data$ta), length.out = 100)

preds_x <- expand.grid(bac = preds_bac, ta = preds_ta)

preds_y <- matrix(predict(m_3, newdata = preds_x),
                 nrow = 100,
                 ncol = 100)

scatter3D(data$bac, data$ta, data$fps, pch = 19, col = "black", ticktype = "detailed",
          xlab = "BAC", ylab = "TA", zlab = "FPS",
          surf = list(x = preds_bac, y = preds_ta, z = preds_y, alpha = .9, border = "light grey"))
```



-----

```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta = mean(preds_ta)) 

preds<- preds_x |> 
  bind_cols(predict(m_3, preds_x, interval = "confidence", level = .95) |>
  as_tibble()) 
```

```{r}
ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds$bac, y = preds$fit),
              color = "black", linewidth = 1) +
  geom_ribbon(aes(x = preds$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +
  labs(x = "BAC",
       y = "FPS") 
```


-----

```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta = c(round(mean(preds_ta)), round(mean(preds_ta)-(sd(preds_ta)*1.5)), 
                              round(mean(preds_ta)+(sd(preds_ta)*1.5))))

preds <- preds_x |> 
  bind_cols(predict(m_3, preds_x) |>
  as_tibble()) 
```

```{r}
ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds$bac, y = preds$value, color = as.factor(preds$ta)),
              linewidth = 1) +
  labs(x = "BAC",
       y = "FPS",
       color = "TA") 
```




-----


<span style="color: red;">Question: What would change if TA was mean centered?</span>   

```{r}
#| code-fold: true

m_3 |> 
  broom::tidy()
```

```{r}
#| code-fold: true

m_3 |> 
  broom::glance() |> 
  select(r.squared)
```

-----

```{r}
#| code-fold: true

data <- data |> 
  mutate(ta_c = ta - mean(ta))

m_3_c <- lm(fps ~ bac + ta_c, data = data) 

m_3_c |> 
  broom::tidy()
```

```{r}
#| code-fold: true

m_3_c |> 
  broom::glance() |> 
  select(r.squared)
```

-----

<span style="color: red;">Question: What do you report and why?</span>   

-----


```{r}
#| code-fold: true

m_3_c |> 
  broom::tidy()
```

-----

$\eta_p^2$ effect size for BAC
```{r}
sse_c_bac <- sum(residuals(lm(fps ~ ta_c, data = data))^2)
sse_a <- sum(residuals(m_3_c)^2)

(p_eta_bac <- (sse_c_bac - sse_a)/sse_c_bac)
```

$\eta_p^2$ effect size for TA
```{r}
sse_c_ta <- sum(residuals(lm(fps ~ bac, data = data))^2)

(p_eta_ta <- (sse_c_ta - sse_a)/sse_c_ta)
```

$\eta_p^2$ effect size for intercept
```{r}
sse_c_int <- sum(residuals(lm(fps ~ bac + ta - 1, data = data))^2)

(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)
```

-----

95% CI
```{r}
confint(m_3_c)
```

-----

## Describing Model Results

We regressed fear-potentiated startle (FPS) on Blood alcohol concentration (BAC). We included trait anxiety (mean-centered) as a covariate in this model to increase power to test substantive questions about BAC. We tested partial effects, controlling for all other predictors in the model, from the full model that included both predictors. We provide raw regression coefficients, 95% confidence intervals for these coefficients, and partial eta squared ($\eta_p^2$) to quantify effect sizes for each predictor in Table 1.

FPS was 42.1 $\mu V$ for participants with 0.00% BAC and average trait anxiety, $t(93) = 7.12, p< .001$, indicating that our threat manipulation successfully increased FPS above zero when sober. As expected, the effect of the trait anxiety covariate was significant and reduced error variance by approximately 19%, $t(93)= 4.73, p< .001$.  FPS increased by 0.2 $\mu V$ for every 1 unit increase in trait anxiety.

As predicted, the effect of BAC was significant and reduced error variance by approximately 4%, $t(93)= -2.04, p= .044$.  FPS decreased 1.8 $\mu V$ for every .01% increase in BAC (see Figure 1).

-----


```{r}
coef <- m_3_c |> 
  broom::tidy() |> 
  select(term, estimate, statistic, p.value) |> 
  mutate(p.value = if_else(p.value < .001, "<.001", as.character(round(p.value, 2))))

ci <- confint(m_3_c) |> 
  round(2) |> 
  as_tibble() |> 
  unite(ci, `2.5 %`, `97.5 %`, sep = ", ") |> 
  mutate(ci = str_c("(", ci, ")"))

p_eta <- tibble (peta = c(p_eta_int, p_eta_bac, p_eta_ta)) 

table <- coef |> 
  bind_cols(ci, p_eta) |> 
  mutate(` ` = factor(term, levels = c("(Intercept)", "bac", "ta_c"),
                     labels = c("Intercept", "Blood Alcohol Concentration", "Trait Anxiety"))) |> 
  select(` `, 
         b = estimate,
         `95% CI (b)` = ci,
         `Partial eta squared` = peta,
         t = statistic,
         p = p.value) |> 
  knitr::kable(digits = 2, align = c("l", "c", "c", "c", "c", "c"))
```


-----

**Table 1**
```{r}
#| code-fold: true

table
```

Notes:   
$R^2 = `r round(broom::glance(m_3_c)$r.squared, 3)`, F(2,93) = 13.44, p < .001$***    
Trait anxiety was mean-centered   


-----


```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta_c = mean(data$ta_c))

preds_pub <- predict(m_3_c, preds_x, se.fit = TRUE) |>
  as_tibble() |>
  mutate(upper = fit + se.fit,
         lower= fit - se.fit)
```


```{r}
plot_pub <- ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds_x$bac, y = preds_pub$fit),
              color = "black", linewidth = 1) +
  geom_ribbon(aes(x = preds_x$bac, ymin = preds_pub$lower, ymax = preds_pub$upper), alpha = 0.2) +
  labs(x = "Blood alcohol concentration",
       y = "Fear-potentiated startle") +
  xlim(c(0, .15)) +
  ylim(c(-100, 200))
```


-----

```{r}
plot_pub
```

-----


<span style="color: red;">Question: What other table should you consider in your results?</span>  

-----


<span style="color: blue;">Table of simple correlations between variables. Can summarize with other important info fairly concisely. Could also include reliability, skewness, kurtosis, etc.</span>

```{r}
#| code-fold: true

tibble(` ` = c("Trait Anxiety", "Blood Alcohol Concentration", "Mean", "SD"),
       `Fear Potentiated Startle` = c(cor(data$ta_c, data$fps), cor(data$bac, data$fps),
                                      mean(data$fps), sd(data$fps)),
       `Trait Anxiety` = c(NA, cor(data$bac, data$ta_c), mean(data$ta_c), sd(data$ta_c)),
       `Blood Alcohol Concentration` = c(NA, NA, mean(data$bac), sd(data$bac))) |> 
  knitr::kable(digits = 2, align = c("l", "c", "c", "c"))
```

-----

## Multiple Predictors: Dichotomous Predictor

**Example:** Evaluate the effects of a new intervention for depression.   

$N=200$ participants, randomly assigned to receive new invervention vs. standard of care control.  

Measure depression with CES-D pre- (baseline) and post-intervention.    

<span style="color: red;">Question: How do you evaluate the effect of the new intervention in a one predictor model?</span>   

-----

<span style="color: blue;">1. Code one regressor for intervention group using dummy or zero-centered coefficients. How different?</span>

<span style="color: blue;">2. Regress depression scores from post-intervention on intervention group.</span>   

<span style="color: blue;">3. Test if $b_1$ for Intervention group is non-zero.</span>   

-----

```{r}
data_int <- read_csv(here::here(path_data, "06_two_predictors_intervention.csv"),
                     show_col_types = FALSE)
```

```{r}
m_depress_1 <- lm(depress_post ~ intervention_group, data = data_int)

m_depress_1 |> 
  broom::tidy()
```

<span style="color: red;">Question: What do you conclude? What do you report?</span>   

-----

<span style="color: blue;">The new intervention reduced CES-D depressions scores by approximately `r abs(round(coef(m_depress_1)[2], 1))` units relative to the standard of care control group, $b = -3.6, t(198) = -2.40, p= .0175.$</span>

-----

<span style="color: red;">Question: What else do you report?</span>   


-----

<span style="color: blue;">PRE or $\Delta R^2$. Means/SD of two groups or better - figure with means, standard error and raw (or residual) data points.</span>


```{r}
broom::glance(m_depress_1)$r.squared - broom::glance(lm(depress_post ~ 1, data = data_int))$r.squared
```

*Note: In this case $\Delta R^2$ is the same as model $R^2$ because there is only one predictor (i.e., $R^2$ in our compact/mean-only model is 0).*


-----

<span style="color: red;">Question: What is a *better* analysis to address this question about the effect of the intervention and why is it better?</span>   

-----

<span style="color: blue;">Control for baseline depressions scores to increase power.</span>  

<span style="color: blue;">Baseline depression should be uncorrelated (in the population) with intervention group because participants were randomly assigned to group. Therefore, including baseline depression should not systematically change $b_1$.</span>  

<span style="color: blue;">However, baseline scores are likely strong predictors of post-intervention scores. This will increase model $R^2$, reduce SSE, and therefore reduce the SE for intervention group. More power!</span>

-----

**Previous one predictor model**
```{r}
#| code-fold: true

m_depress_1 |> 
  broom::tidy()
```


**Two predictor model**
```{r}
#| code-fold: true

data_int <- data_int |> 
  mutate(depress_base_c = depress_base - mean(depress_base))

m_depress_2 <- lm(depress_post ~ intervention_group + depress_base_c, data = data_int)

m_depress_2 |> 
  broom::tidy()
```

-----

$\eta_p^2$ effect size for intercept
```{r}
sse_c_int <- sum(residuals(lm(depress_post ~ intervention_group + depress_base_c - 1,
                              data = data_int))^2)
sse_a <- sum(residuals(m_depress_2)^2)

(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)
```

$\eta_p^2$ effect size for intervention group
```{r}
sse_c_group <- sum(residuals(lm(depress_post ~ depress_base_c,
                              data = data_int))^2)

(p_eta_group <- (sse_c_group - sse_a)/sse_c_group)
```

$\eta_p^2$ effect size for baseline depression
```{r}
sse_c_base <- sum(residuals(lm(depress_post ~ intervention_group, data = data_int))^2)

(p_eta_base <- (sse_c_base - sse_a)/sse_c_base)
```

Model $R^2$
```{r}
m_depress_2 |> 
  broom::glance() |> 
  select(r.squared)
```

-----

## Plot Option 1: Raw Data

```{r}
preds_int <- tibble(intervention_group = c(0,1),
                depress_base_c = 0) 

preds_int <- preds_int|> 
  bind_cols(predict(m_depress_2, preds_int, se.fit = TRUE) |>
  as_tibble() |>
  mutate(upper = fit + se.fit,
         lower= fit - se.fit))
```

```{r}
plot_raw <- ggplot() +
  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, 
               fill = as.factor(preds_int$intervention_group)),
           alpha = .4, color = "black") +
  geom_jitter(aes(x = as.factor(data_int$intervention_group), y = data_int$depress_post), 
              width = .02, height = NULL) +
  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, 
                    ymax = preds_int$fit+preds_int$se.fit, 
                    x = as.factor(preds_int$intervention_group)), width = .4) +
  scale_fill_manual(values = c("black", "light grey")) +
  scale_x_discrete(breaks = c(0, 1),
                   labels = c("Standard of Care", "New Intervention")) +
  theme(legend.position = "none") +
  labs(x = "Intervention Group",
       y = "CES-D Score") 
```

-----

```{r}
plot_raw
```



## Plot Option 2: Residuals

```{r}
preds_res <- data_int |> 
  bind_cols(tibble(resids = residuals(m_depress_2))) |> 
  mutate(depress_post_res = depress_post + resids)
```


```{r}
plot_res <- ggplot() +
  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, 
               fill = as.factor(preds_int$intervention_group)),
           alpha = .4, color = "black") +
  geom_jitter(aes(x = as.factor(preds_res$intervention_group), y = preds_res$depress_post_res), 
              width = .02, height = NULL) +
  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, 
                    ymax = preds_int$fit+preds_int$se.fit, 
                    x = as.factor(preds_int$intervention_group)), width = .4) +
  scale_fill_manual(values = c("black", "light grey")) +
  scale_x_discrete(breaks = c(0, 1),
                   labels = c("Standard of Care", "New Intervention")) +
  theme(legend.position = "none") +
  labs(x = "Intervention Group",
       y = "CES-D Score") 
```

-----

```{r}
plot_res
```



## Adding a Third Predictor

```{r}
#| code-fold: true

m_depress_2 |> 
  broom::tidy()
```

<span style="color: red;">Question: What would you do and why if you had also measured sex (unbalanced or balanced) across intervention groups? What would change?</span>   


-----

## Summary: New and Familiar Concepts

- Interpretation of $b_0$, $b_1$, and $b_2$, in 2+ predictor model with continuous and dichotomous focal predictor.  

- Impact of centering $X_1$ or $X_2$ on $b_0$, $b_1$, and $b_2$.  

- What affects standard errors of $b_j$.  

- What is Multicollinearity, how to detect, what are implications if high, and what are solutions.  

- Model effect size ($R^2$).  

- Effect sizes for $X$s ($b_j$s, $\Delta R^2$, $\eta_p^2$).  

- Text, table, and figure descriptions of results.  






