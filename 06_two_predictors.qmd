# Unit 6: Inferences about two predictors (multiple regression without interaction)

```{r}
#| echo: false

options(scipen = 999) # turns off scientific notation
```


## Multiple Regression 2+ Predictors

In this unit, we will consider the case of multiple predictors.

- Consider how the concepts we have discussed so far generalize to the 2 predictor (3 parameter) model.  
- We will start with 2 quantitative predictors example. Will continue with 1 quantitative and 1 dichotomous predictor example.  
- Learn how to quantify, test, and interpret ‘partial’ effects:
    - $b_j$
    - $\Delta R^2, \eta_p^2$
- Multicollinearity
- Text, table and figure descriptions of results
- Generalization to >2 predictors is straightforward.

--------------------------------------------------------------------------------

## Benefits of Multiple Predictors

1. **Statistical power**:  Goal is to increase power to test focal predictor’s effect on DV by adding it to model that contains additional known predictors of DV.

2. **Additional explanatory power**: Goal is to demonstrate that focal predictor adds explanatory power above and beyond other predictor(s) [Unique effect controlling for other predictors].

3. **Efficiency**:  Can test focal effects of two predictors in one study (each benefiting from increased power per point 1).

4. **Mediation**: We have identified a known cause of a DV.  We add a new focal predictor to test if the effect of our known causal IV on the DV is mediated by our focal predictor (i.e., identify “mechanism” of IV effect).

5. **Better prediction**: If we are using the model to predict the DV for individuals, all real life DVs are the result of multiple causes.  Including them will improve prediction.

--------------------------------------------------------------------------------

## Alcohol and Stress Response Dampening (SRD)

Test for Alcohol *Stress response dampening*   

Manipulate BAC (0.00% - 0.15%)  

Stressor Task (threat of unpredictable shock)  

Measure Stress Response (Fear potentiated startle)  

--------------------------------------------------------------------------------

## Two Parameter (1 Predictor) Model

Set up environment and load data

```{r}
#| message: false
#| code-fold: true

library(tidyverse) 
library(broom)
library(patchwork)
theme_set(theme_classic()) 

path_data <- "data_lecture" 

data <- read_csv(here::here(path_data, "06_two_predictors_fps.csv"),
                 show_col_types = FALSE) 
```

Define `my_skim()` function
```{r}
#| code-fold: true

library(skimr)
my_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),
                                n_missing = ~sum(is.na(.), na.rm = TRUE)),
                     numeric = sfl(p25 = NULL,
                                   p75 = NULL,
                                   hist = NULL),
                     character = sfl(min = NULL, max = NULL),
                     factor = sfl(ordered = NULL))
```

--------------------------------------------------------------------------------

Skim the data
```{r}
data |> my_skim()
```

--------------------------------------------------------------------------------

We can also write some functions to help us with calculations and effect sizes.

```{r}
# calculate sum of squared errors for a model
sse <- function(model) {
  sum(residuals(model)^2)
}

# calculate pre for a model comparison
pre <- function(compact, augmented) {
  (sse(compact) - sse(augmented)) / sse(compact)
}
```

--------------------------------------------------------------------------------

Now, we fit the two parameter model to our data again (same as previous unit)

```{r}
m_2 <- lm(fps ~ 1 + bac, data = data)

m_2 |> tidy()
```

:::{.callout-important}
# Question
Describe the interpretation of $b_1$ (coefficient for BAC) and its significance test.
:::

:::{.fragment}
[$b_1$ *describes* the relationship between BAC and FPS in the units of each measure. FPS will decrease by 184 µV for every 1% increase in BAC (It will decrease by 1.84µV for every .01 increase in BAC).]{style="color:blue;"}

[The significance test for $\beta_1$ tests the null hypothesis that the population relationship between BAC and FPS is 0 (i.e., $\beta_1$ = 0, no relationship). We fail to reject this $H_0$. Conclude that alcohol does not affect FPS.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

## Testing Inferences about $\beta_1$

```{r}
m_2 |> tidy()
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
$H_0: \beta_1 = 0$  
$H_a: \beta_1 \neq 0$  

What could we change about the sampling distribution that would make this $b_1$ be less probable given $H_0$ so that we reject the Null?
:::

:::{.fragment}
[If the standard deviation of the sampling distribution (its standard error) was smaller so that the distribution was narrower, $b_1$ would be less probable given $H_0$.]{style="color:blue;"}`

```{r}
#| code-fold: true
#| fig-height: 4
#| fig-width: 8

tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_2), term == "bac")$std.error, m_2$df.residual)) |> 
  ggplot(aes(x = b1, y = probability)) +
  geom_line() +
  geom_vline(xintercept = subset(broom::tidy(m_2), term == "bac")$estimate, 
             color = "red") +
  labs(title = "Sampling Distribution for b1")
```
:::

## Standard Errors of GLM Coefficients

The formula for the standard error for a coefficient $b_j$ in **multiple** regression (i.e., GLM with more than one regressor) is:  

$SE_{bj} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R^2_y)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R^2_j)}}$  

- $R^2_j$ = variance in $X_j$ accounted for by all other regressors ($X$s) in the model (i.e., how redundant is $X_{a_j}$ in model?).

- This is literally: predict $X_j$ as $Y$ with all other regressors as $X$s.  

:::{.callout-note}
# Note 
Formula for the standard error for $b_0$ is different than for $b_j$.  For the two parameter model $SE_{b0}$ is (need matrix notation for $P > 2$):  

$\sqrt {\frac{SSE}{N-P}}*\sqrt{\frac{1}{N}+\frac{X^2}{(N-1)s_x^2}}$
:::

--------------------------------------------------------------------------------

## SE for b_j and R-Squared

$SE_{b_j}= \frac{s_y}{s_j}*\frac{\sqrt{(1-R_y^2)}}{(N-P)}*\frac{1}{\sqrt{(1-R_j^2)}}$  

If we increase $R_y^2$, we would decrease the SE for our regression coefficient.  

--------------------------------------------------------------------------------

## Model Comparison: Testing Inferences about Beta_1

$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$  

:::{.callout-important}
# Question
What two models are you comparing when you test hypotheses about $\beta_1$ for BAC? Describe the logic.
:::

:::{.fragment}
[Compact model:]{style="color:blue;"}

```{r}
m_c <- lm(fps ~ 1, data = data)
```

- $\hat{FPS}_i = \beta_0+0*BAC_i$
- $SSE_c=$ `r round(sse(m_c))`, $P_c=1$     

[Augmented model:]{style="color:blue;"}
```{r}
m_a <- lm(fps ~ 1 + bac, data = data)
```

- $\hat{FPS}_i = \beta_0+\beta_1*BAC_i$    
- $SSE_a=$ `r round(sse(m_a))`, $P_a=2$   

$F(P_a-P_c, N-P_a) = \frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$

```{r}
anova(m_c, m_a)
```
:::

--------------------------------------------------------------------------------

$F(P_a-P_c, N-P_a) = \frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$  

:::{.callout-important}
# Question
What could you change from this model comparison perspective to increase $F$ and probability to reject the $H_0$ about $\beta_1$?
:::

:::{.fragment}
[Make $SSE_a$ smaller by explaining more variance in $Y_i$.]{style="color:blue;"}

[Of course, $R^2 = \frac{SSE_{mean-only} - SSE_a}{SSE_{mean-only}}$]{style="color:blue;"}

[If you decrease $SSE_a$ or increase model $R^2$, you will have more power to reject $H_0$ regarding parameter estimates.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

## Two Parameter (1 Regressor) Model (Continued)

```{r}
#| code-fold: true

m_2 |> tidy()
```

:::{.callout-important}
# Question
What can we do analytically to decrease SSE (increase model $R^2$) in any model?
:::

:::{.fragment}
[Include another regressor ($X$) in the model that accounts for additional variance in $Y$ (reduces SSE).]{style="color:blue;"}

- [When we add $X$s to the model to reduce error or increase $R^2$, rather than to explicitly test questions about these $X$s, we call these $X$s covariates.]{style="color:blue;"}

- [Ideally, covariates should be orthogonal (uncorrelated) with the other $X$s.]{style="color:blue;"}

- [In this experiment, I could have measured another predictor of stress response, Trait Anxiety (TA). TA might be expected to be a robust predictor of FPS. It also should be uncorrelated with BAC because I manipulated BAC.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

Here are the distributions of $Y$ and our two potential $X$s

```{r}
#| code-fold: true
 
plot_fps <- data |> 
  ggplot(aes(x = fps)) +
  geom_histogram(aes(y = after_stat(density)),
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +
  geom_rug(color = "red")

plot_bac <- data |> 
  ggplot(aes(x = bac)) +
  geom_histogram(aes(y = after_stat(density)), boundary = 0,
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  geom_rug(color = "red")

plot_ta <- data |> 
  ggplot(aes(x = ta)) +
  geom_histogram(aes(y = after_stat(density)), boundary = 0, 
                 color = "black", fill = "light grey", bins = 10) +
  geom_density() +
  geom_rug(color = "red") 

plot_fps + plot_bac + plot_ta
```

--------------------------------------------------------------------------------

We can look at correlations among these variables using `corr.test()` from the `psych` package (you may need to install that package)

```{r}
r <- data |> 
  select(where(is.numeric)) |> 
           psych::corr.test()

# corr.test() is not tidy and does not return a dataframe.  Annoying!!
r$r
```

--------------------------------------------------------------------------------

We can also visualize these bivariate relationships using scatterplots

```{r}
data |> 
  ggplot(aes(x = bac, y = fps)) +
  geom_point(alpha = .6)
```

--------------------------------------------------------------------------------

```{r}
data |> 
  ggplot(aes(x = ta, y = fps)) +
  geom_point(alpha = .6)
```

--------------------------------------------------------------------------------

```{r}
data |> 
  ggplot(aes(x = bac, y = ta)) +
  geom_point(alpha = .6)
```


## The Two Predictor and General Linear Models

The two (and $k$) are simple generalizations of the models you have already learned

- Simply add more $X$s to the model 
- Estimate parameters for these $X$s

DATA = MODEL + ERROR

**Two Predictor Model for Sample Data**  

- $Y_i=b_0+b_1X_1+b_2X_2+e_i$  

- $\hat{Y_i}=b_0+b_1X_1+b_2X_2$  


**$k$ Predictor Model for Sample Data**   

- $Y_i=b_0+b_1X_1+...+b_kX_k+e_i$  

- $\hat{Y_i}=b_0+b_1X_1+...+b_kX_k$  

--------------------------------------------------------------------------------

## Testing BAC in a Three Parameter Model (2 Predictors)

```{r}
m_3 <- lm(fps ~ 1 + bac + ta, data = data)

m_3 |> tidy()
```

$\hat{FPS} = `r round(coef(m_3)[1], 1)` + `r round(coef(m_3)[2], 1)` * BAC + `r round(coef(m_3)[3], 1)` * TA$

:::{.callout-important}
# Question
What parameter estimate is used to test our research question about the effect of BAC? What are our $H_0$ and $H_a$ for the associated population parameter?
:::

:::{.fragment}
[$H_0:\beta_1=0; H_a: \beta_1 \neq 0$]{style="color:blue;"}

[We use $b_1$ (`r round(coef(m_3)[2], 1)`) to test our hypothesis about the population effect of BAC ($\beta_1$).]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

m_3 |> tidy()
```

:::{.callout-important}
# Question
Describe conclusion and logic of the test of $H_0:\beta_1=0$ from sampling distribution perspective.
:::

:::{.fragment}
[If $H_0$ is true, we expect a sampling distribution for $b_1$ to have a mean of 0 and an SE of 86.6 (red curve below).]{style="color:blue;"}

[A sample $b_1$ = -177.0 is unlikely (about 2 standard deviations below mean; p = .0437). Therefore, we reject our $H_0$ and conclude that $\beta_1 \neq 0$.]{style="color:blue;"}

[Conclusion is that BAC affects FPS.]{style="color:blue;"}

```{r}
#| code-fold: true

tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(tidy(m_3), term == "bac")$std.error, m_3$df.residual)) |> 
  ggplot(aes(x = b1, y = probability)) +
  geom_line(color = "red") +
  geom_vline(xintercept = subset(tidy(m_3), term == "bac")$estimate, 
             color = "blue") +
  geom_vline(xintercept = -1 * subset(tidy(m_3), term == "bac")$estimate, 
             color = "green") 
```
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
Describe conclusion and logic of the test of $H_0:\beta_1=0$ from model comparison perspective.
:::

:::{.fragment}
[$H_0: \beta_1 = 0; H_a: \beta_1 \neq 0$]{style="color:blue;"}

[**Compact Model:**]{style="color:blue;"}

```{r}
m_c <- lm (fps ~ 1 + ta, data = data)
```

- $\hat{FPS}=\beta_0 + 0*BAC+\beta_2*TA$    
- $SSE_c =$ `r round(sse(m_c), 1)`, $P_c = 2$   

[**Augmented Model:**]{style="color:blue;"}

```{r}
m_a <- lm(fps ~ 1 + bac + ta, data = data)
```

- $\hat{FPS}=\beta_0 + \beta_1*BAC+\beta_2*TA$    
- $SSE_a =$ `r round(sse(m_a), 1)`, $P_a = 3$ 

$F(P_a - P_c, N - P_a) = \frac{(SSE_c - SSE_a)/(P_a-P_c)}{SSE_a/(N-P_a)}$   
$F(1, 93) =$ `r round((sse(m_c) - sse(m_a)) / (sse(m_a) / 93), 2)`, $p =$ `r round(pt(-2.04, 93, lower.tail = TRUE)*2, 4)`
:::

--------------------------------------------------------------------------------

**Two parameter model test of BAC**   

```{r}
#| code-fold: true
m_2 <- lm(fps ~ 1 + bac, data = data)

tidy(m_2)
```

$SSE =$ `r round(sse(m_2), 1)`

**Three parameter model test of BAC**   

```{r}
#| code-fold: true
m_3 <- lm(fps ~ 1 + bac + ta, data = data)
tidy(m_3)
```

$SSE =$ `r round(sse(m_3), 1)`

:::{.callout-important}
# Question
What changed about test of $\beta_1$ (BAC effect) and why?
:::

:::{.fragment}
[**Two parameter model test of BAC**]{style="color:blue;"}    

```{r}
glance(m_2) |> pull(r.squared)
```

[**Three parameter model test of BAC**]{style="color:blue;"}   

```{r}
glance(m_3) |> pull(r.squared)
```

[SSE decreased and $R^2$ increased.]{style="color:blue;"}

[As a result, SE for BAC decreased leading to a larger t-statistic]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

## Standard Error of Partial Regression Coefficient ($b_j$)

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

:::{.callout-important}
# Question
What happens to $SE_{b_j}$ as model $R^2$ ($R^2_y$) increases (holding other factors constant)?
:::

:::{.fragment}
[$SE_{b_j}$ decreases as model $R^2$ increases. In other words, the sampling distribution get narrower.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What happens to significance test of $b_j$ as $\text{SE}_{b_j}$ decreases (holding other factors constant)?
:::

:::{.fragment}
[$t$ increases and associated p-value decreases (More Power!).]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

## Sampling Distributions and Power

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  


**Two parameter model test of BAC**   

- $t(96-2) = \frac{-184 - 0}{95.9}$    
- $t(94) = -1.92$  
- $p = .0579$
```{r}
#| code-fold: true
tidy(m_2)
```

**Three parameter model test of BAC**   

- $t(96-3) = \frac{-177 - 0}{86.6}$    
- $t(93) = -2.04$  
- $p = .0437$
```{r}
#| code-fold: true
tidy(m_3)
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

distr_data <- tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_3), term == "bac")$std.error, m_3$df.residual),
       group = "3 parameter model") |> 
  bind_rows(tibble(b1 = seq(-400,400,.01),
       probability = dt(b1/subset(broom::tidy(m_2), term == "bac")$std.error, m_2$df.residual),
       group = "2 parameter model"))

distr_data |> 
  ggplot(aes(x = b1, y = probability, color = group)) +
  geom_line() +
  scale_color_manual(values = c("red", "blue")) +
  geom_vline(xintercept = subset(broom::tidy(m_2), term == "bac")$estimate, 
             color = "red") +
  geom_vline(xintercept = subset(broom::tidy(m_3), term == "bac")$estimate, 
             color = "blue") 
```

--------------------------------------------------------------------------------

## Sampling Distributions and Precision

$CI_b=b \pm t(\alpha; N-P)*SE_b$


```{r}
confint(m_2)
```

$\Delta$ `r round(as_tibble(confint(m_2))[[2]][2]-as_tibble(confint(m_2))[[1]][2],3)`


```{r}
confint(m_3)
```

$\Delta$ `r round(as_tibble(confint(m_3))[[2]][2]-as_tibble(confint(m_3))[[1]][2],3)`

## Standard Error of Partial Regression Coefficient ($b_j$; Continued)

$t(N-P) = \frac{b_j - 0}{SE_{b_j}}$  

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

$R^2_j$ = variance in $X_j$ accounted for by all other predictors in model (i.e., how redundant is $X_j$ in model?).  

--------------------------------------------------------------------------------

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

:::{.callout-important}
# Question
What other factors affect SE for regression coefficients and how?
:::

:::{.fragment}
- [Increasing $N$ decreases SE (increases power).]{style="color:blue;"}

- [Increasing $P$ increases SE (decreases power)]{style="color:blue;"}

- [Increasing $s_y$ increases SE (decreases power).]{style="color:blue;"}

- [Increasing $s_j$ decreases SE (increases power).]{style="color:blue;"}

- [Increasing $R_j^2$ increases SE (decreases power).]{style="color:blue;"}
:::

## Power and SSE in Two and Three Parameter Models

**Two parameter model test of BAC**   

- Compact Model: 
  - $\hat{FPS}= `r round(coef(lm(fps~1, data = data)), 1)` + 0*{BAC}$   
  - $SSE_c =$ `r round(sum(residuals(lm(fps~1, data = data))^2), 1)`, $P_c$ = 1  

- Augmented Model: 
  - $\hat{FPS}= `r round(coef(m_2)[1],1)` + `r round(coef(m_2)[2],1)` *BAC$   
  - $SSE_a =$ `r round(sum(residuals(m_2)^2), 1)`, $P_a$ = 2  


**Three parameter model test of BAC**   

- Compact Model: 
  - $\hat{FPS}= `r round(coef(lm(fps~ta, data = data))[1],1)` + 0*BAC + `r round(coef(lm(fps~ta, data = data))[2],1)` * TA$   
  - $SSE_c =$ `r round(sum(residuals(lm(fps~ta, data = data))^2),1)`, $P_c$ = 2  

- Augmented Model: 
  - $\hat{FPS}= `r round(coef(m_3)[1],1)` + `r round(coef(m_3)[2],1)` *BAC + `r round(coef(m_3)[3], 1)` *TA$   
  - $SSE_a =$ `r round(sum(residuals(m_3)^2), 1)`, $P_a$ = 3  


$F(P_a - P_c, N - P_a) = \frac{(SSE_c - SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$ 

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
How can you see the increase in power from the model comparison perspective (look back at last slide)?
:::

:::{.fragment}
**Two parameter model test of BAC** 

- $F(2-1, 96-2) = \frac{(`r round(sum(residuals(lm(fps~1, data = data))^2),1)` - `r round(sum(residuals(m_2)^2),1)`)/(2-1)}{`r round(sum(residuals(m_2)^2),1)` / (96-2)}$

- $F(1, 94) = \frac{(`r round(sum(residuals(lm(fps~1, data = data))^2) -  sum(residuals(m_2)^2),1)`)/(1)}{`r round(sum(residuals(m_2)^2)/94, 1)`}$

- $F (1, 94) = `r round((sum(residuals(lm(fps ~ 1, data=data))^2) - sum(residuals(m_2)^2))/(sum(residuals(m_2)^2)/94), 2)`, p = `r round(pt(-1.92, 94, lower.tail = TRUE)*2, 4)`$

**Three parameter model test of BAC** 

- $F(3-2, 96-3) = \frac{(`r round(sum(residuals(lm(fps~ta, data = data))^2),1)` - `r round(sum(residuals(m_3)^2),1)`)/(3-2)}{`r round(sum(residuals(m_3)^2),1)` / (99-3)}$

- $F(1, 93) = \frac{(`r round(sum(residuals(lm(fps~ta, data = data))^2) -  sum(residuals(m_3)^2),1)`)/(1)}{`r round(sum(residuals(m_3)^2)/93, 1)`}$

- $F (1, 93) = `r round((sum(residuals(lm(fps ~ ta, data=data))^2) - sum(residuals(m_3)^2))/(sum(residuals(m_3)^2)/93), 2)`, p = `r round(pt(-2.04, 93, lower.tail = TRUE)*2, 4)`$

[Decreased $SSE_a$ in three parameter model. Flip side of increased model $R^2$.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

$F(P_a - P_c, N - P_a) = \frac{(SSE_c = SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$   

- Impact of $N$ on $P_a$ also clear.

- Impact of $s_y$ and $s_{x_j}$ and multicollinearity less clear in formula.

- Connection to precision of parameter estimation less clear in formula.

--------------------------------------------------------------------------------

## R-Squared_j and Multicollinearity

$SE_{b_j} = \frac{s_y}{s_j}*\frac{\sqrt{(1-R_Y^2)}}{\sqrt{(N-P)}}*\frac{1}{\sqrt{(1-R_j^2)}}$  

- $t(N-P) = \frac{b-0}{SE_b}$   
- $CI_b= b \pm t(\alpha; N-P)*SE_b$   

This decrease in **power and precision** for model parameter estimates (regression coefficients) associated with redundancy among the predictors is called the **problem of Multicollinearity**.  

--------------------------------------------------------------------------------

When there are more than two `X`s in the model, it is **not** sufficient to examine only bivariate correlations among $X$s. 

To determine if a problem exists, calculate Variance Inflation Factors (VIF) for each $X$.  

- $VIF_j = \frac{1}{(1-R^2_j)}$   

- VIF tells you how much $SE_{b_j}$ is increased because of redundancy. VIFs $\ge$ 5 are considered problematic (SE increased by factor of 2.2).  

--------------------------------------------------------------------------------

We can use `car::vif()` to calculate VIFs in R.

```{r}
car::vif(m_3)
```

\

A related concept is **tolerance** 

- $X_i= 1- R^2_j$ 
- Tolerance decreases toward 0 as multicollinearity increases.

--------------------------------------------------------------------------------

Solutions for Multicollinearity include:  

- Drop redundant variable.

- Factor analysis (e.g., PCA) to produce factors that reflect major sources of variance among the redundant predictors.

- This is only a problem for the `X`s in the model with high VIFs. If you don't care about testing them, this is not a problem. Generally, you only care about VIFs for your focal `X`(s).

## Interpretation of Multiple Regression Coefficients

:::{.callout-important}
# Question
What did the value of $b_1$ tell us in a regression model with one $X$?
:::

:::{.fragment}
[The change in $Y$ associated with a one unit increase in $X_1$.]{style="color:blue;"}

[For every 1 unit increase in $X_1$, there will be a $b_1$ unit increase in $Y$.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What about $b_j$ with multiple (e.g., 2) $X$s?
:::

:::{.fragment}
[The change in $Y$ associated with a one unit increase in $X_j$ **controlling for all other $X$s in the model**. "Controlling for" means holding constant.]{style="color:blue;"}

[For every 1 unit increase in $X_j$, there will be a $b_j$ unit increase in $Y$ holding all other $X$s in the model constant.]{style="color:blue;"}
:::

## A Second Example

Now lets switch gears to a new example with new data.

- Do hours of studying per week affect exam performance in 610?
- How might we test this question if we also knew students IQs?

--------------------------------------------------------------------------------

Here are some data to test our question

- Lets make centered `X`s for both of our quantitative predictors
```{r}
data_exam <- read_csv(here::here(path_data, "06_two_predictors_exam.csv"), 
                 show_col_types = FALSE) |> 
  mutate(study_hours_c = study_hours - mean(study_hours),
         iq_c = iq - mean(iq))
```

- And take a quick look at the dataframe
```{r}
data_exam |> head()
```

--------------------------------------------------------------------------------

- And descriptives
```{r}
data_exam |> my_skim()
```

--------------------------------------------------------------------------------

Start by fitting the two parameter model (testing just `study_hours`) 

```{r}
m_exam_2 <- lm(exam ~ 1 + study_hours_c, data = data_exam)

tidy(m_exam_2)
```

--------------------------------------------------------------------------------

```{r}
tidy(m_exam_2)
```

:::{.callout-important}
# Question
What is the interpretation of $b_1$ in this model?
:::

:::{.fragment}
[There is a significant effect of study hours on exam scores.  For every one hour of studying per week, students' exam scores increase by 2.3 points]{style="color:blue;"}
:::


--------------------------------------------------------------------------------

```{r}
tidy(m_exam_2)
```

:::{.callout-important}
# Question
What is the interpretation of $b_0$?
:::

:::{.fragment}
[The expected exam score for a student who studies the mean number of hours per week (from the sample) is 62.1.  This is significantly different from zero, but that test is probably not that meaningful.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

Maybe study hours are related to exam scores only because intelligent students study more (learned early good study habits) and intelligent students do better on exams.  

:::{.callout-important}
# Question
What would you expect about the relationships (i.e., correlations) among study hours, iq, and exam scores if this were true?
:::

:::{.fragment}
[They would all be positively correlated with each other.  This is, in fact, what we see in these data (because they are fake!)]{style="color:blue;"}

```{r}
data_exam |> 
  select(study_hours, iq, exam) |> 
  cor() |> 
  round(2)
```
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
How could you assess the unique effect of `study_hours`, controlling for `iq`?
:::

:::{.fragment}
[Model exam scores as a function of both `study_hours` and `iq`. $b_1$ (effect of study hours) in this model is the unique effect of study_hours, controlling for IQ.]{style="color:blue;"}

```{r}
m_exam_3 <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)
```
:::

--------------------------------------------------------------------------------

```{r}
tidy(m_exam_3)
```

:::{.callout-important}
# Question
Do hours of studying per week affect performance in 610 after controlling for student IQ?</span>   
:::

:::{.fragment}
[Yes, for every one hour of studying per week, students' exam scores increase by 1 point, controlling for IQ.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

```{r}
tidy(m_exam_3)
```

:::{.callout-important}
# Question
What is the interpretation of $b_0$ in this model?
:::

:::{.fragment}
[It is the predicted value for exam scores for someone of mean IQ who studies the mean number of hours per week.  This is descriptively useful but the statistical test against 0 is not very meaningful.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What would change in this model if we did not center `study_hours` and `iq`?
:::

:::{.fragment}
[The intercept would be the expected exam score for a student who studies 0 hours per week and has an IQ of 0.]{style="color:blue;"}

[There would be no change in either $b_1$ or $b_2$.  These are still the unique effects of each of their associated $X$s, holding the other $X$ constant in the model.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

```{r}
tidy(m_exam_2)
```

```{r}
tidy(m_exam_3)
```

\

Look at the effect of `study_hours` across the two models

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
Why did the effect of `study_hours` get smaller after controlling for IQ (see last slide and think about pattern of correlations among predictors)?
:::

:::{.fragment}
```{r}
data_exam |> 
  select(study_hours, iq, exam) |> 
  cor() |> 
  round(2)
```

[1. When study_hours increases, IQ increases.]{style="color:blue;"}

[2. When IQ increases, exam scores increase.]{style="color:blue;"}

[3. The partial effect of `study_hours` on exam scores is smaller if IQ is not allowed to increase.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

## Causal Models

Lets think about a causal model for the relationship between study hours and exam scores

:::{.callout-important}
# Question
How do we get the total (overall) effect of study hours on exam scores, not controlling for any other variables?
:::

:::{.fragment}
[To get the total effect of study_hours on exam score, predict exam scores from study hours, without controlling for any other variables.  This is just the two parameter model we fit earlier.]{style="color:blue;"}
```{r}
lm(exam ~ 1 + study_hours_c, data = data_exam) |> 
  tidy()
```
:::

--------------------------------------------------------------------------------

This is a "path diagram" of our causal model at this point

We can show this total effect of `study_hours` on `exam` using the parameter estimate from the one predictor model.

```{r}
#| echo: false 
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    Y [label = 'exam']
    A->Y [label = '2.34', fontsize = 5]
}
")
```

:::{.callout-note}
# Note
It is common to use standardized parameter estimates in path diagrams, but for instructive purposes, we will use raw parameter estimates
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
But what if we had a more complicated model that acknowledged that IQ may also affect exam scores.  How do we get the direct (unique, partial) effect of study hours on exam scores, controlling for IQ?  And how do we get the direct effect of IQ on exam scores, controlling for study hours?
:::

:::{.fragment}
```{r}
lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam) |> 
  tidy()
```

[These are the parameter estimates for these two `X`s, from the three parameter model.  Each of these parameter estimates is the effect of that $X$, controlling for the other $X$ in the model]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

And here is the path diagram for our updated causal model

```{r}
#| code-fold: true
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '???', fontsize = 5]
{ rank = same; A; B }
}
")
```

:::{.callout-tip}
# Programming Tip
We left the code for this figure above (folded). You can unfold to see how we used the `Diagrammer` package to create this figure.
:::

--------------------------------------------------------------------------------

The total effect of study hours on exam scores is the sum of the direct effect of study hours on exam scores and the indirect or spurious* effect of study hours on exam scores through IQ.  That is currently missing from our model

- This would be considered an indirect effect if we believe that study hours causes IQ
- This would be a spurious effect if we believe that IQ causes study hours or a third variable (not displayed) causes both study hours and IQ
- The decision about which is the most plausible explanation is a theoretical one and/or a function of your research design
```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '???', fontsize = 5]
{ rank = same; A; B }
}
")
```

--------------------------------------------------------------------------------

To calculate this indirect/spurious effect, we first need to calculate the direct effect of study hours on IQ and add it to the path diagram.

```{r}
m_iq <- lm(iq_c ~ 1 + study_hours_c, data = data_exam)

m_iq |> tidy()
```

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '3.75', fontsize = 5]
{ rank = same; A; B }
}
")
```

--------------------------------------------------------------------------------

**Total effect = Direct + Indirect/Spurious**

`r round(coef(m_exam_2)[2], 2)` = `r round(coef(m_exam_3)[2], 2)` +  (`r round(coef(m_iq)[2], 2)` * `r round(coef(m_exam_3)[3], 2)`)

`r round(coef(m_exam_2)[2], 2)` = `r round(coef(m_exam_3)[2], 2)` +  `r round(coef(m_iq)[2] * coef(m_exam_3)[3], 2)`

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    Y [label = 'exam']
    A->Y [label = '2.34', fontsize = 5]
}
")
```

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'study_hours']
    B [label = 'iq']
    Y [label = 'exam']
    A->Y [label = '1.04', fontsize = 5]
    B->Y [label = '.35', fontsize = 5]
    A->B [label = '3.75', fontsize = 5]
{ rank = same; A; B }
}
")
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
In what situation would $b_j$ for a focal predictor (e.g., $X1$ below) not change when you added an additional $X$ (e.g., $X2$)? 

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
}
")
```

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 6

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    B [label = 'X2']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
    B->Y [label = 'b2', fontsize = 5]
    A->B [label = '?', fontsize = 5]
{ rank = same; A; B }
}
")
```
:::

--------------------------------------------------------------------------------

[1. If X2 was completely uncorrelated (orthogonal) with the focal predictor (X1) in those sample data, there would be no change in the parameter estimate when you added X2. The direct effect of X1 would equal its total effect because the indirect/suprious effect through X2 would be zero.]{style="color:blue;"}

[2. This is why uncorrelated predictors/covariates are considered easier to interpret when trying to increase power. If they are related to the DV, they will increase power (< SE) to test your focal variable but they will not change your estimate of the magnitude of the focal variables parameter estimate.]{style="color:blue;"}

[3. Completely orthogonal variables are typically only observed in experimental designs. However, small/trivial, nonsystematic sample $r$ will occur when population $r=0$.]{style="color:blue;"}

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
}
")
```

```{r}
#| echo: false

DiagrammeR::grViz("
digraph{
  graph[rankdir=LR]
  node [shape = box, fontsize = 5, width = .5, height = .2]
    A [label = 'X1']
    B [label = 'X2']
    Y [label = 'Y']
    A->Y [label = 'b1', fontsize = 5]
    B->Y [label = 'b2', fontsize = 5]
    A->B [label = '0', fontsize = 5]
{ rank = same; A; B }
}
")
```

## Visualizing Total and Direct/Unique Effects

In the previous slides, we displayed the relationships between correlated predictors and $Y$ in a path model

We can also visualize these relationships between correlated predictors and $Y$ using Venn diagrams

```{r}
#| echo: false

# models
m_mo <- lm(exam ~ 1, data = data_exam)
m_sh <- lm(exam ~ 1 + study_hours_c, data = data_exam)
m_iq <- lm(exam ~ 1 + iq_c, data = data_exam)
m_full <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)

# SSE of Exam scores parsed
unexplained <- sse(m_full) |> 
  round()
sh <- (sse(m_iq) - sse(m_full)) |> 
  round()
iq <- (sse(m_sh) - sse(m_full)) |> 
  round()
shared <- ((sse(m_mo) - sse(m_full)) - sh - iq) |> 
  round()
total <- sh + iq + shared + unexplained

# mean-only model
venn_exam_only <- tibble(total) |> 
  ggplot(aes(x = "", y = total)) +
  geom_bar(stat = "identity", width = 1, fill = "green") +
  coord_polar("y", start = 0) +
  annotate("text", x= 1, y = 1, label = total, size = 5) +
  theme_void() # remove background, grid, numeric labels

# SH one predictor
venn_sh_only <- list(`SH` = c(sh + shared),
     `Exam Scores` = c(sh + shared, iq + unexplained)) |> 
  ggvenn::ggvenn(c("SH", "Exam Scores"),
         show_percentage = FALSE,
         show_elements = TRUE,
         fill_color = c("blue", "green"))
  
#  IQ one predictor
venn_iq_only <- list(`IQ` = c(iq + shared),
     `Exam Scores` = c(iq + shared, sh + unexplained)) |> 
  ggvenn::ggvenn(c("IQ", "Exam Scores"),
         show_percentage = FALSE,
         show_elements = TRUE,
         fill_color = c("yellow", "green"))

# Two predictor model
venn_full <- list(`SH` = c(sh, shared),
     `IQ` = c(iq, shared),
     `Exam Scores` = c(sh, iq, shared, unexplained)) |> 
  ggvenn::ggvenn(c("SH", "IQ", "Exam Scores"),
         show_percentage = FALSE,
         show_elements = TRUE,
         fill_color = c("blue", "yellow", "green"))

venn_full
```

## Visualizing Total and Direct/Unique Effects

- The overlap among the circles represent the correlations among the variables
- The numbers represent variability in $Y$ attributed to the $X$s.  
- This variability can be partitioned into unique and shared variance among the predictors
- And there will also be unexplained variability in $Y$ that cannot be explained by either predictor
- We will build up to understanding this display conceptually over the next several (many) slides!

```{r}
#| echo: false

venn_full
```

## The Relevant Models

All of these variabilities can be defined in terms of differences in SSE or $var(\hat{Y_i})$ between various models.  Lets fit all the relevant models here

- Mean only model
```{r}
m_mo <- lm(exam ~ 1, data = data_exam)
```

- Study hours one predictor (two parameter) model
```{r}
m_sh <- lm(exam ~ 1 + study_hours_c, data = data_exam)
```

- IQ one predictor (two parameter) model
```{r}
m_iq <- lm(exam ~ 1 + iq_c, data = data_exam)
```

- Full two predictor (three parameter) model
```{r}
m_full <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)
```

## Total Variability in Y 

The formula for the variance of $Y$ is

- $s^2 = \frac{\Sigma{(Y_i - \bar{Y})^2}}{N - 1}$

The numerator of this term is the SSE of the mean-only model for $Y$

- $SSE_{mean-only} = \Sigma{(Y_i - \hat{Y})^2}$
- because $\hat{Y} = \bar{Y}$ in the mean-only model

So we can think of SSE from the mean only model as an index of the total variability in $Y$ (just without the $N-1$)

```{r}
(var(data_exam$exam) * (200 - 1)) |> round() 
```

```{r}
sse(m_mo) |> round()
```

--------------------------------------------------------------------------------

We can represent our understanding of the variability in `exam` scores from the mean-only model as a single circle of a Venn diagram.  

- There are no circles other than for `exam` because there are no predictors in the mean-only model
- The full variability of `exam` is currently unexplained.  The mean-only model predicts a single value for all observations so it cannot explain why the scores on `exam` differ.

```{r}
#| echo: false

venn_exam_only
```

## The One Predictor Study Hours Model 

Now we can add the variability in `exam` explained by `study_hours` to the Venn diagram

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 5
#| out-height: 5in
#| out-width: 5in

venn_sh_only
```

:::{.callout-important}
# Question
How do we get the unexplained variability in `exam` from this one predictor model?
:::

:::{.fragment}
[This is just the SSE from this one predictor `study_hours` model]{style="color:blue;"}
```{r}
sse(m_sh) |> round()
```

[And, of course, this is just the numerator of the variance of the errors in the one predictor model]{style="color:blue;"}

```{r}
(var(residuals(m_sh)) * (200 -1)) |> round()
```
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 5
#| out-height: 5in
#| out-width: 5in

venn_sh_only
```

:::{.callout-important}
# Question
There are two ways to get the variability in `exam` explained by `study_hours` in this one predictor model.  What are they?
:::

:::{.fragment}
[1.  A model comparison between the mean-only model and the one predictor study hours model.  Error is reduced in the study hours model because study hours explained variability that was previously unexplained in the mean-only model.]{style="color:blue;"}

- $SSE_{m\_mo} - SSE_{m\_sh}$

```{r}
(sse(m_mo) - sse(m_sh)) |> round()
```

[2. The variability in `exam` explained by `study_hours` is also contained in the variance of that model's predicted values.]{style="color:blue;"}


- $Data = Model + Error$
- $Var(Y_i) = Var(\hat{Y_i}) + Var(\epsilon_i)$

```{r}
(var(predict(m_sh)) * (200 -1)) |> round()
```
:::

## The One Predictor IQ Model 

We can do the same thing for the one predictor model with IQ

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 5
#| out-height: 5in
#| out-width: 5in

venn_iq_only
```

:::{.callout-important}
# Question
How do we get the unexplained variability in `exam` in the one predictor model `iq` model?
:::

:::{.fragment}
[The SSE from the one predictor `iq` model]{style="color:blue;"}

```{r}
sse(m_iq) |> round()
```

[Or the numerator of the variance of the errors in this one predictor model]{style="color:blue;"}

```{r}
(var(residuals(m_iq)) * (200 -1)) |> round()
```
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 5
#| out-height: 5in
#| out-width: 5in

venn_iq_only
```

:::{.callout-important}
# Question
There are two ways to get the variability in `exam` explained by `iq` in this one predictor model.  What are they?
:::

:::{.fragment}
[1.  A model comparison between the mean only model and the one predictor study hours model. Error is reduced in the iq model because iq explains variability that was previously unexplained in the mean-only model.]{style="color:blue;"}

- $SSE_{m\_mo} - SSE_{m\_sh}$

```{r}
(sse(m_mo) - sse(m_sh)) |> round()
```

[2. The variability in `exam` explained by `iq` is also contained in the variance of that model's predicted values.]{style="color:blue;"}

```{r}
(var(predict(m_iq)) * (200 -1)) |> round()
```
:::


## The Two Predictor Model

We can represent how `study_hours` and `iq` combine to explain variability in `exam`

- `study_hours` and `iq` each explain some variance in `exam` that can not be explained by the other  (their unique, partial, direct effects)
- But given the correlation between them, they also both predict some of the same variability in `exam`.  This shared variance in `exam` can be predicted by either of these predictors.

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
How do we get the unexplained variability in `exam` in the two predictor model?
:::

:::{.fragment}
[The SSE from the two predictor model]{style="color:blue;"}

```{r}
sse(m_full) |> round()
```
:::

## The Two Predictor Model: Unique Effect of Study Hours

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
How do we get the unique variability in `exam` explained by `study_hours`, controlling for `iq` by comparing SSE across two models?
:::

:::{.fragment}
[The SSE from the one predictor model with `iq` minus SSE from the two predictor model. This indexs how much new (unique) variability can be explained by `study_hours` above and beyond what was already explained by `iq` (i.e., the incremental variability explained by `study_hours`)]{style="color:blue;"}

- $SSE_{m\_iq} - SSE_{m\_full}$
- [This is also the model comparison that is used to test the significance of the effect of `study_hours` in the two predictor model.]{style="color:blue;"}

```{r}
(sse(m_iq) - sse(m_full)) |> round()
```
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
What is a second way to get the unique variability in `exam` explained by `study_hours` controlling for `iq`?
:::

:::{.fragment}
[Using the variability of predicted scores from the two predictor model minus the variability of the predicted scores from the one predictor model with `iq`.]{style="color:blue;"}

- $(Var(\hat{Y}_{m\_full}) - Var(\hat{Y}_{m\_iq})) * (N - 1)$
- [Same model comparison but in terms of the variance of the predicted values (corrected for N-1 put in units of sums of squares)]{style="color:blue;"}
- [We simply focus on the explained vs. unexplained variability.  The variability in the predicted values is sometimes called $SS_{regression}$ or if $X$ is categorical, $SS_{between}$ in ANOVA.]{style="color:blue;"}

```{r}
((var(predict(m_full)) - var(predict(m_iq))) * (200 - 1))  |> round()
```
:::

## The Two Predictor Model: Unique Effect of IQ

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
How do we get the unique variability in `exam` explained by `iq`, controlling for `study_hours` using SSEs from a model comparison?
:::

:::{.fragment}
The SSE from the one predictor model with `study_hours` minus the SSE from the two predictor model.

- $SSE_{m\_sh} - SSE_{m\_full}$
- This is also the model comparison that is used to test the significance of the effect of `iq` in the two predictor model.
- Of course, we could also get this value focusing on the variability of the predicted scores from the two models.

```{r}
(sse(m_sh) - sse(m_full)) |> round()
```
:::

## The Two Predictor Model: Shared effects

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
How do we get the shared variability in `exam` explained by both `study_hours` and `iq` using SSEs?
:::

:::{.fragment}
There are many different ways to get this value. The most straightforward (IMO) is to subtract down this area.

- Start with SSE of the mean-only model (the total variability in `exam`)
- Subtract the SSE of the two predictor model (the unexplained variability in `exam` from that model)
- Then subtract the unique variances for `study_hours` and `iq` that we calculated on the previous slides
:::

## Effect Sizes: Raw Parameter Estimates

You can describe the effects of your predictors in raw units of change or as proportions of explained variability in $Y$

The raw parameter estimates describe the effects of the predictors in the original units of $X$ and $Y$

- The parameter estimates from the one predictor models describe the overall/total effects of each predictor
- The parameter estimates from the full model describe the unique/partial/direct effects of each predictor

```{r}
m_sh |> tidy()
```

```{r}
m_iq |> tidy()
```

```{r}
m_full |> tidy()
```

## Effect Sizes: Proportions of Explained Variability

There are several options to consider for proportions of explained variability in $Y$ 

- These options differ on whether they focus on the total variability explained by the full model vs. unique variability explained by each predictor
- When focusing on unique predictor variability, they differ on the denominator of the proportion (total variability in $Y$ vs. previously unexplained variability in $Y$)

## Effect Sizes: Model R-Squared

You can quantify and test the overall performance of the model (with all $X$s) using the coefficient of determination ($R^2$).

- $R^2 = \frac{\text{Explained Variability in Y}}{\text{Total Variability in Y}}$
- $R^2$ is the proportion of variability in $Y$ explained by the set of all model predictors (all $X$s) relative to the total variability in $Y$
- In our example, $R^2$ describes the combined effect of `study_hours` and `iq`. 
- In more complex models, $R^2$ will always be predictive strength of the set of all $X$s derived from the predictors. 

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

$R^2 = \frac{\text{Total Explained Variability in Y}}{\text{Total Variability in Y}}$

:::{.callout-important}
# Question
Use SS in the Venn diagram to calculate $R^2$
:::

:::{.fragment}
$\frac{643 + 4066 + 3726}{643 + 4066 + 3726 + 18609}$
:::

--------------------------------------------------------------------------------

$R^2 = \frac{\text{Total Explained Variability in Y}}{\text{Total Variability in Y}}$

The numerator of this formula can be calculated as a model comparison of SSEs

- $SSE_{mean-only} - SSE_{full}$   

The denominator can be obtained directly from the SSE of mean-only model.  This yields another definition of the $R^2$ in terms of SSEs

- $R^2 = \frac{SSE_{mean-only} - SSE_{full}}{SSE_{mean-only} }$   

--------------------------------------------------------------------------------

Given that the numerator of $R^2$ can be defined as a model comparison, we can use the model comparison approach to test hypotheses about it

- $H_0: R^2 = 0$
- $H_a: R^2 > 0$

[Compact model: Mean-only model]{style="color:blue;"}

- $\hat{Exam}_i = \beta_0+0*StudyHours_i + 0*IQ_i$
- $P_c=1$     

[Augmented model: Full model]{style="color:blue;"} 

- $\hat{Exam}_i = \beta_0+\beta_1*StudyHours_i + \beta_2*IQ_i$
- $P_a=3$     

$F(P_a-P_c, N-P_a) = \frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$

```{r}
anova(m_mo, m_full)
```

--------------------------------------------------------------------------------

$R^2 = \frac{\text{Total Explained Variability in Y}}{\text{Total Variability in Y}}$

Remember: $var(Y_i) = var(\hat{Y_i}) + var(e_i)$

- $var(Y_i)$ is total variance in $Y$
- $variance(\hat{Y_i})$ is variance in predicted values
- $variance(e_i)$ is variance in residuals

Therefore, we can get $R^2$ by dividing the variance of the predicted values by the variance of the observed values

$R^2 = \frac{var(\hat{Y_i})}{var(Y_i)}$

```{r}
var(predict(m_full)) / var(data_exam$exam)
```

--------------------------------------------------------------------------------

The easiest way to get $R^2$ for any model is the `glance()` function from the `broom` package.  

- Pass in the full model
- `glance()` provides a variety of full model statistics
- You can either `pull()`, `select()` or use `$r.squared` to get the numeric value or a tibble with the value

```{r}
glance(m_full) |> pull(r.squared)
```

```{r}
glance(m_full) |> select(r.squared)
```

```{r}
glance(m_full)$r.squared
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
Do the $R^2$ from the two one predictor models (`study_hours` & `iq`) models add up to the $R^2$ of the two predictor model?
:::

:::{.fragment}
No, if you add the 2 one predictor model $R^2$s, it will overestimate the full model $R^2$ because you will double count the shared variance!

$R^2_{sh} = \frac{643 + 3726}{643 + 3726 + 4066 + 18609}$

$R^2_{iq} = \frac{4066 + 3726}{643 + 3726 + 4066 + 18609}$

$R^2_{full} = \frac{643 + 4066 + 3726}{643 + 3726 + 4066 + 18609}$
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
Is there any situation where the $R^2$ from 2 one predictor models will add up to the $R^2$ of the two predictor model including both predictors?
:::

:::{.fragment}
Yes, if the two $X$s are uncorrelated so that there is no shared variance in $Y$ predicted by both.
:::


## Effect Size: Predictor Effects

There are two proportion of variance effect sizes for the specific $X$s in your model

- You have already learned about Partial eta-squared ($\eta_p^2$), which Judd et al refer to as PRE.  We will return to this in a moment

- You will first learn about Delta $R^2$ ($\Delta R^2$).

## Effect Size: Delta R-Squared

As the name implies, $\Delta R^2$ is the change (increase) in model $R^2$ for the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.

- We can calculate $\Delta R^2$ for each $X$ (or later set of $X$s) in the model
- We need to specify the appropriate compact and augmented model
- Then: $\Delta R^2 = R^2_a - R^2_c$

for `study_hours`

- compact model: $\hat{y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- augmented model: $\hat{y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

For `iq`

- compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

--------------------------------------------------------------------------------

Conceptually, $\Delta R^2$ for an $X$ is the unique (incremental) variability in $Y$ explained by $X$ relative to the total variability in $Y$

- $\Delta R^2 = \frac{\text{Unique Variability in Y Explained by X}}{\text{Total Variability in Y}}$

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
Use SS in the Venn diagram to calculate $\Delta R^2$ for `study_hours` and `iq`
:::

:::{.fragment}
for `study_hours`

- $\frac{643}{643 + 3726 + 4066 + 18609}$

for `iq`

- $\frac{4066}{643 + 3726 + 4066 + 18609}$
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

Following this conceptual understanding of $\Delta R^2$, we can use SSEs from model comparisons to get $\Delta R^2$ for `study_hours` and `iq`

- $\Delta R^2 = \frac{SSE_c - SSE_a}{SSE_{\text{mean-only}}}$
- Use the compact and augmented model associated with the test of the parameter estimate
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

for `study_hours`

- compact model: $\hat{y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- augmented model: $\hat{y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_iq) - sse(m_full)) / sse(m_mo)
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

for `iq`

- compact model: $\hat{y}_i=\beta_0+\beta_1*study\_hours_i+ 0*iq_i$
- augmented model: $\hat{y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_sh) - sse(m_full)) / sse(m_mo)
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
Do the $\Delta R^2$ each of the predictors (`study_hours` & `iq`) add up to the $R^2$ of the two predictor model?
:::

:::{.fragment}
No, if you add $\Delta R^2$s for the two predictors, it will underestimate the full model $R^2$ because you will miss counting the shared variance!

$\Delta R^2_{sh} = \frac{643}{643 + 3726 + 4066 + 18609}$

$\Delta R^2_{iq} = \frac{4066}{643 + 3726 + 4066 + 18609}$

$R^2_{full} = \frac{643 + 4066 + 3726}{643 + 3726 + 4066 + 18609}$
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
# Question
Is there any situation where the $\Delta R^2$ for each of the two predictors will add up to the $R^2$ of the two predictor model including both predictors?
:::

:::{.fragment}
Yes, if the two $X$s are uncorrelated so that there is no shared variance in $Y$ predicted by both.
:::

## Effect Size: Partial Eta-Squared or PRE

$\eta_p^2$ or PRE describes how much SSE was reduced (proportionally) in the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.  

$PRE = \frac{SSE_c - SSE_a}{SSE_c}$

- We can calculate PRE for each $X$ (or later set of $X$s) in the model
- We need to specify the appropriate compact and augmented model

--------------------------------------------------------------------------------

$PRE = \frac{SSE_c - SSE_a}{SSE_c}$

For `study_hours`

- Compact Model: $\hat{Y}_i=\beta_0+0*study\_hours_i+ \beta_2*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_iq) - sse(m_full)) / sse(m_iq)
```

--------------------------------------------------------------------------------

For `iq`

- Compact Model: $\hat{Y}_i=\beta_0+*\beta_1*study\_hours_i+ 0*iq_i$
- Augmented Model: $\hat{Y}_i=\beta_0+\beta_1*study\_hours_i+ \beta_2*iq_i$

```{r}
(sse(m_sh) - sse(m_full)) / sse(m_sh)
```

## Comparing Delta R-Squared and PRE

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
Define $\Delta R^2$ and $PRE$ for `study_hours` in the two predictor model
:::

:::{.fragment}
The numerator is the same.  The denominator is different!

- $\Delta R^2_{sh} = \frac{643}{643 + 3726 + 4066 + 18609}$
- $PRE_{sh} = \frac{643}{643 + 18609}$
:::

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 4
#| out-height: 4in
#| out-width: 4in

venn_full
```

:::{.callout-important}
Define $\Delta R^2$ and $PRE$ for `iq` in the two predictor model
:::

:::{.fragment}
The numerator is the same.  The denominator is different!

- $\Delta R^2_{iq} = \frac{4066}{643 + 3726 + 4066 + 18609}$
- $PRE_{iq} = \frac{4066}{4066 + 18609}$
:::

## Comparing Variance Based Effect Sizes

$R^2 = \frac{\text{SSE}_{\text{mean-only}} - \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$

$\Delta R^2 = \frac{\text{SSE}_c - \text{SSE}_a}{\text{SSE}_{\text{mean-only}}}$

$\eta_p^2 = \frac{\text{SSE}_c - \text{SSE}_a}{\text{SSE}_c}$

--------------------------------------------------------------------------------

$R^2$  

- Describes proportion of explained variance in $Y$ explained by full model relative to total variance in $Y$.   
- **Not** used for a specific predictor but instead for the full model
- Can use F test with model comparison (full vs. mean-only) to test if the model as a whole predicts variance in $Y$ (multi-df test)
- Not used frequently in Psychology but useful in other fields.   

$\Delta R^2$  

- Describes proportion of unique variance in $Y$ explained by $X_j$ relative to total variance in $Y$.  
- If $X$s are orthogonal $\Delta R^2$ will sum to $R^2$.   
- Anchored to total $Y$ variance.  
- Same denominator for all $X$s.  
- Test is statistically equivalent to test of parameter estimate

$\eta_p^2$  

- Describes proportion of reduction of unexplained variance (SSE) by adding $X_j$.  
- Anchored to unexplained variance from compact model.
- Stable in experimental designs when additional IVs are added.
- Test is statistically equivalent to test of parameter estimate

## Multiple Predictors: Dichotomous Predictor

**Example:** Evaluate the effects of a new intervention for depression.   

- $N=200$ participants, randomly assigned to receive new intervention vs. standard of care control.  
- Measure depression with CES-D pre- (baseline) and post-intervention.    

```{r}
data_int <- read_csv(here::here(path_data, "06_two_predictors_intervention.csv"),
                     show_col_types = FALSE)
```

--------------------------------------------------------------------------------

```{r}
data_int |> my_skim()
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
How do you evaluate the effect of the new intervention in a one predictor model?
:::

:::{.fragment}
1. [Code one regressor for intervention group using dummy or zero-centered coefficients. How different?]{style="color:blue;"}

2. [Regress depression scores from post-intervention on intervention group.]{style="color:blue;"}

3. [Test if $b_1$ for Intervention group is non-zero.]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

m_ig <- lm(depress_post ~ intervention_group, data = data_int)
m_ig |> tidy()
```

:::{.callout-important}
# Question 
What do you conclude? What do you report?
:::

:::{.fragment}
[The new intervention reduced CES-D depressions scores by approximately `r abs(round(coef(m_ig)[2], 1))` units relative to the standard of care control group, $b = -3.6, t(198) = -2.40, p= .0175.$]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What else do you report?
:::

:::{.fragment}
- PRE or $\Delta R^2$ (they are the same in the one predictor model!){style="color:blue;"} 
- Means/SD of two groups{style="color:blue;"}
- figure with means, standard error, and raw (or residual) data points{style="color:blue;"}

```{r}
#| code-fold: true

m_mo <- lm(depress_post ~ 1, data = data_int)
glance(m_ig)$r.squared - glance(m_mo)$r.squared
```

::::{.callout-note}
# Note
In this case $\Delta R^2$ is the same as model $R^2$ because there is only one predictor (i.e., $R^2$ in our compact/mean-only model is 0).
::::
:::

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What is a **better** analysis to address this question about the effect of the intervention and why is it better?
:::

:::{.fragment}
[Control for baseline depressions scores to increase power]{style="color:blue;"}

[Baseline depression should be uncorrelated (in the population) with intervention group because participants were randomly assigned to group. Therefore, including baseline depression should not systematically change $b_1$.]{style="color:blue;"}

[However, baseline scores are likely a strong predictor of post-intervention scores. This will increase model $R^2$, reduce SSE, and therefore reduce the SE for intervention group. More power!]{style="color:blue;"}
:::

--------------------------------------------------------------------------------

Review parameter estimates and their tests.  Does this make sense to you?

**Previous one predictor model for IG**

```{r}
#| code-fold: true

m_ig |> tidy()
```

**Two predictor model**

```{r}
#| code-fold: true

data_int <- data_int |> 
  mutate(depress_base_c = depress_base - mean(depress_base))

m_full <- lm(depress_post ~ intervention_group + depress_base_c, data = data_int)

m_full |> tidy()
```

--------------------------------------------------------------------------------

Review $R^2$s.  Does this make sense to you?

**Previous one predictor model for IG**

```{r}
#| code-fold: true

glance(m_ig)$r.squared 
```

**Two predictor model**

```{r}
#| code-fold: true

glance(m_full)$r.squared
```

--------------------------------------------------------------------------------

Review $\Delta R^2$ for `intervention_grouup` across two models.  Do they make sense to you?

**One predictor IG model**

```{r}
#| cold-fold: true

(sse(m_mo) - sse(m_ig)) / sse(m_mo)
```

**Two predictor model**

```{r}
#| cold-fold: true

m_base <- lm(depress_post ~ depress_base_c, data = data_int)
(sse(m_base) - sse(m_full)) / sse(m_mo)
```

--------------------------------------------------------------------------------

Review $PRE$ for `intervention_group` across two models.  Do they make sense to you? What about the differences between $\Delta R^2$ and $PRE$ for the two predictor model?

**One predictor IG model**

```{r}
#| cold-fold: true

(sse(m_mo) - sse(m_ig)) / sse(m_mo)
```

**Two predictor model**

```{r}
#| cold-fold: true

m_base <- lm(depress_post ~ depress_base_c, data = data_int)
(sse(m_base) - sse(m_full)) / sse(m_base)
```

--------------------------------------------------------------------------------

## Visualizing the Model

$\hat{FPS}= `r round(coef(m_3)[1],1)` + `r round(coef(m_3)[2],1)` *BAC + `r round(coef(m_3)[3],1)` *TA$ 

```{r}
#| code-fold: true
#| warning: false

library(plot3D)

preds_bac <- seq(min(data$bac), max(data$bac), length.out = 100)
preds_ta <- seq(min(data$ta), max(data$ta), length.out = 100)

preds_x <- expand.grid(bac = preds_bac, ta = preds_ta)

preds_y <- matrix(predict(m_3, newdata = preds_x),
                 nrow = 100,
                 ncol = 100)

scatter3D(data$bac, data$ta, data$fps, pch = 19, col = "black", ticktype = "detailed",
          xlab = "BAC", ylab = "TA", zlab = "FPS",
          surf = list(x = preds_bac, y = preds_ta, z = preds_y, alpha = .9, border = "light grey"))
```

--------------------------------------------------------------------------------

```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta = mean(preds_ta)) 

preds<- preds_x |> 
  bind_cols(predict(m_3, preds_x, interval = "confidence", level = .95) |>
  as_tibble()) 
```

```{r}
ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds$bac, y = preds$fit),
              color = "black", linewidth = 1) +
  geom_ribbon(aes(x = preds$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +
  labs(x = "BAC",
       y = "FPS") 
```

--------------------------------------------------------------------------------

```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta = c(round(mean(preds_ta)), round(mean(preds_ta)-(sd(preds_ta)*1.5)), 
                              round(mean(preds_ta)+(sd(preds_ta)*1.5))))

preds <- preds_x |> 
  bind_cols(predict(m_3, preds_x) |>
  as_tibble()) 
```

```{r}
ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds$bac, y = preds$value, color = as.factor(preds$ta)),
              linewidth = 1) +
  labs(x = "BAC",
       y = "FPS",
       color = "TA") 
```

--------------------------------------------------------------------------------

<span style="color: red;">Question: What would change if TA was mean centered?</span>   

```{r}
#| code-fold: true

m_3 |> 
  broom::tidy()
```

```{r}
#| code-fold: true

m_3 |> 
  broom::glance() |> 
  select(r.squared)
```

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

data <- data |> 
  mutate(ta_c = ta - mean(ta))

m_3_c <- lm(fps ~ bac + ta_c, data = data) 

m_3_c |> 
  broom::tidy()
```

```{r}
#| code-fold: true

m_3_c |> 
  broom::glance() |> 
  select(r.squared)
```

--------------------------------------------------------------------------------

<span style="color: red;">Question: What do you report and why?</span>   

--------------------------------------------------------------------------------

```{r}
#| code-fold: true

m_3_c |> 
  broom::tidy()
```

--------------------------------------------------------------------------------

$\eta_p^2$ effect size for BAC
```{r}
sse_c_bac <- sum(residuals(lm(fps ~ ta_c, data = data))^2)
sse_a <- sum(residuals(m_3_c)^2)

(p_eta_bac <- (sse_c_bac - sse_a)/sse_c_bac)
```

$\eta_p^2$ effect size for TA
```{r}
sse_c_ta <- sum(residuals(lm(fps ~ bac, data = data))^2)

(p_eta_ta <- (sse_c_ta - sse_a)/sse_c_ta)
```

$\eta_p^2$ effect size for intercept
```{r}
sse_c_int <- sum(residuals(lm(fps ~ bac + ta - 1, data = data))^2)

(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)
```

--------------------------------------------------------------------------------

95% CI
```{r}
confint(m_3_c)
```

--------------------------------------------------------------------------------

## Describing Model Results

We regressed fear-potentiated startle (FPS) on Blood alcohol concentration (BAC). We included trait anxiety (mean-centered) as a covariate in this model to increase power to test substantive questions about BAC. We tested partial effects, controlling for all other predictors in the model, from the full model that included both predictors. We provide raw regression coefficients, 95% confidence intervals for these coefficients, and partial eta squared ($\eta_p^2$) to quantify effect sizes for each predictor in Table 1.

FPS was 42.1 $\mu V$ for participants with 0.00% BAC and average trait anxiety, $t(93) = 7.12, p< .001$, indicating that our threat manipulation successfully increased FPS above zero when sober. As expected, the effect of the trait anxiety covariate was significant and reduced error variance by approximately 19%, $t(93)= 4.73, p< .001$.  FPS increased by 0.2 $\mu V$ for every 1 unit increase in trait anxiety.

As predicted, the effect of BAC was significant and reduced error variance by approximately 4%, $t(93)= -2.04, p= .044$.  FPS decreased 1.8 $\mu V$ for every .01% increase in BAC (see Figure 1).

--------------------------------------------------------------------------------

```{r}
coef <- m_3_c |> 
  broom::tidy() |> 
  select(term, estimate, statistic, p.value) |> 
  mutate(p.value = if_else(p.value < .001, "<.001", as.character(round(p.value, 2))))

ci <- confint(m_3_c) |> 
  round(2) |> 
  as_tibble() |> 
  unite(ci, `2.5 %`, `97.5 %`, sep = ", ") |> 
  mutate(ci = str_c("(", ci, ")"))

p_eta <- tibble (peta = c(p_eta_int, p_eta_bac, p_eta_ta)) 

table <- coef |> 
  bind_cols(ci, p_eta) |> 
  mutate(` ` = factor(term, levels = c("(Intercept)", "bac", "ta_c"),
                     labels = c("Intercept", "Blood Alcohol Concentration", "Trait Anxiety"))) |> 
  select(` `, 
         b = estimate,
         `95% CI (b)` = ci,
         `Partial eta squared` = peta,
         t = statistic,
         p = p.value) |> 
  knitr::kable(digits = 2, align = c("l", "c", "c", "c", "c", "c"))
```

--------------------------------------------------------------------------------

**Table 1**
```{r}
#| code-fold: true

table
```

Notes:   
$R^2 = `r round(broom::glance(m_3_c)$r.squared, 3)`, F(2,93) = 13.44, p < .001$***    
Trait anxiety was mean-centered   

--------------------------------------------------------------------------------

```{r}
preds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), 
                       ta_c = mean(data$ta_c))

preds_pub <- predict(m_3_c, preds_x, se.fit = TRUE) |>
  as_tibble() |>
  mutate(upper = fit + se.fit,
         lower= fit - se.fit)
```

```{r}
plot_pub <- ggplot() +
  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +
  geom_line(aes(x = preds_x$bac, y = preds_pub$fit),
              color = "black", linewidth = 1) +
  geom_ribbon(aes(x = preds_x$bac, ymin = preds_pub$lower, ymax = preds_pub$upper), alpha = 0.2) +
  labs(x = "Blood alcohol concentration",
       y = "Fear-potentiated startle") +
  xlim(c(0, .15)) +
  ylim(c(-100, 200))
```

--------------------------------------------------------------------------------

```{r}
plot_pub
```

--------------------------------------------------------------------------------

<span style="color: red;">Question: What other table should you consider in your results?</span>  

--------------------------------------------------------------------------------

<span style="color: blue;">Table of simple correlations between variables. Can summarize with other important info fairly concisely. Could also include reliability, skewness, kurtosis, etc.</span>

```{r}
#| code-fold: true

tibble(` ` = c("Trait Anxiety", "Blood Alcohol Concentration", "Mean", "SD"),
       `Fear Potentiated Startle` = c(cor(data$ta_c, data$fps), cor(data$bac, data$fps),
                                      mean(data$fps), sd(data$fps)),
       `Trait Anxiety` = c(NA, cor(data$bac, data$ta_c), mean(data$ta_c), sd(data$ta_c)),
       `Blood Alcohol Concentration` = c(NA, NA, mean(data$bac), sd(data$bac))) |> 
  knitr::kable(digits = 2, align = c("l", "c", "c", "c"))
```

--------------------------------------------------------------------------------

## Plot Option 1: Raw Data

Get predicted values for mean baseline depression
```{r}
preds <- tibble(intervention_group = c(0,1),
                depress_base_c = 0) 

preds <- preds|> 
  bind_cols(predict(m_full, preds, se.fit = TRUE) |>
  as_tibble() |>
  mutate(upper = fit + se.fit,
         lower= fit - se.fit))
```

```{r}
#| code-fold: true

ggplot() +
  geom_col(aes(x = as.factor(preds$intervention_group), y = preds$fit, 
               fill = as.factor(preds$intervention_group)),
           alpha = .4, color = "black") +
  geom_jitter(aes(x = as.factor(data_int$intervention_group), 
              y = data_int$depress_post), 
              width = .02, height = NULL) +
  geom_errorbar(aes(ymin = preds$lower, 
                    ymax = preds$upper, 
                    x = as.factor(preds$intervention_group)), width = .4) +
  scale_fill_manual(values = c("black", "light grey")) +
  scale_x_discrete(breaks = c(0, 1),
                   labels = c("Standard of Care", "New Intervention")) +
  theme(legend.position = "none") +
  labs(x = "Intervention Group",
       y = "CES-D Score") 
```

## Plot Option 2: Residuals

Instead of the raw scores, we can plot depression scores after removing the variance due to baseline individual differences

```{r}
res <- tibble(intervention_group = data_int$intervention_group,
              depress_post_res = mean(data_int$depress_post) + residuals(m_full))
```

The variance of the residualized depression scores is lower than for the raw scores because variability from baseline scores has been removed.

```{r}
var(data_int$depress_post)
var(res$depress_post_res)
```

--------------------------------------------------------------------------------

```{r}
ggplot() +
  geom_col(aes(x = as.factor(preds$intervention_group), y = preds$fit, 
               fill = as.factor(preds$intervention_group)),
           alpha = .4, color = "black") +
  geom_jitter(aes(x = as.factor(res$intervention_group), 
                  y = res$depress_post_res), 
              width = .02, height = NULL) +
  geom_errorbar(aes(ymin = preds$lower, 
                    ymax = preds$upper, 
                    x = as.factor(preds$intervention_group)), width = .4) +
  scale_fill_manual(values = c("black", "light grey")) +
  scale_x_discrete(breaks = c(0, 1),
                   labels = c("Standard of Care", "New Intervention")) +
  theme(legend.position = "none") +
  labs(x = "Intervention Group",
       y = "CES-D Score") 
```

## Adding a Third Predictor

```{r}
#| code-fold: true

m_full |> 
  broom::tidy()
```

<span style="color: red;">Question: What would you do and why if you had also measured sex (unbalanced or balanced) across intervention groups? What would change?</span>   

--------------------------------------------------------------------------------

## Summary: New and Familiar Concepts

- Interpretation of $b_0$, $b_1$, and $b_2$, in 2+ predictor model with continuous and dichotomous focal predictor.  
- Impact of centering $X_1$ or $X_2$ on $b_0$, $b_1$, and $b_2$.  
- What affects standard errors of $b_j$.  
- What is Multicollinearity, how to detect, what are implications if high, and what are solutions.  
- Model effect size ($R^2$).  
- Effect sizes for $X$s ($b_j$s, $\Delta R^2$, $\eta_p^2$).  
- Text, table, and figure descriptions of results.  