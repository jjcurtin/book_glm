---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Unit 14: The Generalized Linear Model

```{r}
#| include: false

library(tidyverse)
library(broom)
library(patchwork)
library(skimr)

theme_set(theme_classic()) 

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```

-------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the assumptions of **general** linear models?
:::

:::{.fragment}
The general linear model makes the 5 assumptions below. When these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). 


1. **Exact X**: The IVs are assumed to be known exactly (i.e., without measurement error).
2. **Independence**: Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).
3. **Normality**: All residual distributions are normally distributed.
4. **Constant variance**: All residual distributions have a constant variance, $SEE^2$.
5. **Linearity**: All residual distributions (i.e., for each $Y'$) are assumed to have means equal to zero.
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the consequences of violating each of these assumptions? What options exist when these assumptions are violated?
:::

:::{.fragment}
1. **Exact X**: Biased parameters (to the degree that measurement error exists). Use reliable measures.
2. **Independence**: Inaccurate standard errors, degrees of freedom and significance tests. Use repeated measures or linear mixed effects models or ANCOVA.
3. **Normality**: Inefficient (with large N). Use power transformations, **generalized linear models**.
4. **Constant variance**: Inefficient and inaccurate standard errors. Use power transformations, SE corrections, weighted least squares, **generalized linear models**.
5. **Linearity**: Biased parameter estimates. Use power transformations, polynomial regression, **generalized linear models**.
:::

--------------------------------------------------------------------------------

## An Example

Predicting admission to a grad program in engineering based on quantitative GRE, GPA, and Undergraduate Institution Rank.   

```{r}
#| echo: false

data <- read_csv(here::here("data_lecture/15_grad_admit.csv"),
                 show_col_types = FALSE) |> 
  mutate(rank_2 = fct(rank_2, levels = c("Low rank", "High rank")),
         rank = fct(as.character(rank), levels = c("1", "2", "3","4")))
```

```{r}
data |> head()
```

-------------------------------------------------------------------------------

```{r}
data |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, 
        min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

--------------------------------------------------------------------------------

You will occasionally see linear regressions conducted with binary outcomes. 

- The binary outcome is coded as 0 and 1.
- This model is called the [linear probability model](https://en.wikipedia.org/wiki/Linear_probability_model) where $\hat{Y}$ is considered the probability of membership in the group coded 1

```{r}
m_lm <- lm(admit ~ gpa, data = data)
m_lm |> tidy()
```

::: {.callout-important}
# Question
What is the effect of GPA on admission to grad school?
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the problems with using a general linear model to assess the effects of these predictors on admission outcomes?
:::

:::{.fragment}
1. Residuals will not be normal (not efficient).

2. Residual variance often will not be constant (not efficient, SEs are inaccurate).

3. Probability may not be linear on X (yielding biased parameter estimates).

4. $\hat{Y}$ is not constrained between 0 – 1 (model may make nonsensical predictions if we consider $\hat{Y}$ to be probabilites).
:::

--------------------------------------------------------------------------------

Here is a scatterplot of admit as a function of GPA.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Overplotting can be a problem in scatterplots when one (or both) of the variables are integers. 

Jittering that variable can help.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Clearly not a good model.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Still a problem even if we limit the plot (like we should!) to just the range of observed data for `gpa`
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(2, 4)) +
  scale_y_continuous(limits = c(-2, 1.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Global test of assumptions
```{r}
gvlma::gvlma(m_lm)
```

--------------------------------------------------------------------------------

Normality of residuals
```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_lm), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-4, 4, length.out = 100),
                          y = dnorm(seq(-4, 4, length.out = 100), 
                                    mean = 0, sd = sd(rstudent(m_lm)))), 
            linetype = "dashed", color = "blue")
```

--------------------------------------------------------------------------------

Normality of residuals

```{r}
#| code-fold: true

car::qqPlot(m_lm, id = FALSE, simulate = TRUE,
            main = "QQ Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```

--------------------------------------------------------------------------------

Constant variance of residuals
```{r}
#| code-fold: true

tibble(x = predict(m_lm),
       y = rstudent(m_lm)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs.  Predicted Values",
       x = "Predicted Values",
       y = "Studentized Residuals")
```

--------------------------------------------------------------------------------

```{r}
#| cold-fold: true
#| warning: false

car::spreadLevelPlot(m_lm)
```

--------------------------------------------------------------------------------

Linearity
```{r}
#| code-fold: true

car::crPlots(m_lm, ask = FALSE)
```

--------------------------------------------------------------------------------

## Generalized Linear Models

The Generalized Linear Model is a family of models that includes linear regression, logistic regression, Poisson regression, and others.

The linear model is a special case of this more general(ized) model.

Some of the other models in this family are used to to address problems with model assumptions of the linear model.

- binary outcomes (logistic regression): binomial family
- count data (bounded at 0): poisson family
```{r}
#| echo: false
#| message: false

library(tidyverse)
library(broom)
library(skimr)
library(patchwork)
theme_set(theme_classic()) 
```

`glm(formula, family = familytype(link = linkfunction), data = data)`

```{r}
#| echo: false

tibble(Family = c("binomial", "gaussian", "Gamma", "inverse.gaussian", 
                  "poisson", "quasi", "quasibinomial", "quasipoisson"),
       `Default Link Function` = c('(link = "logit")', '(link = "identity")', 
                                   '(link = "inverse")', '(link = "1/mu^2")', 
                                   '(link = "log")', 
                                   '(link = "indentity", variance = "constant")', 
                                   '(link = "logit")', '(link = "log")')) |> 
  kableExtra::kable() |> 
  kableExtra::kable_styling()
```

--------------------------------------------------------------------------------

Let's fit a logistic regression (binomial) model to predict admission to grad school based on GPA.

```{r}
m_glm <- glm(admit ~ gpa, data = data, family = binomial(logit))
m_glm |> tidy()
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

x <- tibble(gpa = seq(0, 6, length.out = 500))

preds <- predict(m_glm, newdata = x) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(x)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 6)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")

```

NOTE: I plotted GPA from 0 to 6 to show the full range of the logistic function.  In practice, you would limit the x-axis to the range of observed data.

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| warning: false

# warning indicates perfect separation.  This was intentional for this example
ex_data <- tibble(x = c(rep(1:4, 10), rep(5, 10), rep(6:10, 10)),
                 y = c(rep(0, 40), rep(0:1, 5), rep(1, 50))) 

m_ex <- glm(y ~ x, data = ex_data, family = binomial(logit))

preds_ex <- predict(m_ex, newdata = tibble(x = seq(1, 10, length.out = 100))) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(tibble(x = seq(1, 10, length.out = 100)))

ex_data |> 
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = 0, height = .04) +
  scale_x_continuous(limits = c(1, 10), 
                     breaks = c(2, 4, 6, 8, 10)) +
  geom_line(data = preds_ex, aes(x = x, y = value), color = "blue") 
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What other non-linear shapes do you know how to model in the general linear model?
:::

:::{.fragment}
1. Simple monotone relationships with power transforms.

2. You will also eventually learn how to model quadratic, cubic, etc relationships with polynomial regression.
:::

--------------------------------------------------------------------------------

Simple monotone
```{r}
#| echo: false

tibble(x = 1:100,
       y = sqrt(x)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = sqrt(x)")
```


--------------------------------------------------------------------------------

Quadradic

```{r}
tibble(x = -100:100,
       y = x + x^2) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2")
```


--------------------------------------------------------------------------------

Cubic

```{r}
tibble(x = -100:100,
       y = x + x^2 + x^3) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2 + x^3")
```

--------------------------------------------------------------------------------

Not simple monotone (logistic function has two bends)

Not cubic (cubic is unbounded)
```{r}
#| echo: false

m_glm |> tidy()

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

There are two characteristics that vary across the families within the generalized linear models

- The distribution of the residuals
- The link function that connects the $X$s to $\hat{Y}$


--------------------------------------------------------------------------------

**Linear Regression:**   

- Residuals are gaussian.   
- Model predicts raw scores on $Y$. Link function is identity/no transformation ($1 * \hat{Y}$ ).   
  - $1 * \hat{Y} = b_0 + b_1X_1 + ... + b_kX_k$    

**Logistic Regression:**   

- Residuals are binomial   
- Model predicts probability of Y = 1 ($\pi$).  Link function is logit (or log-odds; A transformation of these predicted probabilities).   
  - $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   
  - It is still a linear (on $X$) model

--------------------------------------------------------------------------------

## Logs and Exponentials

You are likely familiar with logs using base 10.

```{r}
log10(10)

log10(100)

log10(1000)

log10(1)

log10(15)

log10(0)

log10(.1)
```

--------------------------------------------------------------------------------

The natural log (often abbreviated ln) is similar but uses base $e$ (approx 2.718) rather than base 10.  

```{r}
log(2.718282)

log(10)

log(1)

log(0)
```

--------------------------------------------------------------------------------

The inverse of the natural log is the exponential function: `exp()`.  

- This function simply raises $e$ to the power of $X$ (whatever value you provide).   
- Logistic regression uses natural logs and exponentials for the transformations that connect $\hat{Y}$ with $X$s.

```{r}
exp(1)

exp(2)

exp(0)

exp(-1)
```

--------------------------------------------------------------------------------

This is the model we are fitting for logistic regression.

- $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   

\

We can manipulate the formula to isolate $\pi$ on one side such that it predicts the probability ($\pi$) of $Y = 1$ as a function of $X$s.

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    
- This is often how we will use the model for prediction (and plots)
- Probability is non-linear on $X$ (as we saw in the previous plot) 

--------------------------------------------------------------------------------

```{r}
m_glm |> tidy()
```

$\pi = \frac{e^{-14.5 + 4.1}}{1 + e^{-14.5 + 4.1X_1}}$   

--------------------------------------------------------------------------------

Logistic regression (and all generalized linear models) can accommodate multiple $X$s.

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

But lets consider simple (one $X$) logistic regression to understand what the intercept and parameter estimates for $X$ tell us about the probability of $Y$.

- $\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

First 

- $b_0 = 0$    
- $b_1 = 0$

No relationship between $X$ and $\pi$.  $\pi$ is always .5.
```{r}
#| echo: false

tibble(x = -10:10,
       b0 = 0,
       b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x))) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line(color = "red") +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = 0$    

- $b_1 = 0 \text{ to }1$   

As $b_1$ becomes more positive, the slope of the logistic function increases.  

- The steeper the slope, the more quickly the probability of $y = 1$ changes as $x$ changes.  
- As $X$ increases, the probability of $Y = 1$ increases.

$\pi$ is 0.5 at mean of $x$ (because $b_0 = 0$).  
```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, .25, .5, 1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = 0$    
- $b_1 = -1 \text{ to }0$   

As $b_1$ becomes more negative (relative to 0), the negative slope of the logistic function increases.  

- The steeper the slope, the more quickly the probability of $y = 1$ changes as $x$ changes.  
- As $X$ increases, the probability of $Y = 1$ decreases.

$\pi$ is 0.5 at mean of $x$ (because $b_0 = 0$).  


```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, -.25, -.5, -1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = -5 \text{ to } 5$    
- $b_1 = 0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

When $b_0 = 0$, the model predicts $\pi = 0.5$ at the mean of $X$.

- As $b_0$ becomes more positive, the model predicts higher probabilites for $Y = 1$ at the mean of $X$.  
- As $b_0$ becomes more negative, the model predicts lower probabilites for $Y = 1$
- This is clearest when $b_1 = 0$.   

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = -5 \text{ to } 5$    
- $b_1 = 1$   

But the same is true for non-zero values for $b_1$.  

- Increases in probability for $Y = 1$ result in left shift of the logistic function.
- Decreases in probability for $Y = 1$ result in right shift of the logistic function.
```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 1) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

## Odds

Odds are a transformation of probabilites that can be useful in some situations (e.g., gambling). 

Odds = $\frac{\pi}{1-\pi}$   

::: {.callout-important}
# Question
What are the odds of obtaining a head on a fair coin toss?
:::

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = \frac{0.5}{0.5} = 1 [1:1]$ 
:::

::::{.fragment}
::: {.callout-important}
# Question
What would the odds of obtaining a head be if I altered the coin to have a probability of heads = 0.67?
:::
::::

:::{.fragment}
Odds = $\frac{0.67}{1-0.67} = \frac{0.67}{0.33} = 2 [2:1]$ 
:::

--------------------------------------------------------------------------------

$\pi = \frac{\text{odds}}{\text{odds} + 1}$    

::: {.callout-important}
# Question
What is the probability of an event that has an odds of 3?
:::

:::{.fragment}
$\pi = \frac{3}{3 + 1} = .75$ 

**Odds can range from 0 - infinity.**
:::

--------------------------------------------------------------------------------

Odds = $\frac{\pi}{1-\pi}$  

::: {.callout-important}
# Question
What are the approximate odds of getting into grad school with a GPA of 3.5?
:::   


```{r}
#| echo: false

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue") +
  labs(y = "p(admit)") +
  geom_vline(xintercept = 3.5, color = "dark green", linewidth = .75) +
  geom_hline(yintercept = .49, color = "dark green", linewidth = .75)
```

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = 1[1:1]$  
:::

--------------------------------------------------------------------------------

We can connect our $X$s to either the probability of $Y = 1$ or the odds of $Y = 1$ 

**Logistic function (probability Y = 1):**   

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  
- $0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1)**    

- $\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    
- $0 \le \text{odds} \le\text{INF}$  


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What are the predicted odds of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

m_glm |> tidy()
```

:::{.fragment}
$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$= e^{-14.5 + 4.1 * 3.5}$   

$= e^{-0.15}$  

$= 0.86 [0.86:1]$   

**Note: Probability of 0.50 occurs with odds of 1.**
:::

--------------------------------------------------------------------------------

Remember that we said the link function for logistic regression is the logit function.  This is yet one more transformation.

**Logistic function (probability Y = 1):**   

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  
- $0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1):**    

- $\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    
- $0 \le \text{odds} \le\text{INF}$  

**Convert odds to log-odds(logit function; log-odds of Y = 1):**   

- $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   
- $-\text{INF} \le logit \le \text{INF}$  
- And now we are back to a linear (on $X$) model!  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What are the log-odds (logit) of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$logit= b_0 + b_1X_1 + ... + b_kX_k$ 

$= b_0 + b_1*gpa$

$= -14.5 + 4.1 * 3.5$   

$= -0.15$  

**Note: Odds of 1 occur when logit = 0.**
:::

--------------------------------------------------------------------------------

Log-odds are not very intuitive.    

However, they are a linear function of our regressors. GLM estimates the parameters in the logit function.   

```{r}
#| code-fold: true

m_glm |> tidy()
```

$logit(pY) = -14.5 + 4.1*gpa$    

We transform the logit function to either odds or probability functions to convey relationships in meaningful units.   

--------------------------------------------------------------------------------

```{r}
#| echo: false

plot_logit <- preds |> 
  mutate(value = log(value/(1-value))) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(-15, 2)) +
  geom_line(color = "blue") +
  labs(title = "Logit(Y)",
       y = "logit(admit)")

plot_odds <- preds |> 
  mutate(value = value/(1-value)) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(0, 8)) +
  geom_line(color = "blue") +
  labs(title = "Odds(Y)",
       y = "odds(admit)")

plot_prob <- ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  geom_line(data = preds |> 
              filter(!gpa > 4), 
            aes(y = value, x = gpa), color = "blue") +
  labs(title = "Probability(Y)",
       y = "p(admit)")

plot_logit + plot_odds + plot_prob
```

--------------------------------------------------------------------------------

Log-odds are a linear function of $X$s, but because log-odds are not intuitive, it decreases the descriptive value of the raw parameter estimates.  

$logit(admission)= -14.5 + 4.1*gpa$    

Not clear how to interpret a parameter estimate of 4.1 for GPA.    

The log-odds of admission increases by 4.1 units for every 1 point increase on GPA??   

```{r}
#| echo: false

plot_logit +
  labs(title = NULL)
```

--------------------------------------------------------------------------------

Odds and probability are more descriptive but they are not linear functions of the $X$s so their parameter estimates aren't very useful to describe effect of $X$s because they are not constant across the values of $X$ (the slope is not linear).    

Can't make simple statement about unit change in odds or probability as a result of unit change in GPA.  

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

```{r}
#| echo: false

plot_logit + plot_odds + plot_prob
```

--------------------------------------------------------------------------------

## Odds Ratios

This is where the odds ratio comes in!   

Odds are defined at a specific point for $X$.   

**Odds ratio ($\Psi$)** = the change in odds for a change in $X$ of some magnitude $c$.   

$\Psi = \frac{odds(X+c)}{odds(X)}$   

$= \frac{e^{b_0 +b_1(X+c)}}{e^{b_0+b_1(X)}}$   

$= e^{c*b_1}$  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What is the odds ratio for a change in GPA of 1.0?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$\Psi = e^{c*b_1}$   

$= e^{1.0*4.1}$   

$= 60.3$     

**The odds of getting into grad school increase by a factor of 60.3 for every 1 point increase in GPA.**    

This is the preferred descriptor for the effect of $X$. For a quantitative variable, choose a meaningful value for $c$ (though often 1 is most appropriate).
:::

--------------------------------------------------------------------------------

$\Psi = e^{c*b_1}$    

- When $b_1 = 0$, $\Psi = 1$, indicating no change in odds for change in $X$.   

- When $b_1 > 0$, $\Psi > 1$, indicating an increase in odds with increasing $X$.   

- When $b_1 < 0$, $\Psi < 1$, indicting a decrease in odds with increasing $X$.   

- Odds ratio ($\Psi$) is never negative.   

--------------------------------------------------------------------------------

## Parameter Significance Tests

There are three common statistical tests for a parameter in logistic regression:   

1. **Z test:** Reported in the `tidy()` output.  

    $z = \frac{b_j}{SE_j}$


2. **Wald test:** Reported by SPSS. Wald is asymptotically distributed as Chi-square with 1 df.   

    $wald = \frac{b_j^2}{SE_j^2}$   

**Both 1 and 2 are not preferred as they have higher type II error rates than the third option.**   

3. **Likelihood ratio test:** Reported in the `anova` output.   

--------------------------------------------------------------------------------

## Likelihood Ratio Test

Deviance is the maximum likelihood generalization of SSE from OLS.   

The likelihood ratio test involves a comparison of two models' deviances.   

To test the effect of GPA, compare deviances.    

- **Compact**: Intercept only (null model)   
- **Augmented**: Model with GPA    
- LR test = compact - augmented    
- Distributed as chi-square with df = df(augmented) - df(compact).    

--------------------------------------------------------------------------------

```{r}
compact <- glm(admit ~ 1, data = data, family = binomial(logit))
augmented <- glm(admit ~ gpa, data = data, family = binomial(logit))

anova(compact, augmented)
```

--------------------------------------------------------------------------------

## Maximum Likelihood Estimation (MLE)

The regression coefficients are estimated using maximum likelihood estimation.   

It is iterative (Newton's method).   

Model may fail to converge if:   

- Ratio of predictors to infrequent cases (10 *events* per predictor).   
- High multicollinearity.
- Sparseness (cells with zero events). Bigger problem for categorical predictor.   
- Complete separation: Predictors perfectly predict the criterion.   

MLE can yield biased parameters with small samples. Definitions of what is not small vary, but $\ge 200$ is a reasonable minimum.   

--------------------------------------------------------------------------------

## Model Assumptions

1. Exact X.
2. Independence.
3. Logistic function and logit correctly specify the form of the relationship (equivalent to the correct fit in linear regression). Logistic function is almost always correct for dichotomous data. You can examine the logit function directly to assess the shape of the relationship.


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What about categorical variables?
:::   


:::{.fragment}
- Handled exactly as in linear regression.  

- Contrast codes, dummy codes.  

- Issues of family-wise error rates apply as before for non-orthogonal and unplanned contrasts. Both Fisher LSD and Holm-Bonferroni are available.

- Odds ratio is for contrast (assuming unit weighted).
:::

--------------------------------------------------------------------------------

Given that you will need to do model comparisons to test contrasts (pocs or dummy), we prefer to manually code contrasts.   

Here is an example with dummy codes for the 4 levels of rank (using low rank/4 as reference) and gpa
```{r}
data <- data |> 
  mutate(d_rank1 = if_else(rank == 1, 1, 0),
         d_rank2 = if_else(rank == 2, 1, 0),
         d_rank3 = if_else(rank == 3, 1, 0))

data |> head()
```

--------------------------------------------------------------------------------

Test main effect of rank (if needed for Fisher LSD).  

- Compact model excludes all the dummy codes
- Compact model DOES include other covariates/predictors (if present in augmented model)
```{r}
m_rank <- glm(admit ~ gpa + d_rank1 + d_rank2 + d_rank3, data = data, family = binomial(logit))
anova(glm(admit ~ gpa, data = data, family = binomial(logit)), m_rank)
```

--------------------------------------------------------------------------------

Test each pairwise contrast

- rank 1 vs. rank 4
```{r}
anova(glm(admit ~ gpa + d_rank2 + d_rank3, data = data, family = binomial(logit)), m_rank)
```

- rank 2 vs. rank 4
```{r}
anova(glm(admit ~ gpa + d_rank1 + d_rank3, data = data, family = binomial(logit)), m_rank)
```

- rank 3 vs. rank 4
```{r}
anova(glm(admit ~ gpa + d_rank1 + d_rank2, data = data, family = binomial(logit)), m_rank)
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What are the odds of getting into grad school from a top (1) ranked university with a gpa of 3.5?
:::

:::{.fragment}

You can use `predict()` with response = "link" to get the log-odds.  Then use `exp()` to get the odds.  
```{r}
odds1 <- tibble(gpa = 3.5, d_rank1 = 1, d_rank2 = 0, d_rank3 = 0) |> 
  predict(m_rank, newdata = _, type = "link") |> 
  exp()
odds1
```
:::

::::{.fragment}
:::{.callout-important}
# Question 
What are the odds of getting into grad school from the lowest (4) ranked university?
:::
::::

:::{.fragment}
```{r}
odds4 <- tibble(gpa = 3.5, d_rank1 = 0, d_rank2 = 0, d_rank3 = 0) |> 
  predict(m_rank, newdata = _, type = "link") |> 
  exp()
odds4
```


And if you wanted the probability, use `response` rather than `link` in the `predict()` function. And remove `exp()`
```{r}
tibble(gpa = 3.5, d_rank1 = 0, d_rank2 = 0, d_rank3 = 0) |> 
  predict(m_rank, newdata = _, type = "response")
```
:::

--------------------------------------------------------------------------------

The odds for low and high rank can be used to calculate the odds ratio.

odds for High rank
```{r}
odds1
```

odds for Low rank
```{r}
odds4
```

Odds ratio
```{r}
odds1/odds4
```

Or directly from the parameter estimate from this contrast ($c$ = 1 for unit weighted contrast or dummy codes)

```{r}
m_rank |> tidy()
```

$c$ = 1
```{r}
m_rank |> tidy() |> 
  filter(term == "d_rank1") |> 
  pull(estimate) |> 
  exp()
```

NOTE: The model we fit was additive so the odds ratio for `rank1` vs. `rank4` (i.e., parameter estimate for `d_rank` is the same across all levels of `gpa`

-------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What about multiple predictors?
:::   

:::{.fragment}
**Nothing new!** (we just used multiple predictors and regressors in the last example)

We tested the main effect and pairwise contrasts for rank, controlling for `gpa` above.  Here we test of the effect of `gpa` controlling for rank.  
```{r}
anova(glm(admit ~ d_rank1 + d_rank2 + d_rank3, data = data, family = binomial(logit)), m_rank)
```

:::

--------------------------------------------------------------------------------

## Results

We analyzed admission (coded 1/0 for yes vs. no, respectively) in a generalized linear model (GLM) that included GPA (quantfied on a four point scale) and School rank (quantified across four levels). We used the binomial family with the logit link function for the GLM because the dependent variable, admission, was dichotomous. School rank was coded using dummy-coded regressors to represent pairwise contrasts between each of the top three highest ranks (1-3) vs. the lowest rank (rank 4). We report the raw parameter estimates from the GLM and the odds ratio to quantify effect size of significant effects.  We report Holm-Bonferroni corrected p-values for the pairwise contrasts across school ranks to protect against inflation of family-wise type 1 error rates.

The effect of GPA was significant, $b = 4.26, SE = 0.45, \chi^2(1) = 142.97, p < .001$, which indicates that the odds of admission increase by a factor of 70.8 for every one point increase in GPA. Applicants from the highest ranked school were significantly more likely to be admitted relative to applicants from the lowest ranked school,  $b = 1.48, SE = 0.46, odds ratio = 4.4, \chi^2(1) = 10.91, p < .001$. There were no signficant differences in admission rates between second and third ranked schools vs. the lowest ranked school.  See figure 1 for display of the probability of admission as a function of GPA and school rank. [Might also consider a table for all parameter estimates, standard errors, odds ratios, and p-values]

--------------------------------------------------------------------------------

## Figure 1

```{r}
#| code-fold: true

ranks <- tibble(rank = fct(rep(c("1", "2", "3", "4"), 50), 
                           levels = c("1", "2", "3", "4")),
                d_rank1 = rep(c(1, 0, 0, 0), 50), 
                d_rank2 = rep(c(0, 1, 0, 0), 50), 
                d_rank3 = rep(c(0, 0, 1, 0), 50))
gpa <- tibble(gpa = rep(seq(2, 4, length.out = 50), 4))
x <- gpa |> 
  bind_cols(ranks)

preds <- x |> 
  bind_cols(predict(m_rank, newdata = x, type = "link",
                 se.fit = TRUE)) |> 
  mutate(upper = plogis(fit + 1.96* se.fit),
         lower = plogis(fit - 1.96 * se.fit),
         fit = plogis(fit)) |>
  select(-se.fit, -residual.scale)

ggplot() +
  geom_point(data = data, aes(x = gpa, y = admit, color = rank)) +
  geom_line(data = preds, aes(y = fit, x = gpa, color = rank), 
            linewidth = .75) +
  labs(y = "p(admit)",
       color = NULL) +
  scale_x_continuous(limits = c(2, 4)) 
```

## All Common Transformations
1. the odds of the event occurring are
odds =
0.2
0.8
= 0.25
2. the log-odds of the event occurring are
ln0.2
0.8

= −1.3863
or
ln(0.25) = −1.3863
3. the probability can be reconstructed as
odds
1 + odds =
0.25
1.25
= 0.2
4. the probability can also be reconstructed as
exp(ln(odds))
1 + exp(ln(odds)) =
exp(−1.3683)
1 + exp(−1.3683) =
0.25
1.25
= 0.2
In R, you can
1. obtain the odds for a given probability by dividing the probability by 1 minus the probability, e.g.,
odds = 0.2/(1-0.2) = 0.25
2. obtain the log-odds for a given probability by taking the natural logarithm of the odds, e.g., log(0.25)
= -1.3862944 or using the qlogis function on the probability value, e.g., qlogis(0.2) = -1.3862944.
3. obtain the probability from the log-odds using exp(x)
1+exp(x)
, where x represents the log-odds value either by
writing the expression out, e.g., exp(-1.3862944)/(1 + exp(-1.3862944)), or by using the plogis
function, e.g., plogis(-1.3862944) = 0.2.
4. obtain the probability from the odds by using odds
1+odds , e.g., 0.25/1.25 = 0.2



