# Unit 14: The Generalized Linear Model

```{r}
#| include: false

library(tidyverse)
library(broom)
library(patchwork)
library(skimr)

theme_set(theme_classic()) 

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```

-------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the assumptions of **general** linear models?
:::

:::{.fragment}
The general linear model makes the 5 assumptions below. When these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). 


1. **Exact X**: The IVs are assumed to be known exactly (i.e., without measurement error).
2. **Independence**: Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).
3. **Normality**: All residual distributions are normally distributed.
4. **Constant variance**: All residual distributions have a constant variance, $SEE^2$.
5. **Linearity**: All residual distributions (i.e., for each $Y'$) are assumed to have means equal to zero.
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the consequences of violating each of these assumptions? What options exist when these assumptions are violated?
:::

:::{.fragment}
1. **Exact X**: Biased parameters (to the degree that measurement error exists). Use reliable measures.
2. **Independence**: Inaccurate standard errors, degrees of freedom and significance tests. Use repeated measures or linear mixed effects models or ANCOVA.
3. **Normality**: Inefficient (with large N). Use power transformations, **generalized linear models**.
4. **Constant variance**: Inefficient and inaccurate standard errors. Use power transformations, SE corrections, weighted least squares, **generalized linear models**.
5. **Linearity**: Biased parameter estimates. Use power transformations, polynomial regression, **generalized linear models**.
:::

--------------------------------------------------------------------------------

## An Example

Predicting admission to a grad program in engineering based on quantitative GRE, GPA, and Undergraduate Institution Rank.   

```{r}
#| echo: false

data <- read_csv(here::here("data_lecture/15_grad_admit.csv"),
                 show_col_types = FALSE) |> 
  mutate(rank_2 = factor(rank_2, levels = c("Low rank", "High rank")))
```

```{r}
data |> head()
```

-------------------------------------------------------------------------------

```{r}
data |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, 
        min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

--------------------------------------------------------------------------------

You will occasionally see linear regressions conducted with binary outcomes. 

- The binary outcome is coded as 0 and 1.
- This model is called the [linear probability model](https://en.wikipedia.org/wiki/Linear_probability_model) where $\hat{Y}$ is considered the probability of membership in the group coded 1

```{r}
m_lm <- lm(admit ~ gpa, data = data)
m_lm |> tidy()
```

::: {.callout-important}
# Question
What is the effect of GPA on admission to grad school?
:::

--------------------------------------------------------------------------------

::: {.callout-important}
# Question
What are the problems with using a general linear model to assess the effects of these predictors on admission outcomes?
:::

:::{.fragment}
1. Residuals will not be normal (not efficient).

2. Residual variance often will not be constant (not efficient, SEs are inaccurate).

3. Probability may not be linear on X (yielding biased parameter estimates).

4. $\hat{Y}$ is not constrained between 0 â€“ 1 (model may make nonsensical predictions if we consider $\hat{Y}$ to be probabilites).
:::

--------------------------------------------------------------------------------

Here is a scatterplot of admit as a function of GPA.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Overplotting can be a problem in scatterplots when one (or both) of the variables are integers. 

Jittering that variable can help.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Clearly not a good model.
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 4)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Still a problem even if we limit the plot (like we should!) to just the range of observed data for `gpa`
```{r}
#| code-fold: true

data |> 
  ggplot(aes(x = gpa, y = admit)) +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(2, 4)) +
  scale_y_continuous(limits = c(-2, 1.2)) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", se = FALSE,
              fullrange = TRUE)  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

Global test of assumptions
```{r}
gvlma::gvlma(m_lm)
```

--------------------------------------------------------------------------------

Normality of residuals
```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_lm), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-4, 4, length.out = 100),
                          y = dnorm(seq(-4, 4, length.out = 100), 
                                    mean = 0, sd = sd(rstudent(m_lm)))), 
            linetype = "dashed", color = "blue")
```

--------------------------------------------------------------------------------

Normality of residuals

```{r}
#| code-fold: true

car::qqPlot(m_lm, id = FALSE, simulate = TRUE,
            main = "QQ Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```

--------------------------------------------------------------------------------

Constant variance of residuals
```{r}
#| code-fold: true

tibble(x = predict(m_lm),
       y = rstudent(m_lm)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs.  Predicted Values",
       x = "Predicted Values",
       y = "Studentized Residuals")
```

--------------------------------------------------------------------------------

```{r}
#| cold-fold: true
#| warning: false

car::spreadLevelPlot(m_lm)
```

--------------------------------------------------------------------------------

Linearity
```{r}
#| code-fold: true

car::crPlots(m_lm, ask = FALSE)
```

--------------------------------------------------------------------------------

## Generalized Linear Models

The Generalized Linear Model is a family of models that includes linear regression, logistic regression, Poisson regression, and others.

The linear model is a special case of this more general(ized) model.

Some of the other models in this family are used to to address problems with model assumptions of the linear model.

- binary outcomes (logistic regression): binomial family
- count data (bounded at 0): poisson family
```{r}
#| echo: false
#| message: false

library(tidyverse)
library(broom)
library(skimr)
library(patchwork)
theme_set(theme_classic()) 
```

`glm(formula, family = familytype(link = linkfunction), data = data)`

```{r}
#| echo: false

tibble(Family = c("binomial", "gaussian", "Gamma", "inverse.gaussian", 
                  "poisson", "quasi", "quasibinomial", "quasipoisson"),
       `Default Link Function` = c('(link = "logit")', '(link = "identity")', 
                                   '(link = "inverse")', '(link = "1/mu^2")', 
                                   '(link = "log")', 
                                   '(link = "indentity", variance = "constant")', 
                                   '(link = "logit")', '(link = "log")')) |> 
  kableExtra::kable() |> 
  kableExtra::kable_styling()
```

--------------------------------------------------------------------------------

Let's fit a logistic regression (binomial) model to predict admission to grad school based on GPA.

```{r}
m_glm <- glm(admit ~ gpa, data = data, family = binomial(logit))
m_glm |> tidy()
```

--------------------------------------------------------------------------------


```{r}
#| code-fold: true

x <- tibble(gpa = seq(0, 6, length.out = 500))

preds <- predict(m_glm, newdata = x) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(x)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  geom_jitter(width = 0, height = .05) +
  scale_x_continuous(limits = c(0, 6)) +
  geom_line(data = preds, aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")

```

NOTE: I plotted GPA from 0 to 6 to show the full range of the logistic function.  In practice, you would limit the x-axis to the range of observed data.

--------------------------------------------------------------------------------

```{r}
#| echo: false
#| warning: false

# warning indicates perfect separation.  This was intentional for this example
ex_data <- tibble(x = c(rep(1:4, 10), rep(5, 10), rep(6:10, 10)),
                 y = c(rep(0, 40), rep(0:1, 5), rep(1, 50))) 

m_ex <- glm(y ~ x, data = ex_data, family = binomial(logit))

preds_ex <- predict(m_ex, newdata = tibble(x = seq(1, 10, length.out = 100))) |> 
  as_tibble() |> 
  mutate(value = boot::inv.logit(value)) |> 
  bind_cols(tibble(x = seq(1, 10, length.out = 100)))

ex_data |> 
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = 0, height = .04) +
  scale_x_continuous(limits = c(1, 10), 
                     breaks = c(2, 4, 6, 8, 10)) +
  geom_line(data = preds_ex, aes(x = x, y = value), color = "blue") 
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question
What other non-linear shapes do you know how to model in the general linear model?
:::

:::{.fragment}
1. Simple monotone relationships with power transforms.

2. You will also eventually learn how to model quadratic, cubic, etc relationships with polynomial regression.
:::

--------------------------------------------------------------------------------

Simple monotone
```{r}
#| echo: false

tibble(x = 1:100,
       y = sqrt(x)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = sqrt(x)")
```


--------------------------------------------------------------------------------

Quadradic

```{r}
tibble(x = -100:100,
       y = x + x^2) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2")
```


--------------------------------------------------------------------------------

Cubic

```{r}
tibble(x = -100:100,
       y = x + x^2 + x^3) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(y = "y = x + x^2 + x^3")
```

--------------------------------------------------------------------------------

Not simple monotone (logistic function has two bends)

Not cubic (cubic is unbounded)
```{r}
#| echo: false

m_glm |> tidy()

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue")  +
  labs(y = "p(admit)")
```

--------------------------------------------------------------------------------

There are two characteristics that vary across the families within the generalized linear models

- The distribution of the residuals
- The link function that connects the $X$s to $\hat{Y}$


--------------------------------------------------------------------------------

**Linear Regression:**   

- Residuals are gaussian.   
- Model predicts raw scores on $Y$. Link function is identity/no transformation ($1 * \hat{Y}$ ).   
  - $1 * \hat{Y} = b_0 + b_1X_1 + ... + b_kX_k$    

**Logistic Regression:**   

- Residuals are binomial   
- Model predicts probability of Y = 1 ($\pi$).  Link function is logit (or log-odds; A transformation of these predicted probabilities).   
  - $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   
  - It is still a linear (on $X$) model

--------------------------------------------------------------------------------

## Logs and Exponentials

You are likely familiar with logs using base 10.

```{r}
log10(10)

log10(100)

log10(1000)

log10(1)

log10(15)

log10(0)

log10(.1)
```

--------------------------------------------------------------------------------

The natural log (often abbreviated ln) is similar but uses base $e$ (approx 2.718) rather than base 10.  

```{r}
log(2.718282)

log(10)

log(1)

log(0)
```

--------------------------------------------------------------------------------

The inverse of the natural log is the exponential function: `exp()`.  

- This function simply raises $e$ to the power of $X$ (whatever value you provide).   
- Logistic regression uses natural logs and exponentials for the transformations that connect $\hat{Y}$ with $X$s.

```{r}
exp(1)

exp(2)

exp(0)

exp(-1)
```

--------------------------------------------------------------------------------

This is the model we are fitting for logistic regression.

- $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   

\

We can manipulate the formula to isolate $\pi$ on one side such that it predicts the probability ($\pi$) of $Y = 1$ as a function of $X$s.

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    
- This is often how we will use the model for prediction (and plots)
- Probability is non-linear on $X$ (as we saw in the previous plot) 

--------------------------------------------------------------------------------

```{r}
m_glm |> tidy()
```

$\pi = \frac{e^{-14.5 + 4.1}}{1 + e^{-14.5 + 4.1X_1}}$   

--------------------------------------------------------------------------------

Logistic regression (and all generalized linear models) can accommodate multiple $X$s.

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$    

But lets consider simple (one $X$) logistic regression to understand what the intercept and parameter estimates for $X$ tell us about the probability of $Y$.

- $\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

First 

- $b_0 = 0$    
- $b_1 = 0$

No relationship between $X$ and $\pi$.  $\pi$ is always .5.
```{r}
#| echo: false

tibble(x = -10:10,
       b0 = 0,
       b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x))) |> 
  ggplot(aes(x = x, y = y)) +
  geom_line(color = "red") +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = 0$    

- $b_1 = 0 \text{ to }1$   

As $b_1$ becomes more positive, the slope of the logistic function increases.  

- The steeper the slope, the more quickly the probability of $y = 1$ changes as $x$ changes.  
- As $X$ increases, the probability of $Y = 1$ increases.

$\pi$ is 0.5 at mean of $x$ (because $b_0 = 0$).  
```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, .25, .5, 1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = 0$    
- $b_1 = -1 \text{ to }0$   

As $b_1$ becomes more negative (relative to 0), the negative slope of the logistic function increases.  

- The steeper the slope, the more quickly the probability of $y = 1$ changes as $x$ changes.  
- As $X$ increases, the probability of $Y = 1$ decreases.

$\pi$ is 0.5 at mean of $x$ (because $b_0 = 0$).  


```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = 0,
            b1 = c(0, -.25, -.5, -1)) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b1 = factor(b1)) |> 
  ggplot(aes(x = x, y = y, color = b1)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = -5 \text{ to } 5$    
- $b_1 = 0$   

```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 0) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

When $b_0 = 0$, the model predicts $\pi = 0.5$ at the mean of $X$.

- As $b_0$ becomes more positive, the model predicts higher probabilites for $Y = 1$ at the mean of $X$.  
- As $b_0$ becomes more negative, the model predicts lower probabilites for $Y = 1$
- This is clearest when $b_1 = 0$.   

--------------------------------------------------------------------------------

$\pi = \frac{e^{b_0 + b_1X_1}}{1 + e^{b_0 + b_1X_1}}$    

- $b_0 = -5 \text{ to } 5$    
- $b_1 = 1$   

But the same is true for non-zero values for $b_1$.  

- Increases in probability for $Y = 1$ result in left shift of the logistic function.
- Decreases in probability for $Y = 1$ result in right shift of the logistic function.
```{r}
#| echo: false

expand.grid(x = -10:10,
            b0 = c(-5, 0, 5),
            b1 = 1) |> 
  mutate(y = exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x)),
         b0 = factor(b0)) |> 
  ggplot(aes(x = x, y = y, color = b0)) +
  geom_line() +
  ylab("p(y = 1)") +
  scale_y_continuous(limits = c(0, 1))
```

--------------------------------------------------------------------------------

## Odds

Odds are a transformation of probabilites that can be useful in some situations (e.g., gambling). 

Odds = $\frac{\pi}{1-\pi}$   

::: {.callout-important}
# Question
What are the odds of obtaining a head on a fair coin toss?
:::

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = \frac{0.5}{0.5} = 1 [1:1]$ 
:::

::::{.fragment}
::: {.callout-important}
# Question
What would the odds of obtaining a head be if I altered the coin to have a probability of heads = 0.67?
:::
::::

:::{.fragment}
Odds = $\frac{0.67}{1-0.67} = \frac{0.67}{0.33} = 2 [2:1]$ 
:::

--------------------------------------------------------------------------------

$\pi = \frac{\text{odds}}{\text{odds} + 1}$    

::: {.callout-important}
# Question
What is the probability of an event that has an odds of 3?
:::

:::{.fragment}
$\pi = \frac{3}{3 + 1} = .75$ 

**Odds can range from 0 - infinity.**
:::

--------------------------------------------------------------------------------

Odds = $\frac{\pi}{1-\pi}$  

::: {.callout-important}
# Question
What are the approximate odds of getting into grad school with a GPA of 3.5?
:::   


```{r}
#| echo: false

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(2, 4)) +
  geom_line(data = preds |> 
              filter(!(gpa > 4 | gpa < 2)), 
            aes(y = value, x = gpa), color = "blue") +
  labs(y = "p(admit)") +
  geom_vline(xintercept = 3.5, color = "dark green", linewidth = .75) +
  geom_hline(yintercept = .49, color = "dark green", linewidth = .75)
```

:::{.fragment}
Odds = $\frac{0.5}{1-0.5} = 1[1:1]$  
:::

--------------------------------------------------------------------------------

We can connect our $X$s to either the probability of $Y = 1$ or the odds of $Y = 1$ 

**Logistic function (probability Y = 1):**   

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  
- $0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1)**    

- $\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    
- $0 \le \text{odds} \le\text{INF}$  


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What are the predicted odds of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

m_glm |> tidy()
```

:::{.fragment}
$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$= e^{-14.5 + 4.1 * 3.5}$   

$= e^{-0.15}$  

$= 0.86 [0.86:1]$   

**Note: Probability of 0.50 occurs with odds of 1.**
:::

--------------------------------------------------------------------------------

Remember that we said the link function for logistic regression is the logit function.  This is yet one more transformation.

**Logistic function (probability Y = 1):**   

- $\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  
- $0 \le\pi \le 1$   

**Convert $\pi$ to odds (odds of Y = 1):**    

- $\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    
- $0 \le \text{odds} \le\text{INF}$  

**Convert odds to log-odds(logit function; log-odds of Y = 1):**   

- $ln(\frac{\pi}{1-\pi}) = b_0 + b_1X_1 + ... + b_kX_k$   
- $-\text{INF} \le logit \le \text{INF}$  
- And now we are back to a linear (on $X$) model!  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What are the log-odds (logit) of getting into grad school with a GPA of 3.5?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$logit= b_0 + b_1X_1 + ... + b_kX_k$ 

$= b_0 + b_1*gpa$

$= -14.5 + 4.1 * 3.5$   

$= -0.15$  

**Note: Odds of 1 occur when logit = 0.**
:::

--------------------------------------------------------------------------------

Log-odds are not very intuitive.    

However, they are a linear function of our regressors. GLM estimates the parameters in the logit function.   

```{r}
#| code-fold: true

m_glm |> tidy()
```

$logit(pY) = -14.5 + 4.1*gpa$    

We transform the logit function to either odds or probability functions to convey relationships in meaningful units.   

--------------------------------------------------------------------------------

```{r}
#| echo: false

plot_logit <- preds |> 
  mutate(value = log(value/(1-value))) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(-15, 2)) +
  geom_line(color = "blue") +
  labs(title = "Logit(Y)",
       y = "logit(admit)")

plot_odds <- preds |> 
  mutate(value = value/(1-value)) |> 
  filter(!gpa > 4) |> 
  ggplot(aes(y = value, x = gpa)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(0, 8)) +
  geom_line(color = "blue") +
  labs(title = "Odds(Y)",
       y = "odds(admit)")

plot_prob <- ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 4)) +
  geom_line(data = preds |> 
              filter(!gpa > 4), 
            aes(y = value, x = gpa), color = "blue") +
  labs(title = "Probability(Y)",
       y = "p(admit)")

plot_logit + plot_odds + plot_prob
```

--------------------------------------------------------------------------------

Log-odds are a linear function of $X$s, but because log-odds are not intuitive, it decreases the descriptive value of the raw parameter estimates.  

$logit(admission)= -14.5 + 4.1*gpa$    

Not clear how to interpret a parameter estimate of 4.1 for GPA.    

The log-odds of admission increases by 4.1 units for every 1 point increase on GPA??   

```{r}
#| echo: false

plot_logit +
  labs(title = NULL)
```

--------------------------------------------------------------------------------

Odds and probability are more descriptive but they are not linear functions of the $X$s so their parameter estimates aren't very useful to describe effect of $X$s because they are not constant across the values of $X$ (the slope is not linear).    

Can't make simple statement about unit change in odds or probability as a result of unit change in GPA.  

$\frac{\pi}{1-\pi} = e^{b_0 + b_1X_1 + ... + b_kX_k}$    

$\pi = \frac{e^{b_0 + b_1X_1 + ... + b_kX_k}}{1 + e^{b_0 + b_1X_1 + ... + b_kX_k}}$  

```{r}
#| echo: false

plot_logit + plot_odds + plot_prob
```

--------------------------------------------------------------------------------

## Odds Ratios

This is where the odds ratio comes in!   

Odds are defined at a specific point for $X$.   

**Odds ratio ($\Psi$)** = the change in odds for a change in $X$ of some magnitude $c$.   

$\Psi = \frac{odds(X+c)}{odds(X)}$   

$= \frac{e^{b_0 +b_1(X+c)}}{e^{b_0+b_1(X)}}$   

$= e^{c*b_1}$  

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What is the odds ratio for a change in GPA of 1.0?
:::   

```{r}
#| code-fold: true

tidy(m_glm)
```

:::{.fragment}
$\Psi = e^{c*b_1}$   

$= e^{1.0*4.1}$   

$= 60.3$     

**The odds of getting into grad school increase by a factor of 60.3 for every 1 point increase in GPA.**    

This is the preferred descriptor for the effect of $X$. For a quantitative variable, choose a meaningful value for $c$ (though often 1 is most appropriate).
:::

--------------------------------------------------------------------------------

$\Psi = e^{c*b_1}$    

- When $b_1 = 0$, $\Psi = 1$, indicating no change in odds for change in $X$.   

- When $b_1 > 0$, $\Psi > 1$, indicating an increase in odds with increasing $X$.   

- When $b_1 < 0$, $\Psi < 1$, indicting a decrease in odds with increasing $X$.   

- Odds ratio ($\Psi$) is never negative.   

--------------------------------------------------------------------------------

## Parameter Significance Tests

There are three common statistical tests for a parameter in logistic regression:   

1. **Z test:** Reported in the `tidy()` output.  

    $z = \frac{b_j}{SE_j}$


2. **Wald test:** Reported by SPSS. Wald is asymptotically distributed as Chi-square with 1 df.   

    $wald = \frac{b_j^2}{SE_j^2}$   

**Both 1 and 2 are not preferred as they have higher type II error rates than the third option.**   

3. **Likelihood ratio test:** Reported in the `anova` output.   

--------------------------------------------------------------------------------

## Likelihood Ratio Test

Deviance is the maximum likelihood generalization of SSE from OLS.   

The likelihood ratio test involves a comparison of two models' deviances.   

To test the effect of GPA, compare deviances.    

- **Compact**: Intercept only (null model)   
- **Augmented**: Model with GPA    
- LR test = compact - augmented    
- Distributed as chi-square with df = df(augmented) - df(compact).    

--------------------------------------------------------------------------------

```{r}
compact <- glm(admit ~ 1, data = data, family = binomial(logit))
augmented <- glm(admit ~ gpa, data = data, family = binomial(logit))

anova(compact, augmented)
```

--------------------------------------------------------------------------------

## Maximum Likelihood Estimation (MLE)

The regression coefficients are estimated using maximum likelihood estimation.   

It is iterative (Newton's method).   

Model may fail to converge if:   

- Ratio of predictors to infrequent cases (10 *events* per predictor).   
- High multicollinearity.
- Sparseness (cells with zero events). Bigger problem for categorical predictor.   
- Complete separation: Predictors perfectly predict the criterion.   

MLE can yield biased parameters with small samples. Definitions of what is not small vary, but $\ge 200$ is a reasonable minimum.   

--------------------------------------------------------------------------------

## Model Assumptions

1. Exact X.
2. Independence.
3. Logistic function and logit correctly specify the form of the relationship (equivalent to the correct fit in linear regression). Logistic function is almost always correct for dichotomous data. You can examine the logit function directly to assess the shape of the relationship.


--------------------------------------------------------------------------------

:::{.callout-important}
# Question 

What about categorical variables?
:::   


:::{.fragment}
- Handled exactly as in linear regression.  

- Contrast codes, dummy codes.  

- Issues of family-wise error rates apply as before for non-orthogonal and unplanned contrasts. Holm-bonferroni is available.

- Odds ratio is for contrast (assuming unit weighted).
:::

--------------------------------------------------------------------------------

```{r}
levels(data$rank_2)
```

```{r}
matrix_contr <- matrix(c(-.5, .5), 
                       ncol = 1,
                       dimnames = list(c("low rank", "high rank"),
                                       c("poc")))

contrasts(data$rank_2) <- matrix_contr

contrasts(data$rank_2)
```

--------------------------------------------------------------------------------

```{r}
m_rank <- glm(admit ~ rank_2, data = data, family = binomial(logit))

tidy(m_rank)
```


--------------------------------------------------------------------------------

```{r}
m_rank_c <- glm(admit ~ 1, data = data, family = binomial(logit))

anova(m_rank_c, m_rank)
```

:::{.callout-important}
# Question 

What are the odds of getting into grad school from a high rank university?
:::

:::{.fragment}
```{r}
exp(m_rank$coefficients[1] +  m_rank$coefficients[2] * 0.5)
```
:::

::::{.fragment}
:::{.callout-important}
# Question 

What are the odds of getting into grad school from a low rank university?
:::
::::

:::{.fragment}
```{r}
exp(m_rank$coefficients[1] +  m_rank$coefficients[2] * -0.5)
```
:::

--------------------------------------------------------------------------------

```{r}
data_new <- tibble(rank_2 = factor(c("High rank", "Low rank")))

(p <- predict(m_rank, newdata = data_new, type = "response")) 
```

odds for High rank
```{r}
p[1] / (1-p[1]) 
```

odds for Low rank
```{r}
p[2] / (1-p[2]) 
```

Odds ratio
```{r}
(p[1] / (1-p[1]))  / (p[2] / (1-p[2]))
```

$c$ = 1
```{r}
exp(m_rank$coefficients[2]) 
```

--------------------------------------------------------------------------------

:::{.callout-important}
# Question 
What about multiple predictors?
:::   

:::{.fragment}
**Nothing new!**
:::

--------------------------------------------------------------------------------

```{r}
data <- data |> 
  mutate(gpa_c = gpa - mean(gpa))

contrasts(data$rank_2)

m_int <- glm(admit ~ gpa_c + rank_2, data = data, family = binomial(logit)) 

summary(m_int)
```

--------------------------------------------------------------------------------

Likelihood ratio test for `rank_2`
```{r}
m_compact <- glm(admit ~ gpa_c + gpa_c:rank_2, data = data, family = binomial(logit))

anova(m_compact, m_int)
```

Likelihood ratio test for interaction

```{r}
m_compact <- glm(admit ~ gpa_c + rank_2, data = data, family = binomial(logit))

anova(m_compact, m_int)
```

--------------------------------------------------------------------------------

odds ratio for 2nd coefficient

```{r}
exp(m_int$coefficients[2])
```


odds ratio for 3rd coefficient

```{r}
exp(m_int$coefficients[3])
```

--------------------------------------------------------------------------------

## Results

We analyzed admission (yes vs. no) in a generalized linear model (GLM) that included GPA, School rank (High rank vs. Low rank) and their interaction as regressors. We used the binomial family with the logit link function for the GLM because the dependent variable, admission, was dichotomous. GPA was mean centered and School rank was coded using centered, unit weighted contrast codes to represent the contrast of High vs. Low rank. We report the raw parameter estimates from the GLM and the odds ratio to quantify effect size of significant effects.

The effect of GPA was significant, $b = 4.37, SE = 0.46, \chi^2(1) = 148.50, p < .001$, which indicates that the odds of admission increase by a factor of 78.7 for every one point increase in GPA. The effect of School rank was significant, $b = 0.96, SE= 0.28, \chi^2(1) = 12.90, p< .001$, which indicates that the odds of admission increase by a factor of 2.6 for students at high relative to low ranked undergraduate institutions. The interaction between GPA and School rank was not significant, $b = -0.97, SE= 0.92, \chi^2(1) = 12.90, p= .282$. See figure 1 for display of the probability of admission as a function of GPA and school rank.

--------------------------------------------------------------------------------


## Figure 1

```{r}
#| code-fold: true

m_int_plot <- glm(admit ~ gpa*rank_2, data = data, family = binomial(logit)) 

x <- expand.grid(gpa = seq(0, 4, length.out = 100),
                 rank_2 = c("Low rank", "High rank"))

preds <- predict(m_int_plot, newdata = x, se.fit = TRUE) |> 
  as_tibble() |> 
  mutate(fit = boot::inv.logit(fit),
         upper = fit + se.fit,
         lower = fit - se.fit) |> 
  bind_cols(x)

ggplot(data = data, aes(x = gpa, y = admit)) +
  geom_point() +
  geom_line(data = preds, aes(y = fit, x = gpa, color = rank_2), 
            linewidth = .75) +
  labs(y = "p(admit)",
       color = NULL) +
  scale_x_continuous(limits = c(0, 4)) 
```

