--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 13: Categorical Variables > 2 Levels {.unnumbered}

```{r}
#| echo: false

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```


## Learning Objectives

- Two categorical coding schemes to know:   
  
    1. Dummy coding (reference/control group).  
    2. Contrast coding (whatever you can dream up!, POCs).   
    
- How best to test the comparisons coded by these methods if the comparisons are:   

    1. Planned and orthogonal.   
    2. planned and non-orthogonal.   
    3. Unplanned.
    
- How to test simple effects when categorical variables have > 2 levels.   

----

<span style="color: red;">Question: How do we handle analysis of categorical variables in regression (a *linear model*)?</span>   

----

<span style="color: blue;">We will need to recode the categorical variable into $N_{\text{levels}}/\text{groups}-1$ regressors that represent comparisons/contrasts within the overall categorical variable.</span>   


<span style="color: blue;">Testing the **set** of regressors will allow us to test the overall effect of the categorical variable but we will often not care about this test. We are generally much more focused on planned or unplanned comparisons between the groups/levels.</span>   

-----

## Example

Examine the differences in overall health among patients with alcohol use disorder, patients with depression, and healthy controls.   

```{r}
#| code-fold: true
#| message: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(patchwork)

theme_set(theme_classic()) 

path_data <- "data_lecture"
```

```{r}
data <- read_csv(here::here(path_data, "13_three_groups.csv"),
                 show_col_types = FALSE) |> 
  mutate(group = factor(group))

slice_sample(data, n = 6)
```

----

<span style="color: red;">Question: Why can we not handle multilevel categorical variables by simply coding each group with a different consecutive value (e.g., alcohol = 1, depress = 2 , healthy = 3)?</span>   

----

<span style="color: blue;">There is no meaningful way to order the multiple groups. The shape of the relationship will completely change based on arbitrary ordering of the groups (exception is when categorical variable is ordinal).</span>   

<span style="color: blue;">Moreover, we are often interested in individual pair-wise group comparisons that would be lost by forcing linear relationship.</span>   

```{r}
#| code-fold: true

order_1 <- data |> 
  mutate(group = case_match(group,
                            "alcohol" ~ 1,
                            "depress" ~ 2,
                            "healthy" ~ 3)) |> 
  ggplot(aes(x = group, y = health)) +
  geom_point() +
  scale_y_continuous(breaks = c(2, 4, 6, 8, 10), limits = c(0, 10)) +
  scale_x_continuous(name = NULL, breaks = c(1, 2, 3), 
                     labels = c("alcohol", "depress", "healthy"))

order_2 <- data |> 
  mutate(group = case_match(group,
                            "alcohol" ~ 2,
                            "depress" ~ 3,
                            "healthy" ~ 1)) |> 
  ggplot(aes(x = group, y = health)) +
  geom_point() +
  scale_y_continuous(breaks = c(2, 4, 6, 8, 10), limits = c(0, 10)) +
  scale_x_continuous(name = NULL, breaks = c(1, 2, 3), 
                     labels = c("healthy", "alcohol", "depress"))

order_3 <- data |> 
  mutate(group = case_match(group,
                            "alcohol" ~ 3,
                            "depress" ~ 1,
                            "healthy" ~ 2)) |> 
  ggplot(aes(x = group, y = health)) +
  geom_point() +
  scale_y_continuous(breaks = c(2, 4, 6, 8, 10), limits = c(0, 10)) +
  scale_x_continuous(name = NULL, breaks = c(1, 2, 3), 
                     labels = c("depress", "healthy", "alcohol"))


order_1 + order_2 + order_3
```

----

## 3 Group Dummy Coding (Non-orthogonal)  

```{r}
#| echo: false

(tibble_d <- tibble(Group = c("alcohol", "depress", "healthy"),
       alcohol_d = c(1, 0, 0),
       depress_d = c(0, 1, 0)))
```

- Need 2 regressors (`alcohol_d`, `depress_d`) to represent a 3 level categorical variable.   

- Regressor 1 is coded 1 for membership in Group 1 (alcohol) and 0 for all other group membership.

- Regressor 2 is coded 1 for membership in Group 2 (depress) and 0 for all other group membership.

- Reference group (healthy) is coded 0 for all regressors.

----


```{r}
data <- data |> 
  mutate(alcohol_d = if_else(group == "alcohol", 1, 0),
         depress_d = if_else(group == "depress", 1, 0))

slice_sample(data, n = 6)
```

----

```{r}
m_d <- lm(health ~ alcohol_d + depress_d, data)

broom::tidy(m_d)
```

----

## 3 Group Dummy: What does $b_j$ test?  

Each regression coefficient represents the contrast between its respective target group (the group that was coded 1 for that contrast) and the **reference** group. This is because each effect represents the effect of that specific IV holding constant the other IV.


```{r}
tibble_d
```

----

For example, consider the effect of the first regressor (`alcohol_d`). Considered by itself, it is the contrast of patients with alcohol use disorder (1) vs. other groups (0). However, if you held the second regressor (`depress_d`) constant at zero you would be examining the effect of patients with alcohol use disorder vs. other groups when it is 0 on `depress_d` (i.e., the control group). 

```{r}
tibble_d
```

----

```{r}
broom::tidy(m_d)

data |> 
  group_by(group) |> 
  summarise(mean = mean(health))
```

----

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/dummy_3d.png"))
```

----


## Dummy Coding Group Mean Estimates

**The Prediction equation:**   

$\hat{Y} = 7.94+-3.36*\text{alcohol}_d+ - 1.50*\text{depress}_d$     


Patients with alcohol use disorder:    

$= 7.94 + -3.36*(1)+-1.50*(0)$   
$= 4.58$   


Patients with depression:    

$= 7.94 + -3.36*(0)+-1.50*(1)$   
$= 6.44$   


Healthy controls:    

$= 7.94 + -3.36*(0)+-1.50*(0)$   
$= 7.94$

<span style="color: red;">Question: What is $b_0$ when using dummy codes?</span>   

----

<span style="color: blue;">The predicted value (mean) for the reference group.</span>    

----

## Dummy Coding Decomposition

**The Prediction equation:**   

$\hat{Y} = 7.94+-3.36*\text{alcohol}_d+ - 1.50*\text{depress}_d$     

alcohol vs. healthy contrast:   

$= (b_0 + b_1*1 + b_2*0) - (b+0 + b_1*0 + b_2*0)$   
$= (b_0 + b_1) - (b_0)$   
$= b_1$   

depress vs. healthy contrast:   

$= (b_0 + b_1*0 + b_2*1) - (b+0 + b_1*0 + b_2*0)$   
$= (b_0 + b_2) - (b_0)$   
$= b_2$   
 
alcohol vs. depress contrast:   

$= (b_0 + b_1*1 + b_2*0) - (b+0 + b_1*0 + b_2*1)$   
$= (b_0 + b_1) - (b_0 + b_2)$   
$= b_1 - b_2$   

----

## Introducing Factors and Contrasts

```{r}
class(data$group)
```

```{r}
levels(data$group)
```

Create contrast matrix
```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 3) 
col_names <- str_c(colnames(matrix_contr), "_v_healthy")
colnames(matrix_contr) <- col_names

matrix_contr
```


----

Set contrasts for `group` factor variable
```{r}
contrasts(data$group) <- matrix_contr
```

Run model

```{r}
m_d_2 <- lm(health ~ group, data = data)
tidy_m_d_2 <-broom::tidy(m_d_2) |> 
  mutate(term = str_replace(term, "group", "")) # remove variable name from term

tidy_m_d_2
```



----

<span style="color: red;">Question: How are these t-tests of $b_j$ in GLM different from what you would get if you simply ran two separate between groups t-tests?</span>   

----

<span style="color: blue;">The GLM t-tests of $b_j$ use the full error term including all subjects (including the “ignored” group; see dfs below). This yields a more powerful test.</span>     

GLM degrees of freedom
```{r}
m_d_2$df.residual
```


t-test 
```{r}
data_sub <- data |> 
  filter(group != "depress")

t.test(health ~ group, data = data_sub, var.equal = TRUE)
```

----


<span style="color: red;">Question: What may be missing from our analysis of the `group` variable at this point?</span>   

----

<span style="color: blue;">We might want a test of the contrast between patients with alcohol use disorder vs. patients with depression.</span>     

----

<span style="color: red;">Question: How could we get this test?</span>   

----

<span style="color: blue;">Recode the group contrasts such that the patients with depression is the reference group. Then test and interpret the alcohol vs. depress parameter estimate.</span>   

----


```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 2) 
col_names <- str_c(colnames(matrix_contr), "_v_depress")
colnames(matrix_contr) <- col_names

contrasts(data$group) <- matrix_contr
```

```{r}
m_d_3 <- lm(health ~ group, data = data)

tidy_m_d_3 <-broom::tidy(m_d_3) |> 
  mutate(term = str_replace(term, "group", "")) 

tidy_m_d_3
```

----

<span style="color: red;">Question: What does the 2 df F-test of $R^2$ test conceptually?</span>  

```{r}
anova(m_d_3)

broom::glance(m_d_3)$r.squared
```

----

<span style="color: blue;">In this special case (with only one IV/Predictor) it provides a test of the main effect of group.</span>     

<span style="color: blue;">A significant main effect means that at least 1 contrast in one set of all possible sets of orthogonal contrasts is significant.</span>        

<span style="color: blue;">Not generally very useful. Tradition is still to report it (unless using POCs) but likely for wrong reasons. (more in this later).</span>   

----

<span style="color: red;">Question: Why does model $R^2$ not test the main effect if there are other predictors in the model?</span>

----

<span style="color: blue;">Model $R^2$ is the variance in $Y$ explained by all the regressors in the model. Only tests the main effect of an IV if only regressors for that IV are in the model.</span>  

----

## Variance Based Effect Sizes

```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 3) 
col_names <- str_c(colnames(matrix_contr), "_v_healthy")
colnames(matrix_contr) <- col_names

contrasts(data$group) <- matrix_contr
```


$\eta_p^2$ for group
```{r}
sse_c <- sum(residuals(lm(health ~ 1, data = data))^2)
sse_a <- sum(residuals(m_d_2)^2)

(sse_c - sse_a)/sse_c
```

----

Quarto handles regressors internally once you set contrasts for the factor. It is generally not necessary to add the regressors to the dataframe except for handful of special circumstances (e.g., some figures, variance effect sizes for contrasts).   

```{r}
tibble_reg <- model.matrix(~ group, data = data) |> 
  as_tibble() |> 
  select(-1, r_1 = 2, r_2 = 3)

data <- data |> 
  select(-c(ends_with("_d"))) |> 
  bind_cols(tibble_reg)
```

```{r}
slice_sample(data, n = 6)
```

----

```{r}
m_r <- lm(health ~ r_1 + r_2, data = data)

broom::tidy(m_r)
```

$\eta_p^2$ for alcohol v. healthy (r_1)
```{r}
sse_c <- sum(residuals(lm(health ~ r_2, data = data))^2)
sse_a <- sum(residuals(m_r)^2)

(sse_c - sse_a)/sse_c
```

$\eta_p^2$ for depress v. healthy (r_2)
```{r}
sse_c <- sum(residuals(lm(health ~ r_1, data = data))^2)
sse_a <- sum(residuals(m_r)^2)

(sse_c - sse_a)/sse_c
```

----

## Planned Orthogonal Contrasts


The second coding option for regressors for categorical IVs yields planned orthogonal contrasts (POCs).   

Planned orthogonal contrasts allow you to:    

1. Parse IV variance into its components (more on this later).
2. Test for contrasts other than pairwise.


----

<span style="color: red;">Question: You predict that psychopathology will reduce overall health. What contrast do you test? What is a second contrast that is orthogonal?</span>

----

<span style="color: blue;">For conceptual understanding:</span>     

```{r}
#| echo: false

c_a <- tibble(` ` = c("c_1a", "c_2a"),
              alcohol = c(0.5, 1),
              depress = c(0.5, -1),
              healthy = c(-1, 0))

c_a
```

<span style="color: blue;">John's preference (unit weighted):</span>       


```{r}
#| echo: false

c_b <- tibble(` ` = c("c_1b", "c_2b"),
              alcohol = c(0.33, 0.5),
              depress = c(0.33, -0.5),
              healthy = c(-0.67, 0))

c_b
```

----

<span style="color: red;">Question: Both of these options represent two conceptual comparisons. What are these two comparisons?</span>

```{r}
#| echo: false

c_a |> 
  bind_rows(c_b)
```

----

<span style="color: blue;">`c_1` compares a combined group of patients with alcohol use disorder and depression to healthy controls.</span>     
 
<span style="color: blue;">`c_2` compares patients with alcohol use disorder to patients with depression.</span>  

----

<span style="color: red;">Question: How will the analyses in Quarto be similar and different?</span>

----

<span style="color: blue;">The parameter estimates for `c_1` and `c_2` will be different across the two models. However, the statistical test of the null for each parameter will be identical (t/F and p-value). This should not be surprising given that the contrasts in each `c_1` option are simply a linear transformation of the other. It is simply a change of scale similar to multiplying a quantitative variable by some constant.</span>     
 
<span style="color: blue;">The model $R^2$ and the test of the null will be identical.</span>  

----

```{r}
data <- data |> 
  mutate(c_1a = case_match(group,
                           "alcohol" ~ 0.5,
                           "depress" ~ 0.5,
                           "healthy" ~ -1),
         c_2a = case_match(group,
                           "alcohol" ~ 1,
                           "depress" ~ -1,
                           "healthy" ~ 0))
```


```{r}
m_c_a <- lm(health ~ c_1a + c_2a, data = data)
broom::tidy(m_c_a)
```

----


```{r}
data <- data |> 
  mutate(c_1b = case_match(group,
                           "alcohol" ~ 0.333,
                           "depress" ~ 0.333,
                           "healthy" ~ -.667),
         c_2b = case_match(group,
                           "alcohol" ~ .5,
                           "depress" ~ -.5,
                           "healthy" ~ 0))
```


```{r}
m_c_b <- lm(health ~ c_1b + c_2b, data = data)
broom::tidy(m_c_b)
```

```{r}
#| echo: false

data <- data |>
  select(-c(r_1,r_2, starts_with("c_")))
```


----

<span style="color: red;">Question: Why are the parameter estimates for `c_1` different and which is preferred?</span>

```{r}
#| echo: false

c_a |> 
  slice(1) |> 
  bind_rows(c_b |> 
              slice(1))
```


----

<span style="color: blue;">In each instance $b_1$ represents change in health for a 1 unit change on the `c_1` regressor. For `c_1b` a 1 unit change moves fully from the alcohol/depress group to the healthy group. $b_1$ was 2/3 the size for `c_1a` because 1 unit only moves 2/3 of the way between the two sets in the contrast.</span>     
 
<span style="color: blue;">Therefore, unit weights allow you to report $b_1$ and have it represent the difference in the means of the two groups/sets.</span>


----

```{r}
#| echo: false

c_b
```


```{r}
broom::tidy(m_c_b)
```

```{r}
data |> 
  group_by(group) |> 
  summarise(mean = mean(health))
```

You can do the math to confirm that $b_1$ and $b_2$ match the mean differences associated with the `c_1` and `c_2` comparisons.  

----

## How to Calculate Unit Weights

We can write any comparison in terms of coefficients multiplied by group means for all groups in the following form:    

$C = \text{set U} - \text{set V}$   

$C = (c_1*\text{group}_1 + c_2*\text{group}_2...)-(c_3*\text{group}_3+c_4*\text{group}_4)$   

- You can think of this as a contrast between two sets of the groups.   
- Call them set U (with u groups included) and set V (with v groups included).
- A third set can be ignored by assigning coefficients of 0.

----


$C = \text{set U} - \text{set V}$   

$C = (c_1*\text{group}_1 + c_2*\text{group}_2...)-(c_3*\text{group}_3+c_4*\text{group}_4)$   

The coefficients in set U should all be: $\frac{v}{(u + v)}$.   

The coefficients in set V should all be: $\frac{u}{(u + v)}$.   

The coefficients in the ignored set should all be 0.

----

<span style="color: red;">Describe `c_1` (patient groups vs. healthy controls) and `c_2` (patients with alcohol use disorder vs. depression in terms of Set U and Set V.</span>    

----


<span style="color: blue;">$C_1 = (c_1*\text{alcohol} + c_2*\text{depress})-(c_3*\text{healthy})$</span>   

Set U: `alcohol`, `depress`; Set V: `healthy`    


<span style="color: blue;">$C_2 = (c_1*\text{alcohol})-(c_3*\text{depress})$</span>   

Set U: `alcohol`; Set V: `depress`; Ignored: `healthy`

----


<span style="color: red;">Convert these to coefficients using $\frac{v}{(u + v)}$ & $\frac{u}{(u + v)}$.</span>   

----

```{r}
#| echo: false

tibble(` ` = c("c_1", "c_2"),
       alcohol = c("1/3", "1/2"),
       depress = c("1/3", "-1/2"),
       healthy = c("-2/3", "0")) 
```



<span style="color: blue;">But you will never do this! R does this for you.</span>

----

```{r}
matrix_contr <- matrix(c(.333, .333, -.667,
                         .5,  -.5, 0), 
                       ncol = 2,
                       dimnames = list(c("alcohol", "depress", "healthy"),
                                       c("patient_v_healthy", "alcohol_v_depress")))

matrix_contr

contrasts(data$group) <- matrix_contr
```


----

```{r}
m_poc <- lm(health ~ group, data = data)

tidy_m_poc <- broom::tidy(m_poc) |> 
  mutate(term = str_replace(term, "group", ""))

tidy_m_poc
```

<span style="color: red;">Question: What does each of the $b_j$s test?</span>  

----

<span style="color: blue;">Each test whether the contrast that we specified is 0 or not. More specifically, how likely would it be to get our sample $b_j$ if $\beta_j = 0$ for each of these two contrasts.</span>

----

<span style="color: red;">Question: What does the 2 df F-test of $R^2$ test conceptually?</span>  


----

<span style="color: blue;">It provides a test of the main effect of `group`; same as before with dummy coded regressors. Any unique coding system using 2 regressors for three groups will work for the main effect.</span>    

<span style="color: blue;">As before, you don’t want to use model $R^2$ for the main effect in any model other than one way ANOVA. Use `anova()` function for tests of main effects.</span>

```{r}
anova(m_poc)

broom::glance(m_poc)$r.squared
```

----

## Display: Group Means + SEM

```{r}
data |> 
  group_by(group) |> 
  summarise(mean = mean(health),
            se = sd(health)/sqrt(n())) |> 
  ggplot(aes(x = group, y = mean)) +
  geom_col(color = "black", fill = "light grey") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) +
  labs(y = "Health", x = NULL)
```


**<span style="color: red;">NOT RECOMMENDED!</span>**

----

## Dummy Coding Group Mean Estimates


<span style="color: red;">Question: How can we get standard errors for the group mean point estimates using the linear model parameters?</span>  

----

<span style="color: blue;">Fit three separate models. Set a different group as the reference group in each model. $b_0$ will be the predicted mean for the reference group in each model. The standard error for $b_0$ is the standard error for this parameter.</span>

----

```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 1) 
col_names <- str_c(colnames(matrix_contr), "_v_healthy")
colnames(matrix_contr) <- col_names
contrasts(data$group) <- matrix_contr

m_alc <- lm(health ~ group, data = data)

broom::tidy(m_alc) |> 
  mutate(term = str_replace(term, "group", ""))
```

```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 2) 
colnames(matrix_contr) <- col_names
contrasts(data$group) <- matrix_contr

m_dep <- lm(health ~ group, data = data)

broom::tidy(m_dep) |> 
  mutate(term = str_replace(term, "group", ""))
```

```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 3) 
col_names <- str_c(colnames(matrix_contr), "_v_healthy")
contrasts(data$group) <- matrix_contr

m_health <- lm(health ~ group, data = data)

broom::tidy(m_health) |> 
  mutate(term = str_replace(term, "group", ""))
```


----

<span style="color: blue;">This standard error is the standard error you should graph! But there is an easier way to get these.</span>    


```{r}
data_new <- tibble(group = levels(data$group))

data_new
```

```{r}
preds <- predict(m_poc, newdata = data_new, se.fit = TRUE) |> 
  as_tibble() |> 
  mutate(upper = fit + se.fit,
         lower = fit - se.fit) |> 
  bind_cols(data_new)

preds
```

----

## Display: Point Estimates + SE

```{r}
preds |> 
  ggplot(aes(x = group, y = fit)) +
  geom_col(color = "black", fill = "light grey") +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
  labs(y = "Health", x = NULL)
```

----

## Parsing Variance: Dummy Codes

`alcohol` dummy codes
```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 1) 
contrasts(data$group) <- matrix_contr

data_alc <- model.matrix(~ group, data = data) |> 
  as_tibble() |> 
  select(-1, r_1 = 2, r_2 = 3) |> 
  bind_cols(data)
```

Model $R^2$
```{r}
m_alc <- lm(health ~ r_1 + r_2, data = data_alc)
broom::glance(m_alc)$r.squared
```

$\sum \Delta R^2$
```{r}
(delta_r1 <- (broom::glance(lm(health ~ r_1, data = data_alc))$r.squared) - 
  (broom::glance(lm(health ~ 1, data = data_alc))$r.squared))

(delta_r2 <- (broom::glance(m_alc)$r.squared) - 
  (broom::glance(lm(health ~ r_1, data = data_alc))$r.squared))

delta_r1 + delta_r2  
```


----

`depress` dummy codes
```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 2) 
colnames(matrix_contr) <- col_names
contrasts(data$group) <- matrix_contr

data_dep <- model.matrix(~ group, data = data) |> 
  as_tibble() |> 
  select(-1, r_1 = 2, r_2 = 3) |> 
  bind_cols(data)
```

Model $R^2$
```{r}
m_dep <- lm(health ~ r_1 + r_2, data = data_dep)
broom::glance(m_dep)$r.squared
```

$\sum \Delta R^2$
```{r}
(delta_r1 <- (broom::glance(lm(health ~ r_1, data = data_dep))$r.squared) - 
  (broom::glance(lm(health ~ 1, data = data_dep))$r.squared))

(delta_r2 <- (broom::glance(m_dep)$r.squared) - 
  (broom::glance(lm(health ~ r_1, data = data_dep))$r.squared))

delta_r1 + delta_r2  
```

----

`healthy` dummy codes
```{r}
matrix_contr <- contr.treatment(levels(data$group), base = 3) 
colnames(matrix_contr) <- col_names
contrasts(data$group) <- matrix_contr

data_health <- model.matrix(~ group, data = data) |> 
  as_tibble() |> 
  select(-1, r_1 = 2, r_2 = 3) |> 
  bind_cols(data)
```

Model $R^2$
```{r}
m_health <- lm(health ~ r_1 + r_2, data = data_health)
broom::glance(m_health)$r.squared
```

$\sum \Delta R^2$
```{r}
(delta_r1 <- (broom::glance(lm(health ~ r_1, data = data_health))$r.squared) - 
  (broom::glance(lm(health ~ 1, data = data_health))$r.squared))

(delta_r2 <- (broom::glance(m_health)$r.squared) - 
  (broom::glance(lm(health ~ r_1, data = data_health))$r.squared))

delta_r1 + delta_r2  
```

----


## Parsing Variance: POCs

```{r}
matrix_contr <- matrix(c(.333, .333, -.667,
                         .5,  -.5, 0), 
                       ncol = 2,
                       dimnames = list(levels(data$group),
                                       c("patient_v_healthy", "alcohol_v_depress")))

contrasts(data$group) <- matrix_contr

data_poc <- model.matrix(~ group, data = data) |> 
  as_tibble() |> 
  select(-1, poc_1 = 2, poc_2 = 3) |> 
  bind_cols(data)
```

Model $R^2$
```{r}
m_poc<- lm(health ~ poc_1 + poc_2, data = data_poc)
broom::glance(m_poc)$r.squared
```

$\sum \Delta R^2$
```{r}
(delta_poc_1 <- (broom::glance(lm(health ~ poc_1, data = data_poc))$r.squared) - 
  (broom::glance(lm(health ~ 1, data = data_poc))$r.squared))

(delta_poc_2 <- (broom::glance(m_poc)$r.squared) - 
  (broom::glance(lm(health ~ poc_1, data = data_poc))$r.squared))

delta_poc_1 + delta_poc_2  
```


----

## POC Contrast Coding: Common Contrasts

**3 group contrasts**   

$C_1: \frac{(\text{group}_1 + \text{group}_2)}{2} \text{vs.} \text{group}_3$    

```{r}
#| echo: false

tibble(group_1 = .333,
       group_2 = .333,
       group_3 = -.667)
```


$C_2: \text{group}_1 \text{vs.}  \text{group}_2$
```{r}
#| echo: false

tibble(group_1 = .5,
       group_2 = -.5,
       group_3 = 0)
```

----

**4 group contrasts**   

$C_1: \frac{(\text{group}_1 + \text{group}_2 + \text{group}_3)}{3} \text{vs.} \text{group}_4$    

```{r}
#| echo: false

tibble(group_1 = .25,
       group_2 = .25,
       group_3 = .25,
       group_4 = -.75)
```

$C_2: \frac{(\text{group}_1 + \text{group}_2)}{2} \text{vs.} \text{group}_3$     
```{r}
#| echo: false

tibble(group_1 = .333,
       group_2 = .333,
       group_3 = -.667,
       group_4 = 0)
```

$C_3: \text{group}_1 \text{vs.}  \text{group}_2$

```{r}
#| echo: false

tibble(group_1 = .5,
       group_2 = -.5,
       group_3 = 0,
       group_4 = 0)
```

----

**4 group contrasts**   

$C_1: \frac{(\text{group}_1 + \text{group}_2)}{2} \text{vs.} + \frac{(\text{group}_3 + \text{group}_4)}{2}$    

```{r}
#| echo: false

tibble(group_1 = .5,
       group_2 = .5,
       group_3 = -.5,
       group_4 = -.5)
```

$C_2: \text{group}_1 \text{vs.}  \text{group}_2$

```{r}
#| echo: false

tibble(group_1 = .5,
       group_2 = -.5,
       group_3 = 0,
       group_4 = 0)
```

$C_3: \text{group}_3 \text{vs.}  \text{group}_4$

```{r}
#| echo: false

tibble(group_1 = 0,
       group_2 = 0,
       group_3 = .5,
       group_4 = -.5)
```

----

**4 group contrasts (polynomials)**   

Linear:  

```{r}
#| echo: false

tibble(group_1 = -3,
       group_2 = -1,
       group_3 = 1,
       group_4 = 3)
```

Quadratic:  

```{r}
#| echo: false

tibble(group_1 = 1,
       group_2 = -1,
       group_3 = -1,
       group_4 = 1)
```


Cubic:  

```{r}
#| echo: false

tibble(group_1 = -1,
       group_2 = 3,
       group_3 = -3,
       group_4 = 1)
```

**Note that range is not 1.**

----

## Single and Multiple Comparisons 


<span style="color: red;">Question: What do you gain from testing the main effect (or interaction) of a categorical variable that has more than 2 levels? What do you not gain?</span>    

----

<span style="color: blue;">You learn that some contrast across the groups/conditions in some set of orthogonal contrasts is significant. Does this really tell you something interesting?</span>    

<span style="color: blue;">You do not learn which groups are different from each other. In almost all instances our question is about differences between groups (or combinations of groups; or patterns across groups). We need contrasts.</span>    

<span style="color: blue;">The test of the main effect is not needed (with one exception) to reduce the probability of making any Type I errors. In many instances, it will increase the probability of Type II errors. This is a **big** misconception in our field.</span>

----

**Option 1: POCs**   

```{r}
tidy_m_poc
```

----

**Option 2: Planned non-orthogonal pairwise comparisons**    

```{r}
matrix_contr <- matrix(c(.5, 0, -.5,
                         0,  .5, -.5), 
                       ncol = 2,
                       dimnames = list(levels(data$group),
                                       c("alcohol_v_healthy", "depress_v_healthy")))

contrasts(data$group) <- matrix_contr

lm(health ~ group, data = data) |> 
  broom::tidy() |> 
  mutate(term = str_replace(term, "group", ""))
```

```{r}
#| echo: false

# alternate way to set contrasts, gets same results as above

# data <- data |> 
#   mutate(c_1a = case_match(group,
#                            "alcohol" ~ .5,
#                            "depress" ~ 0,
#                            "healthy" ~ -.5),
#          c_2a = case_match(group,
#                            "alcohol" ~ 0,
#                            "depress" ~ .5,
#                            "healthy" ~ -.5))
# 
# 
# lm(health ~ c_1a + c_2a, data = data) |> 
#   broom::tidy() 
```


```{r}
matrix_contr <- matrix(c(-.5, .5, 0,
                         -.5,  0, .5), 
                       ncol = 2,
                       dimnames = list(levels(data$group),
                                       c("depress_v_alcohol", "healthy_v_alcohol")))

contrasts(data$group) <- matrix_contr

lm(health ~ group, data = data) |> 
  broom::tidy() |> 
  mutate(term = str_replace(term, "group", ""))
```

```{r}
#| echo: false

# alternate way to set contrasts, gets same results as above

# data <- data |>
#   mutate(c_1a = case_match(group,
#                            "alcohol" ~ -.5,
#                            "depress" ~ .5,
#                            "healthy" ~ 0),
#          c_2a = case_match(group,
#                            "alcohol" ~ -.5,
#                            "depress" ~ 0,
#                            "healthy" ~ .5))
# 
# 
# lm(health ~ c_1a + c_2a, data = data) |>
#   broom::tidy()
```

----

**Option 3: Unplanned; observed patterns of means dictate comparisons**   

```{r}
data |> 
  group_by(group) |> 
  summarise(mean = mean(health))
```

----

<span style="color: red;">Question: What validity concerns do we need to evaluate for these various options?</span>  

----

<span style="color: blue;">Statistical/conclusion validity.</span>    

1. <span style="color: blue;">Do results clearly support your theory (fundamental)?</span>
2. <span style="color: blue;">Probability of Type 1 error.</span>
3. <span style="color: blue;">Probability of Type 2 error.</span>

<span style="color: blue;">We should wonder (and be concerned about) what happens to the probability of making an error across *all* of our tests.</span>

----

**Test-wise error rate:** The probability of making a type I (or II) error for any specific statistical test. <span style="color: red;">What is it for type I and type II?</span>   

**Family-wise error rate:** The probability of making a type I (or II) error among a *family* of statistical tests reported (and performed).   

**Experiment-wise error rate:** The probability of making a type I (or II) error among all statistical tests reported (and performed).   

**Bonferroni Inequality:**   
$\text{alpha}_{\text{set}} \le \text{set size}*\text{alpha}_{\text{test}}$

----

- In Psychology, we are generally concerned with test-wise and family-wise error rates (and most discussion is about Type I errors).  

- Orthogonal effects in factorial ANOVA (e.g., two main effects and interaction in 2-way ANOVA) are typically considered to come from different families (different questions).

- POCs within a multi-df effect (main effect or interaction) are typically considered to come from different families. This makes most sense if the contrasts are testing clearly different questions.

- Non-orthogonal contrasts within multi-df effects are clearly related (in same family).

----

<span style="color: red;">Question: What do we now know about statistical validity for POC approaches?</span>  


----

<span style="color: blue;">Can only use POCs when your research questions map onto $n_{\text{levels}}-1$ orthogonal predictions.</span>    

<span style="color: blue;">Need to proceed cautiously because statistical results may support predictions but patterns of means may undermine your conclusions (pharmacological and expectancy example - draw graphs for two scenarios).</span>    

<span style="color: blue;">Must be comfortable with these contrasts testing *different* questions and therefore being in different families.</span>

----

<span style="color: red;">Question: What do we do with multiple planned, non-orthogonal contrasts, and worse still unplanned contrasts?</span>  

----

**Many** procedures exist. Kirk summarizes more but...

**<span style="color: blue;">For planned, non-orthogonal</span>**     

**Fischer LSD** is reasonable with planned, non-orthogonal contrasts for 3 level variables (family-wise alpha controlled and reasonable power).   

**Holm-Bonferonni** is most flexible but need to keep planned contrasts to a minimum (family-wise type I is strictly controlled, but power drops with increasing number of contrasts). Fisher LSD may be more powerful in some instances with 3 level variables.    


**<span style="color: blue;">For post-hoc/unplanned</span>**    

**Scheffe** can be used when you can't limit yourself a priori. Power sucks!   

----

## Fischer LSD (Protected Testing)

- First test omnibus (multi-df) effect (main effect or interaction). If non-significant, stop.    

- If omnibus effect is significant, test pairwise comparisons among groups/conditions using dummy codes (other contrasts may be used if planned). No additional “protection” is needed.   


<span style="color: red;">Question: How is family-wise alpha controlled?</span>  

----

<span style="color: blue;">Only proceed when omnibus test indicates some comparison is significant (monte carlo of null main effect).</span>

<span style="color: blue;">Problems emerge if omnibus is significant but you do multiple tests, only some of which are significant). Not a problem, generally, with 3 level variable and 3 tests (e.g. pairwise comparisons). Inadequate with more levels (monte-carlo for three group with effect; thought experiment with 1 significant group difference in 4 level).</span>   


----


## Holm-Bonferroni Method

1. Suppose there are $k$ null hypotheses to be tested and the overall type I error rate is $\alpha$. 

2. Order the tests from smallest to largest p-value.

3. Multiply the smallest p-value by $k$ and compare to $\alpha$. If reject, continue. 

4. Multiply the next smallest p-value by $k-1$. Its new p-value is the max of this p-value and all preceding it.

5. Continue doing this until a hypothesis cannot be rejected. At that point, stop and accept all remaining hypotheses.

Note: Does **not** require significant omnibus test for type I protection.

Holm, S. (1979). A simple sequentially rejective multiple test procedure. *Scandinavian Journal of Statistics*, 6 (2): 65–70.


----

```{r}
p = c(.001, .009, .005, .100, .200, .040, .011)
p
```

```{r}
p.adjust(p, method= 'bonferroni')
```

```{r}
p.adjust(p, method= 'holm')
```

Monte carlo example

----

## Scheffe Test

- Can be used for complete exploratory comparisons.

- Adjust the F-statistic for each comparison by dividing it by (number of groups - 1).

- Get new p-value using: `pf(F, df1, df2, lower.tail = FALSE)`.

- If you are working with t values, square them to convert to F before dividing.

- Provides good alpha protection. Test of omnibus effect is **not** required. Very conservative with high Type II errors.

----

## Monte Carlo Examples

1. Generic use of POCs.

2. All pairwise contrasts.
    
    - No effect, no  protection
    - No effect, Fisher 3 LSD
    - No effect, Holm-Bonferroni
    
<span style="color: red;">Question: What would family-wise error rate be if we conducted only 2 dummy contrasts?</span>  

----


<span style="color: blue;">$\le .10$, but they would definitely be considered in the same family (related questions) and most people won't tolerate this.</span>







