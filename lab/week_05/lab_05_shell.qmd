---
title: "Lab 5 Shell" 
author: "TAs" 
date: "`r lubridate::today()`"
format: 
  html:
    toc: true 
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

## Housekeeping

Thank you all for your valuable feedback/comments on hw3:D! Based on your answers, we have decided to make following changes to our workflow in future lab sections:

-   At the beginning of each lab, we'll take a few minutes to answer questions about lecture content

-   We'll now include some simple content-related exercises during lab (in addition to coding!)

-   Some of you suggested releasing the TA script before lab. After some serious discussions between John and the TAs, we were hesitant to do so. We still think it's best practice to write out code by hand and to debug when errors occur. We do understand your concerns about falling behind during lab when some lines of codes don't work. So alternatively, we have decided to implement minor checkpoints throughout each lab where we will stop and make sure everyone is caught up. Of course, we can help you debug outside of these points, too. :\^)

## Questions about the application exam or other material?

### Conventions for decimals ("sig. Figs") when reporting statistics

b and F values:

-   Always include the digit(s) preceding the decimal
-   If the number is less than one, report 0.xxx (e.g., 0.852)
-   Otherwise, report two decimal places after the decimal (e.g., 2.85)

p values:

-   Because p values are bounded by 0 and 1, *do not* report the digit before the decimal place
-   Report 3 digits after the decimal
-   Although you can just report the most stringent level of significance which the p value falls under (e.g., p = .003 --\> p \< .005), it is good practice to report exact p values to the 3rd decimal place, for transparency of your results...
-   ...Except for p values smaller than .001, just report p \< .001!

## Concept Question Exercise

What is our measurement of the total prediction error associated with a given model? In the context of testing statistical models, what does it mean to "reduce error"?

> You answer here.

## Exercise Week!

### Exercise 1: Calculating Parameter Estimates

Abdul is running a study to see if listening to classical music before going to sleep leads to more restful sleep. He assigns one group of participants to listen to classical music before bed and another group to listen to no music before bed. He then asks participants to report how restful their sleep was each night.

Abdul runs a simple regression analysis (`sleep` \~ `group_c`) and reports the following results:\
$b_0 = 3.01, \text{SE} = .08, b_1 = 1.15, \text{SE} = .16, F(1, 197) = 53.01, p < .001, \eta_p^2 = .21.$


#### a. Note above that Abdul centered the `group` variable (-.5/no music, .5/music). What would the value of $b_0$ and $b_1$, and the SE for $b_1$ be if `group` had been dummy coded (0/no music, 1/music)?

$b_0 = $   
$b_1 = $ 
$b_1 \text{ SE} = $


#### b. What would the value of $b_0$ and $b_1$, and the SE for $b_1$ be if `group` had been coded as 1 (no music) and 0 (music)?

$b_0 = $
$b_1 = $ 
$b_1 \text{ SE} = $

#### c. What would the value of $b_0$ and $b_1$, and the SE for $b_0$ and $b_1$ be if `group` had been coded as -1 (no music) and 1 (music)?

$b_0 = $   
$b_0 \text{ SE} = $  
$b_1 = $  
$b_1 \text{ SE} = $

Note that the df numerator and denominator do not change and so $F$ and $p$ do not change. $\eta_p^2$ does not change either. The fact that $F$ and $t$ do not change illustrates why the new SE for $b_1$ is half of the old SE. Remember that $t = b_1/\text{SE}_{b_1}$. If the new $b_1$ is half of the old $b_1$, then the new SE for $b_1$ needs to be half of the old SE for the $t$ to remain the same.

#### d. Using the original values provided above, calculate the $t$-value for $b_1$.

t =

#### e. What was the sample size in this study?

$n = $

`CHECKPOINT`


### Exercise 2: Model Predictions

For this exercise, we are going to be using a dataset called `Prestige` that is part of the `car` package. To use these data, we do not need load the library, but you will need to have the package installed in Quarto. Note that because of this, we won't be assigning our `path_data` object here. 

```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(GGally, include.only = "ggpairs")
library(psych, include.only = "corr.test")

theme_set(theme_classic()) 
```

**How do we get the documentation for the Prestige dataset?**
```{r}
#| eval: false


```

**How do we load data that are included in an R package?** 
```{r}


```

#### a. The variables `education` and `income` were calculated based on averages from the year 1971 in Canada. The average income in Canada in 1970 was estimated at \$5,000 exactly. Conduct a test to determine if the average income in 1971 is significantly different from the average income in 1970.

Calculate sum of squared errors for our compact model using average income in 1970.

```{r}
d <- d |> 
  mutate(squ_err_c= )

(sse_c <- d |> 
  summarise(sse_c = ) |> 
  pull(sse_c))
```

Calculate sum of squared errors for our augmented model using average income in 1971.

```{r}
d <- d |> 
  mutate(squ_err_a = )

(sse_a <- d |> 
  summarise(sse_a = ) |> 
  pull(sse_a))
```

Let's now practice defining a function to calculate the sum of squared errors.

```{r}


(sse_a <- sse(lm(income ~ 1, data = d))) 
```

Define `n`, `n_param_a`, and `n_param_c` and manually calculate the F-statistic and p-value.

```{r}
n <- 
n_param_a <- 
n_param_c <- 

(f_stat <- )

df_n <- 
df_d <- 

pf()
```

**Was the average income in 1971 significantly different from the average income in 1970?**

*You want to know if there is a relationship between `income` and job `type` (blue versus white collar). Specifically, you hypothesize that blue collar jobs will have a lower income than white collar jobs. However, you suspect that the prestige of an occupation also has a relationship with income and should be controlled for.*

#### b. Recode `type` and mean-center `prestige` to prepare our dataset for analysis.

Recode `type` so that `bc` is 0 and `wc` is 1. Call this variable `type_bc`.

```{r}
d <- d |> 
  mutate(type_bc = ) 

```

Recode `type` so that `bc` is 1 and `wc` is 0. Call this variable `type_wc`.

```{r}
d <- d |> 
  mutate(type_wc = )

```

Recode `type` so that `bc` is -.5 and `wc` is .5. Call this variable `type_c`.

```{r}
d <- d |> 
  mutate(type_c = )

```

Mean center `prestige`. Call this variable `prestige_c`.

```{r}
d <- d |> 
  mutate(prestige_c = )
```

`CHECKPOINT`

#### c. Fit a linear model in which `income` is regressed on `type_bc`. What do the estimates for $b_0$ and $b_1$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}
m <- lm() 
broom::tidy(m)
```

$b_0$ = predicted income for an individual who works a blue collar job. A person working a blue collar job makes about \$595 a month \[a meaningless value\].

$b_1$ = the predicted income difference between blue- and white-collar workers. There is a significant difference in income between blue- and white-collar workers. People working blue collar jobs make $1,225 more than those working white collar jobs (as we move from 0, blue collar jobs, to 1, white collar jobs, we are seeing a decrease of 1,225 units). 


##### Fit a linear model in which income is regressed on `type_wc`. What do the estimates for $b_0$ and $b_1$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}
m_2 <- lm() 
broom::tidy(m_2)
```

$b_0$ = the predicted income for an individual who works a white-collar job. A person working a white-collar job loses about \$630 month \[meaningless\].

$b_1$ = The difference in income between blue- and white-collar jobs (`type_wc`). There is a significant relationship between job type and income. People working white-collar jobs make \$1,225 less than those working blue-collar jobs (as we move from 0, white collar jobs, to 1, blue collar jobs, we are seeing an increase of 1,225 units).


##### Fit a linear model in which income is regressed on `type_c`. What do the estimates for $b_0$ and $b_1$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}
m_3 <- lm() 
broom::tidy(m_3)
```

$b_0$ = the predicted income for an individual who works a theoretical half-white, half-blue collar job with an average level of prestige. A person working a half-white, half-blue-collar job with average prestige has a predicted income of \$5,072 a month.

$b_1$ = the difference in income between blue- and white-collar jobs (`type_c`). There is a significant relationship between job type and income. People working blue-collar jobs make $1,225 more than those working white-collar jobs (when we move from blue collar jobs, -0.5, to white collar jobs, 0.5, we see a decrease of 1,225 units).


`CHECKPOINT`

#### g. If you wanted $b_0$ to reflect the predicted average income for a white-collar job that has an average level of prestige, what model would you fit? Run that model. Using code, calculate and interpret $\eta_p^2$ and $\Delta R^2$ for each parameter. Which parameter estimate(s) change across all four of these models?

1)  Define a function called "pre()" that takes the value of "compact" and "augmented" to calculates the pre.

2)  Define a function called "delta_r2" that takes the value of "mean", "compact", "augmented" to calculate delta r2.

```{r}
pre <- function(compact, augmented) {

}

delta_r2 <- function(mean, compact, augmented) {

}
```

```{r}
m_4 <- lm() 
broom::tidy(m_4)
```

$\eta_p^2$ and $\Delta R^2$ for `type_wc`

```{r}
pre(compact = , 
    augmented = )

delta_r2(mean = , 
          compact = , 
          augmented = )
```

Job type explains 13 percent of the variance in income after removing the variance explained by prestige ($\eta_p^2$ = 0.13).

Job type uniquely explains 8 percent of the total variance in income ($\Delta R^2$ = .08).

$\eta_p^2$ and $\Delta R^2$ for `prestige_c`

```{r}
pre(compact = ,
    augmented = )

delta_r2(mean = , 
          compact = , 
          augmented = )
```

Prestige explains 45 percent of the variance in income after removing the variance explained by job type ($\eta_p^2$ = 0.45).

Prestige uniquely explains 44 percent of the total variance in income ($\Delta R^2$ = 0.44).

$b_0$. $b_1$ becomes negative or positive depending on which level of type is assigned as the lower number, but the absolute value of the slope does not change otherwise.

`CHECKPOINT`

### Exercise 3: Graphing

There is a common belief that more crime occurs in hotter locations. Based on this belief, we might predict that southern states have greater crime rates than northern states. We will be using the free Crime dataframe from MASH to test this prediction.


#### a. Import `lab_05_graphing.csv`, clean the variable names, and `glimpse()` the dataframe.

```{r}
d <- read_csv(here::here(path_data, "lab_05_graphing.csv"),
              show_col_types = FALSE) |> 
  janitor::clean_names() |>
  glimpse()
```


#### b. First, make it clear what each of level of `southern` represents. `southern` is coded such that 0 = Northern and 1 = Southern. Use `mutate()` to reflect that in a new factor variable called `southern_fac`.

```{r}
d <- d |> 
  mutate(southern_fac = factor(southern, 
                                levels = c(0, 1),
                                labels = c("northern", "southern")))
```


#### c. Center the numeric `southern` variable and get descriptives on this variable. Is the mean of the centered variable 0? Why or why not?

```{r}
d <- d |> 
  mutate(southern_c = )

d |> 
  skim(southern_c)
```

We see that the mean of this centered variable is not 0. This is because we have an unequal N across groups.

```{r}
janitor::tabyl(d$southern_fac)
```

#### d. Create a linear model in which `crime_rate` is regressed on `southern_c`. Get a summary of the model. What does the summary tell us?

```{r}
m <- lm()
broom::tidy(m)
```

The summary tells us if there is a relationship between the number of crime offenses per million people (`crime_rate`) and whether or not a state is Southern (`southern`). There is no significant difference in crime rate between northern and southern states, b1 = -3.23, t(46) = -.36, p = .72.

#### e. Create a quick and dirty plot of the relationship between `crime_rate` and `southern`.

```{r}
plot(d$southern, d$crime_rate) + 
  abline(lm(d$crime_rate ~ d$southern))
```

#### f. In just three steps, set up a new dataframe called `d_plot`. This dataframe should contain predicted values for the outcome variable, SE, lower and upper confidence intervals, centered numeric values for northern and southern states, and character values that can serve as labels for northern and southern states.

```{r}
d_new <- tibble(southern_c = , 
                   southern_fac = )

preds <- predict() |>
  as_tibble() |> 
  

d_plot <- d_new |> 
  bind_cols(preds)
```

#### g. Build a bar graph in ggplot using the dataframe `d_plot`.

Your plot should include:

-   bars representing the dichotomous `southern` variable.
-   error bars.
-   raw data points.
-   publication quality labels
-   no legend

```{r}
d_plot |> 
  ggplot(aes()) + 
  geom_bar() +
  geom_errorbar() +
  geom_point() +
  labs(title = "Crime Rates in the North and South",
       x = "Region", 
       y = "Number of Criminal Offences per Million Citizens") +
  theme()
```
