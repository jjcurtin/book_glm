---
title: "Lab 11 TA Script" 
author: "TAs" 
date: "`r lubridate::today()`"
format: 
  html:
    toc: true 
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

## Conceptual Exercise

Researchers recently regressed happiness on participants' self-confidence as the sole predictor. After checking their model assumptions, the researchers decide that happiness needed to be transformed down the ladder. Thus, they applied a log2 transformation to happiness and got the following estimate for b1: .084.    

The research team knows that saying "every additional unit of self-confidence increases log2(happiness) by .084" is not very informative.   

**What is the interpretation of b1 in raw units of happiness?**  

```{r}

```


## Introduction to Interactions

**Note that we will jump into continuous x dichotomous interaction effects right away, but do keep in mind that everything you learn today readily applies to continuous x continuous interactions. **

```{r}
#| message: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(broom)
library(skimr)

theme_set(theme_classic()) 

path_data <- "lab/data"
```


Imagine a hypothetical study by a graduate student, Andrew. In this study, Andrew examined whether preschoolers benefit from collaborative learning (vs. passive learning) when they are learning about the external world.   

Often, teachers believe that working actively in a group to learn is better than learning by passively observing. However, Andrew believes that thereâ€™s a developmental difference in preschoolers ability to learn from collaboration, such that only older preschoolers would benefit from and learn better in collaboration, whereas this benefit would be considerably smaller for younger preschoolers.   

To test this hypothesis, Andrew had preschoolers learn about the physical phenomenon that a sponge soaks up water with an adult experimenter. In the Observation condition, the preschoolers watched the adult experimenter demonstrate using a sponge to soak up water. In the Collaboration condition, the child and adult played with a sponge and water together to learn about this phenomenon.   

The preschoolers' learning was assessed 5 times within a 2 hour-block after the learning period. The experimenter would "accidentally" pour water on the desk and ask the preschooler if they can help. A sponge was placed on the desk along with other toys. The percentage of times that the preschoolers used the sponge to soak up water was recorded as the indicator of the preschooler's learning.   

code book:   

- `age` (continuous)
- `condition` (either Observe [0] or Collaborate [1])
- `learning` (sponge uses in percentage)
- `prosociality` (don't worry about this one for now)   

Read in `lab_11_data.csv`.   

```{r}
d <- read_csv(here::here(path_data, "lab_11_data.csv"),
              show_col_types = FALSE) |> 
  glimpse()
```

**What type of interaction are we interested in?** 



### Preparations

Descriptive statistics
```{r}
skim(d) |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```


**What should we do first before testing interactions? Why?**    




```{r}



```


### Additive Model

First, let's fit a model without the interaction term for pedagogical purposes. Use centered predictors.

```{r}
m_1 <- lm(learning ~ condition_c + age_c, data = d)
tidy(m_1)
```

Interpret the coefficients:    





### Model with an Interaction Term

We will now add an interaction term $b3$ to the model: 
$y = b0 + b1*condition_c + b2*age_c + b3*condition_c*age_c$    

By rewriting the math, the model above is essentially the same as: 
$y = b0 + b2*age_c + (b1 + b3*age_c)*condition_c$   
such that the effect of condition is now estimated as a linear model in itself, where the regression coefficient of condition is intercept + some function of age. 

The effect of condition is therefore free to vary across levels of participant age, or in other words, age now moderates the effect of condition.

Also note that the two models above are both statistically equivalent to:      
$y = b0 + b1*condition_c + (b2 + b3*condition_c)*age_c$  
where condition moderates the effect of age.

Therefore, although one may theorize that it is age moderating the effect of condition, the possibility that condition moderates the effect of age is represented by the same interaction term equally well. The direction of causality is not distinguishable by just looking at our statistical model.


To test the interaction effect, we need to create a product term of age*condition.

**Method 1:**   
"Dumb" method that works for mostly all software or calculation by hand: Manually take the product of `condition_c` and `age_c` and assign it to a new variable. Then fit the model including `condition_c`, `age_c` and the interaction term.
```{r}
d <- d |> 
  mutate

m_int_1 <- 
tidy(m_int_1)
```

**Method 2:**   
Easier method in R (same result).
```{r}
m_int_2 <- 
tidy(m_int_2)
```

**Method 3:**   
Much easier method in R (same result).
```{r}
m_int <- 
tidy(m_int)
```

**Why is the following model NOT the correct model to test our question?**   
```{r}
m_wrong <- lm(learning ~ condition_c:age_c, data = d)
tidy(m_wrong)
```




**What has changed from the additive model?**
```{r}
tidy(m_int)
tidy(m_1)
```

To test the interaction effect, `m_int` is compared to the additive model `m_1`.
```{r}
anova(m_1, m_int)
```

This is the same as conducting a t-test for the interaction term.
```{r}
tidy(m_int)
```


**Interpret each coefficient in the interaction model.**    

1. Intercept: b0 = 0.386  



2. condition_c: b = 0.113     



3. age_c: b = 0.013    



4. interaction: b = 0.025   

It is useful to think back to the rewrite of our mathematical models:

$y = b0 + b2*age_c + (b1 + b3*age_c)*condition_c$   
$y = b0 + b1*condition_c + (b2 + b3*condition_c)*age_c$  

Effect of condition_c = $b1 + b3*age_c$
Effect of age_c = $b2 + b3*condition_c$




OR   




**Calculate the effect size of the interaction effect.**    

```{r}
pre <- function(compact, augmented) {
  sse_c <- sum(residuals(compact)^2)
  sse_a <- sum(residuals(augmented)^2)
  
  (sse_c - sse_a) / sse_c
}
```

```{r}
pre(lm(learning ~ condition_c + age_c, data = d), m_int)
```


## Hand-calculating predictions and simple effects


$y = 0.386 + 0.013*age_c + (0.113 + 0.025*age_c)*condition_c$   
$y = 0.386 + 0.113*condition_c + (0.013 + 0.025*condition_c)*age_c$   
$y = 0.386 + 0.113*condition_c + 0.013*age_c + 0.025*condition_c*age_c$    


**What is the effect (regression coefficient) of age in the Observation condition?**    
For the Observation condition, condition_c = -0.5     
beta_age = (0.013 + 0.025*condition_c) = 
```{r}

```

**What does this number tell us?**   



**What is the effect (regression coefficient) of age in the Collaborate condition?**   
For the Observation condition, condition_c = 0.5     
beta_age = (0.013 + 0.025*condition_c) =
```{r}

```

**What does this number tell us?**   



**What is the effect (regression coefficient) of condition among 40-mos-old preschoolers?**   
For 40-mos-old preschoolers, age_c = 
```{r}

```

beta condition = (0.113 + 0.025*age_c) = 
```{r}

```


**Is the effect of condition significant among older preschoolers (60-mos-old)?**     
How do we test that??    
Recentering!!    
```{r}
d <- d |> 
  mutate

m_int_60 <- lm(learning ~ condition_c * age_60, data = d)
tidy(m_int_60)
```




### Graphing a continuous x dichotomous interaction

A graph is more informative with uncentered predictors, as we are not concerned with the interpretation of regression coefficients of the simple effects.   

Refit your interaction model with uncentered predictors.
```{r}
m_graph <- 
```

**How did the regression coefficients and their standard errors change if we did not center the predictors? Why?**

```{r}
tidy(m_graph)
```


Generate predictions for the graph. Note that because we are plotting interactions, we need to sample a range of possible values for both age and condition. We will NOT hold either one constant at its average this time.
```{r}
x <- expand.grid

preds <- predict(m_graph, newdata = x, se.fit = TRUE) |> 
  as_tibble() |> 
  mutate(upper = fit + se.fit, 
         lower = fit - se.fit) |> 
  bind_cols(x)
```

Recode condition to be a factor with descriptive labels for plotting, Do this in both your `preds` and raw `d` data.
```{r}
preds <- preds |> 
  mutate(condition_str = recode(condition, 
                                "0" = "Observation", 
                                "1" = "Collaboration"))
d <- d |> 
  mutate(condition_str = recode(condition, 
                                "0" = "Observation", 
                                "1" = "Collaboration"))
```

Plot the raw data and model predictions. 
```{r}
ggplot() +
  geom_jitter(
    
  ) +
  geom_smooth(
    
  ) + 
  labs(x = "Age (months)", 
       y = "Learning (percentage)", 
       main = "Collaborative learning is beneficial in older preschoolers",
       color = NULL, 
       fill = NULL) + 
  theme(legend.position = "top") + 
  scale_x_continuous(breaks = seq(min(d$age), max(d$age),by = 2),expand = c(0,0))+
  scale_y_continuous(breaks = c(0.0,0.2,0.4,0.6,0.8,1.0),
                     labels = scales::percent, 
                    expand = c(0,0)) + 
  scale_color_brewer(palette = "Set1") + 
  scale_fill_brewer(palette = "Set1") 
```

Don't like R's default colors? you can use other color sets (see [http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/figure/unnamed-chunk-14-1.png](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/figure/unnamed-chunk-14-1.png))


