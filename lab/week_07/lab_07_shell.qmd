---
title: "Lab 7 Student Shell" 
author: "TAs" 
date: "`r lubridate::today()`"
format: 
  html:
    toc: true 
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

## Review

**Best practices:**

-   Only use library() to load packages you will actually use

-   remove only those na values that would interfere with the current
    model comparison of interest

-   An object for a different purpose should have a different name. For
    example, when you mutate to add a column, your dataframe is used for
    the same purpose and can have the same name. However, if you make
    two sub-dataframes with different sets of NA values removed, they
    should have different names

-   generate predictions manually over several steps for accurate SE bar
    in graphs, rather than using geom_smooth's inbuilt method = "lm"
    argument. Although both ways work for the 1-parameter model, they
    don't work for multiple parameter models.

-   read all prompts carefully, including the study description and
    codebook! Several people missed the reverse coding needed for
    composite scores.

-   Don't print your dataframe or use view() if you are going to be
    rendering your document

-   use enter to break up long pipe calls

-   comments directly relating to or explaining a single line of code
    can go within code chunks; longer explanations and interpretations
    should be outside of code chunks

**Class norms**

-   \_d indicates dummy coded, while \_c indicates mean-centered or
    contrast-coded

-   dichotomous variables are never centered by subtracting the mean,
    they are centered by assigning -0.5 and 0.5

-   dataframe-type objects should be kept in tibble form unless
    otherwise specified

-   Error bands are +- 1 SE

-   use theme_classic() for ggplot

-   use snake_case - no capital letters, and different words should be
    seperated by a "\_"

**APA style statistics reporting**

*b~0~* = 6.53, *t*(38) = 5.04, *p* \< .001

*b~1~* = 3.49, 95% CI \[3.21, 3.77\], *F*(1, 38) = 4.23, *p* = .047,

-   report the value being tested - mean, b0, or b1 generally
-   test statistics are italicised
-   test statistics must always be reported with all df values
-   CI requires specifying the level and uses \[lower, upper\] for the
    values
-   2 decimal points for estimates and statistics, 3 decimal points and
    no 0 before the decimal point for p values
-   report rounded p values unless below .001, and in that case use \<
    .001

**What does an effect size measure tell you?**

**What does a test statistic, with associated df and p values tell
you?**

## Set up for the week!

First, let's set-up our script. We will also set up our path object
because we will need that later on! If you have created your own
functions script, this is a great place to source that in.

```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(skimr)

path_data <- "lab/data"

theme_set(theme_classic()) 
```

## PART 1: Finishing up Week 6 exercises (Model Predictions)

For this exercise, we will be working some more with the Prestige
dataset.

Now, let's load in the Prestige dataset...

```{r}
d <- carData::Prestige |>
  filter(type != "prof") |> # remember, we dropped these observations
  mutate(type = fct_drop(type)) |> # and then we made sure to drop that level of our factor
  glimpse()
```

### a. Recode `type` and mean-center `prestige` to prepare our dataset for analysis.

Recode `type` so that `bc` is 0 and `wc` is 1. Call this variable
`type_bc`.

```{r}

```

Recode `type` so that `bc` is 1 and `wc` is 0. Call this variable
`type_wc`.

```{r}

```

Recode `type` so that `bc` is -.5 and `wc` is .5. Call this variable
`type_c`.

```{r}

```

Mean center `prestige`. Call this variable `prestige_c`.

```{r}

```

### b. Fit a linear model in which `income` is regressed on `type_bc` while statistically controlling for `prestige` (uncentered). What do the estimates for $b_0$, $b_1$, and $b_2$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}

```

$b_0$:

$b_1$:

$b_2$:

#### Fit a linear model in which income is regressed on `type_wc` while statistically controlling for `prestige` (uncentered). What do the estimates for $b_0$, $b_1$, and $b_2$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}

```

$b_0$:

$b_1$:

$b_2$:

#### Fit a linear model in which income is regressed on `type_c` while statistically controlling for `prestige_c`. What do the estimates for $b_0$, $b_1$, and $b_2$ represent? Provide a conceptual interpretation for each parameter estimate.

```{r}

```

$b_0$:

$b_1$:

$b_2$:

### c. If you wanted $b_0$ to reflect the predicted average income for a white-collar job that has an average level of prestige, what model would you fit?

`CHECKPOINT`

## PART 2: Spooky three predictor models & VIF

We're going to use some spectacularly sPoOkY data this lab, it's on
Halloween consumer behavior.

Imagine a hypothetical study by a local newspaper. Earlier this year, 79
community members responded to a survey about how they have celebrated
Halloween in the past and how they plan to celebrate Halloween this
(coming) October.

The survey included many items, including the followings:

`costume`: In the past, you have often celebrated Halloween by wearing a
costume.

`decorate`: In the past, you have often celebrated Halloween by
decorating your home.

`plan_buy_costume`: For Halloween this year, it is likely that you will
buy some costumes.

`shop_early`: For Halloween this year, it is likely that you will go
shopping before September.

The participants responded using a Likert scale: -2 = strongly disagree,
-1 = mildly disagree, 0 = neither agree nor disagree, 1 = mildly agree,
and 2 = strongly agree.

## Analyses

You are a small-scale Halloween retailer. To prepare for the early
Halloween market (pre-september), you hope to gain some insights on what
the market will look like using these data.

Let's load in these data. We've already cleaned this dataset for you,
but we will come back to these data later on for an in-depth overview of
data wrangling and cleaning.

```{r}
d <- read_csv(here::here(path_data, "lab_07_data.csv"),
              show_col_types = FALSE) |> 
  glimpse()
```

Let's view correlations, distributions, and scatterplots all in one step
using `GGally::ggpairs()`.

```{r}

```

**Which 2 variables are most weakly correlated?**

**Most strongly correlated?**

### Exercise Time!

#### Regression with 1 predictor

Note: We do not encourage progressively adding predictors when analyzing
data (think about researcher degrees of freedom!). We're doing it for
pedagogical purposes in lab so you can see what adding predictors can do
to results.

You wonder if people who often celebrate by wearing a costume tend to
shop early for Halloween. Run a model to see if costume predicts
shopping before September (`shop_early`).

```{r}

```

**1a. What's the result?**

**1b. What is the interpretation of the coefficient for `costume`?**

`CHECKPOINT`

#### Regression with 2 predictors

**Why might we add another predictor(s)?**

New question: You wonder if celebrating by wearing a costume predicts
shopping before September when statistically controlling for celebrating
by decorating one's home.

i.e., Do people who often celebrate by wearing a costume tend to shop
early for Halloween, after accounting for how often they celebrate by
decorating their home.

**Run a model to see if `costume` predicts shopping before September
(`shop_early`).**

```{r}

```

**2a. How did the regression coefficient, its standard error, F
statistic, and p value associated with `costume` change?**

**2b. What's the interpretation of the coefficient for `costume`?**

**2c. What's the interpretation of the coefficient for `decorate`?**

**2d. How much of the variance in `shop_early` is uniquely explained by
each of the two predictors?**

If you didn't source in your functions elsewhere, remember to run this!

```{r}
sse <- function(model) {
  sum(residuals(model)^2)
}

delta_r2 <- function(compact, augmented) {
  glance(augmented)$r.squared - glance(compact)$r.squared
}
```

$\Delta R^2$ for `costume`

```{r}

```

$\Delta R^2$ for `decorate`

```{r}

```

**2e. How does the** $\Delta R^2$ of `costume` in `m_2` compare to the
$R^2$ of `costume` in `m_1`?

```{r}

```

**End of exercise!**

`CHECKPOINT`

### VIF

There is an increase in the SE for `costume` from `m_1` to `m_2`. The
coefficient also decreased. **Why?**

```{r}

```

We can find a partial answer by revisiting the correlation between the
variables.

```{r}

```

`costume` and `decorate` are strongly correlated (r = .866)! **Why might
that be?**\
People who tend to celebrate Halloween with costumes also tend to be
into decorating their house.

```{r}

```

In fact, `decorate` accounts for 75% of the variance in `costume`!

Because of the strong correlation between `costume` and `decorate`, it's
likely that a lot of the variance in `shop_early` explained by `costume`
overlaps with the variance explained by `decorate`.

When variables in the model are very highly correlated, this is called
*multicollinearity*. When multicollinearity occurs, it inflates standard
errors and changes the magnitude of regression coefficients.

We can directly test the multicollinearity of each predictor in our
model using the `vif()` function in the `car` package.

```{r}
car::vif(m_2)
```

vif stands for **variance inflation factor**. A conventional cutoff for
vif is 5, such that if you have a vif value \>= 5 for your focal
predictor, then multicollinearity is a problem.

Good-to-know knowledge:\
$VIF_j = 1 / (1â€“R_j^2)$\
$R_j^2$: percent variance in predictor j that is explained by all other
predictors.

**What level of** $R_j^2$ is problematic, given the conventional cutoff
for vif = 5?

### Regression with 3 Predictors

As a Halloween retailer, you want to know if people who actually plan to
purchase costumes are more likely to shop for Halloween early on (before
September), when controlling for the degree to which they celebrate
Halloween by wearing costumes and by decorating their home.

**What is the focal predictor?**

**Fit the new model.**

```{r}

```

**What is the interpretation of the intercept for `m_3`?**

**What is the interpretation of the coefficient for `plan_buy_costume`
and what does the coefficient tell us?**

**After removing the variance explained by `costume` and `decorate`, how
much of the residual variance in `shop_early` does `plan_buy_costume`
account for?**

```{r}

```

### Plotting

Create a plot showing the effect of `plan_buy_costume` on `shop_early`
when controlling for `costume` and `decorate` (i.e., `m_3`).

Since `plan_buy_costume` is the focal predictor, we will just set the
other covariates to their mean for our predictions.

```{r}

```
