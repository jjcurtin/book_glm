--- 
editor_options:  
  chunk_output_type: console
--- 
 
# Unit 10: Inferences about Two Continuous Predictors and their Interaction

```{r}
#| echo: false

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```

## Interactive Models: Two Quantitative Variables

**Example:**   

Effect of positive attitudes (1-5) about birth control and peer pressure to not use birth control (1-5) on intention to use birth control (0-30) among sexually active female adolescents.

```{r}
#| message: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(broom)
library(patchwork)
library(skimr)

theme_set(theme_classic()) 

path_data <- "data_lecture"

data <- read_csv(here::here(path_data, "10_interactions_birth_control.csv"),
                 show_col_types = FALSE) |> 
  glimpse()
```

-------------------------------------------------------------------------------

```{r}
data |> 
  select(att:bc) |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

-------------------------------------------------------------------------------

```{r}
data |> 
  select(-subid) |> 
  cor()
```

::: {.callout-important}
# Question

If you regressed BC on Att and PP in two separate linear models, what can you tell me about these two models based on the correlations above? 
:::

:::{.fragment}
[Regression coefficient for Att will be positive. Regression coefficient for PP will be negative. $R^2$ will be bigger for Att model ($R^2$ = .56) than for PP model ($R^2$ = .25).]{style="color:blue;"}
:::

-------------------------------------------------------------------------------

```{r}
m_att <- lm (bc ~ att, data = data)
m_pp <- lm (bc ~ pp, data = data)
```

```{r}
tidy(m_att)
glance(m_att)$r.squared
```

```{r}
tidy(m_pp)
glance(m_pp)$r.squared
```

-------------------------------------------------------------------------------

```{r}
data |> 
  select(-subid) |> 
  cor()
```

::: {.callout-important}
# Question

Based on the correlations, what can you tell me about the model (parameter estimates and $R^2$) including both Att and PP as regressors?
:::

:::{.fragment}
[The regression coefficients for Att and PP will match the coefficients from their respective bivariate models b/c Att and PP are fully orthogonal (uncorrelated). PP and Att each predict fully unique variance in BC.]{style="color:blue;"}   

[The $R^2$ for the additive model will be equal to the sum of the $R^2$s from the two bivariate models, again b/c Att and PP are orthogonal.]{style="color:blue;"}
:::

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

What about $\eta_p^2$ for Att in the 1 predictor model vs. the 2 predictor model? Specify the augmented and compact models to test Att for 1 and 2 predictor approaches. Specify the formula for $\eta_p^2$.
:::

:::{.fragment}
[**Test for Att in 1 predictor model:**]{style="color:blue;"}    
[$BC_c = b_0 + 0*Att$]{style="color:blue;"}    
[$BC_a = b_0 + b_1*Att$]{style="color:blue;"}    

[**Test for Att in 2 predictor model:**]{style="color:blue;"}    
[$BC_c = b_0 + 0*Att + b_2*PP$]{style="color:blue;"}    
[$BC_a = b_0 + b_1*Att + b_2*PP$]{style="color:blue;"}    

[$\eta_p^2 = \frac{SSE_c - SSE_a}{SSE_c}$]{style="color:blue;"}   

- [Numerator is same for the both 1 and 2 predictor tests of Att.]{style="color:blue;"}   
- [Denominator is smaller for 2 predictor test.]{style="color:blue;"}   
- [Therefore, $\eta_p^2$ for Att is bigger in 2 predictor model. Att produces a bigger proportional reduction in error.]{style="color:blue;"}  
- [Same would be true for PP]{style="color:blue;"}  
:::

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

What about $\Delta R^2$ for Att in the 1 predictor model vs. the 2 predictor model? Specify the augmented and compact models to test Att for 1 and 2 predictor approaches. Specify the formula for $\Delta R^2$.
:::

:::{.fragment}
[**Test for Att in 1 predictor model:**]{style="color:blue;"}    
[$BC_c = b_0 + 0*Att$]{style="color:blue;"}    
[$BC_a = b_0 + b_1*Att$]{style="color:blue;"}    

[**Test for Att in 2 predictor model:**]{style="color:blue;"}    
[$BC_c = b_0 + 0*Att + b_2*PP$]{style="color:blue;"}    
[$BC_a = b_0 + b_1*Att + b_2*PP$]{style="color:blue;"}    

[$\Delta R^2 = \frac{SSE_c - SSE_a}{SSE_{mean-only}}$]{style="color:blue;"}    

[Numerator is same for the both 1 and 2 predictor tests of Att.]{style="color:blue;"}   
[Denominator is same for both 1 and 2 predictor tests.]{style="color:blue;"}    
[Therefore, $\Delta R^2$ for Att does not change. Att explains the same proportion of **total** variance (error) in both models.]{style="color:blue;"}     
[Same would be true for PP]{style="color:blue;"}     
:::

-------------------------------------------------------------------------------

```{r}
m_add <- lm(bc ~ att + pp, data = data)
tidy(m_add)
```

$BC = 8.0 + 3.0 * Att + -2.0*PP$   

::: {.callout-important}
# Question

What has this model required (or what havent we tested for)?
:::

:::{.fragment}
[It requires that the effect of each IV on the DV is constant across all levels/scores of the other IV.]{style="color:blue;"}   

[We have not allowed or tested for an interaction.]{style="color:blue;"}
:::

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

Where are $b_{Att}, b_{PP},$ and $b_0$ in the figure below?
:::

```{r}
#| code-fold: true

preds <- expand.grid(att = 1:5, pp = c(1, 3, 5))
preds_y <- predict(m_add, newdata = preds)

plot_m_add <- ggplot() +
  geom_line(aes(x = preds$att, y = preds_y, color = factor(preds$pp)), 
            linewidth = 1) +
  labs(x = "Attitude about Birth Control",
       y = "Birth Control Use",
       color = "Peer Pressure") 

plot_m_add
```

:::{.fragment}

[$b_{Att}$ = slope of lines. Constant for all three lines.]{style="color:blue;"}    
[$b_{PP}$ = separation of lines. Lines are separated by $2*b_{PP}$. Constant across Att.]{style="color:blue;"}    
[$b_0$ = predicted value at Att = 0 and PP= 0. Not displayed in figure.]{style="color:blue;"}
:::

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

How might we benefit from including a third regressor in the model to represent the interaction between Att and PP? Hint, there are two benefits.
:::

:::{.fragment}

[1. If Att X PP effect is significant, it will increase $R^2$, decrease SEs, and therefore increase power to test all effects.]{style="color:blue;"}    

[2. If Att X PP is significant, it will provide us with a more complex, nuanced perspective on the nature of the Att and PP effects on BC.]{style="color:blue;"}    

**Definition: An interaction exists when the effect of 1 predictor on the DV differs across levels/values of the other predictor.**  
:::

-------------------------------------------------------------------------------

- Regressors for interaction terms are calculated as the product of the regressors (for the predictors) in the interaction. In this case, we simply multiply Att X PP.   

- You will typically want to **center** the $X$ in the primary model to yield tests of *main effects* of each $X$ as well as tests of the interaction (More on this as the slides develop).     

- In R, you don’t need to actually compute the product term regressor directly. `A:B` in the `lm()` formula will include the A X B interaction regressor(s).    

- `A*B` is further shorthand to include A, B, and A X B in the model.   

- You should generally **not** include A X B in a model that does not include lower order effects (e.g., A, and B).    

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

What will change in the two predictor additive model if we center both $X$s?
:::

:::{.fragment}
[1. $b_0$ and its SE will change. $b_0$ is the predicted value at 0 on all regressors in the model.]{style="color:blue;"} [Advanced question: Will SE be bigger or smaller in centered model?]{style="color:red;"}

[2. $b_{Att}$ and $b_{PP}$ (and their SEs) will remain the same. The additive model forces the effect for each $X$ to be same across all values of other $X$s. Therefore, the effect of ATT is the same if PP = 0 or PP= 1, or PP=3, etc. Given this, centering PP does not change $b$ for ATT.]{style="color:blue;"}         

[3. Of course, $R^2$ also remains the same.]{style="color:blue;"}
:::

-------------------------------------------------------------------------------

```{r}
tidy(m_add)
glance(m_add)$r.squared
```

```{r}
data <- data |> 
  mutate(att_c = att - mean(att),
         pp_c = pp - mean(pp))

m_add_c <- lm(bc ~ att_c + pp_c, data = data)
tidy(m_add_c)
glance(m_add_c)$r.squared
```

-------------------------------------------------------------------------------

::: {.callout-important}
# Question

What will change when we add the interaction term (with centered $X$s) relative to the centered two predictor additive model?
:::

:::{.fragment}

[1. An additional regressor will be included for Att x PP.]{style="color:blue;"}    

[2. If Att x PP accounts for DV variance, $R^2$ will increase and SEs for coefficients (and intercept) will be reduced.]{style="color:blue;"}    

[3. $b_0$ will remain the predicted value at 0 for all regressors. No change from centered two predictor additive model.]{style="color:blue;"}    

[4. $b_{Att}$ and $b_{PP}$ are respective effects at 0 on all other regressors. Including an interaction now allows for each $X$ effect to vary across levels/values of other $X$s.]{style="color:blue;"}    

[Thus, $b_{Att}$ is now the (*simple*) effect of Att at PP (centered) = 0 and the $b_{PP}$ is now the (*simple*) effect of PP at Att (centered) = 0.]{style="color:blue;"} 
:::

-------------------------------------------------------------------------------

## Main Effects and Simple Effects

In ANOVA terms, The **Main Effect** of an IV is the overall effect of that IV on the DV averaging across the levels of the other IV(s) in the model.      

A **Simple Effect** of an IV is the effect of that IV at a specific level of the other IV(s) in the model.     

From this perspective, you can think about a main effect as a special simple effect where the specific level of the other IV is its average value.    

-------------------------------------------------------------------------------

```{r}
m_int_c <- lm(bc ~ att_c * pp_c, data = data)

tidy(m_int_c)
glance(m_int_c)$r.squared
```

```{r}
tidy(m_add_c)
glance(m_add_c)$r.squared
```

-------------------------------------------------------------------------------

$BC = 11.0 + 3.0 * Att_c + -2.0*PP_c$    

$BC = 11.0 + 3.0 * Att_c + -2.0*PP_c + -1.0*Att_c*PP_c$     

::: {.callout-important}
# Exercise

Link intercepts and coefficients from each model to their respective figures.
:::

*Note:* figures use raw (not centered) predictors. Means for both predictors are 3.

```{r}
#| echo: false 

m_int <- lm(bc ~ att * pp, data = data)

preds_int_y <- predict(m_int, newdata = preds)

plot_m_int <- ggplot() +
  geom_line(aes(x = preds$att, y = preds_int_y, color = factor(preds$pp)), 
            linewidth = 1) +
  labs(x = "Attitude about Birth Control",
       y = "Birth Control Use",
       color = "Peer Pressure") 

plot_m_add + plot_m_int
```

------------------------------------------------------------------------------

$BC = 11.0 + 3.0 * Att_c + -2.0*PP_c + -1.0*Att_c*PP_c$     

::: {.callout-important}
# Question

Question: What would the interactive model look like if we hadn’t centered each IV?
:::

:::{.fragment}

```{r}
m_int <- lm(bc ~ att * pp, data = data) 
tidy(m_int)
glance(m_int)$r.squared
```

:::

------------------------------------------------------------------------------

$BC = 11.0 + 3.0 * Att_c + -2.0*PP_c + -1.0*Att_c*PP_c$ 

$BC = -1.0 + 6.0 * Att + 1.0*PP + -1.0*Att*PP$   

::: {.callout-important}
# Exercise

Link intercept and coefficients from the raw model to the expanded figure below on right.
:::

```{r}
#| echo: true

preds_expanded <- expand.grid(att = 0:5, pp = 0:5)
preds_int_y_expanded <- predict(m_int, newdata = preds_expanded)

plot_m_int_expanded <- ggplot() +
  geom_line(aes(x = preds_expanded$att, y = preds_int_y_expanded, 
                color = factor(preds_expanded$pp)), 
            linewidth = 1) +
  labs(x = "Attitude about Birth Control",
       y = "Birth Control Use",
       color = "Peer Pressure") 

plot_m_int + plot_m_int_expanded
```

------------------------------------------------------------------------------

$BC = 11.0 + 3.0 * Att_c + -2.0*PP_c + -1.0*Att_c*PP_c$ 

$BC = -1.0 + 6.0 * Att + 1.0*PP + -1.0*Att*PP$    

::: {.callout-important}
# Question

So what does $b_{Att*PP}$ indicate?
:::

:::{.fragment}
[The coefficient for the interaction indicates how the simple effect of each $X$ changes for a one unit increase on the other $X$.]{style="color:blue;"}    

**The interaction coefficient applies symmetrically to the effects of both $X$s.**
:::

-----

Considering Att in raw model:   

- Att effect is 6 for PP = 0    
- Att effect is 5 for PP = 1    
- Att effect is 4 for PP = 2    
- Att effect is 3 for PP = 3   
- Att effect is 2 for PP = 4   
- Att effect is 1 for PP = 5    

```{r}
#| code-fold: true

plot_m_int_expanded
```

-----

Considering PP in raw model:   

- PP effect is 1 for Att = 0   
- PP effect is 0 for Att = 1    
- PP effect is -1 for Att = 2    
- PP effect is -2 for Att = 3    
- PP effect is -3 for Att = 4    
- PP effect is -4 for Att = 5     

```{r}
#| code-fold: true

plot_m_int_expanded
```

-----

## Interactive Models: Coefficient Magnitudes

- Interaction and its test is obtained from any model (regardless of centering).    

- *Main* effects of IVs are obtained from model with all IVs centered on mean. You may or may not choose to report these main effects depending on the situation.     

- Magnitude of *simple* effects of either IV can be calculated directly from the raw or centered model (I prefer the raw model for ease of thinking in raw units). <span style="color: red;">How?</span>

-----

## Interactive Models: Raw Model Coefficients

```{r}
m_int <- lm(bc ~ att * pp, data = data) 
broom::tidy(m_int)
```

-----

## Interactive Models: Simple Effect Magnitudes

Magnitude of *simple* effects of either IV can be calculated directly from the raw or centered model (I prefer the raw model for ease of thinking in raw units). 

::: {.callout-important}
# Question

How?
:::

$BC = -1.0 + 6.0 * Att + 1.0*PP + -1.0*Att*PP$   

:::{.fragment}

[Effect of Att = 6.0 + -1.0 * PP]{style="color:blue;"}    

[Effect of PP = 1.0 + -1.0 * Att]{style="color:blue;"}    
:::

----

::: {.callout-important}
# Question

How could you obtain formal significance tests for any specific simple effects (e.g., effect of Att at PP = 1, 3, & 5)?
:::

:::{.fragment}

$BC = -1.0 + 6.0 * Att + 1.0*PP + -1.0*Att*PP$    

[Effect of Att = 6.0 + -1.0 * PP]{style="color:blue;"}   

[Effect of Att = 6.0 + -1.0 * (1) = 5]{style="color:blue;"}

```{r}
data <- data |> 
  mutate(pp_1 = pp - 1)

m_pp_1 <- lm(bc ~ att * pp_1, data = data)
broom::tidy(m_pp_1)
```

:::

-----


$BC = -1.0 + 6.0 * Att + 1.0*PP + -1.0*Att*PP$   

[Effect of Att = 6.0 + -1.0 * PP]{style="color:blue;"}       

[Effect of Att = 6.0 + -1.0 * (5) = 1]{style="color:blue;"}    

```{r}
data <- data |> 
  mutate(pp_5 = pp - 5)

m_pp_5 <- lm(bc ~ att * pp_5, data = data)
broom::tidy(m_pp_5)
```

-----

## Interactive Models: Reporting Considerations

::: {.callout-important}
# Question

So what do you report and in what order?
:::


:::{.fragment}
[There is no one answer here. Some say you never report main effects when there are significant interactions. This camp would say, report only interaction and possibly simple effects.]{style="color:blue;"}   

[Others (including me) believe that main effects are sometimes useful to report even when interaction is significant. *Main* effects (effect at “mean” of other IV) provides an anchor for effect. The interaction then indicates how this effect changes across values of the other IV. *Simple* effects can sometimes allow you to describe further how this effect changes at various values of other IV.]{style="color:blue;"}    

[The other complexity surrounds the nature of the two IVs.  Sometimes, often, one is focal and one is moderator. However, sometimes neither/both are focal…]{style="color:blue;"}    
:::

-----

## Interactive Models: Sample Report

*A sample brief report: Attitudes are focal, main effect and simple effects reported.*   

We analyzed birth control use in a General Linear Model with Positive Attitudes about Birth Control (ATT) and Negative Peer Pressure (PP) as quantitatively measured predictors. We also included the interaction between these two predictors in the model. We mean-centered all predictors.   

The overall model accounted for a significant amount of variance in Birth Control use, $R^2= 0.94, F(3,121) = 605.00, p < .001$.  The effect of ATT was significant, 95% CI(b) = [2.8, 3.2], $\Delta R^2 = 0.56, t(121) = 33.00, p < .001$, such that birth control use increased by 3 units for every one unit increase in positive attitudes about birth control for participants who experienced average peer pressure (i.e., PP=3). However, PP significantly moderated the ATT effect on Birth Control, 95% CI(b)= [-1.1, -0.9], $\Delta R^2 = 0.13, t(121) = 15.56, p < .001$, indicating that the magnitude of the ATT effect decreased for every one unit increase in PP (see Figure 1).  Despite this, the simple effects of ATT remained significant across meaningful levels of PP.  For example, the simple effect of ATT for participants who were experiencing high negative peer pressure (i.e., PP = 5) was significant, 95% CI(b)= [0.7, 1.3], $\Delta R^2 = 0.02, t(121) = 6.35, p < .001$.  The simple effect of ATT for participants who were experiencing low negative peer pressure (PP=1) was also significant, 95% CI(b)= [4.7, 5.3], $\Delta R^2 = 0.52, t(121)= 31.75, p < .001$.  

-----

```{r}
preds_pub <- expand.grid(att = 1:5, pp = c(2, 4))
preds_pub_y <- predict(m_int, newdata = preds_pub, se.fit = TRUE) 

preds_pub <- preds_pub |> 
  bind_cols(preds_pub_y) |> 
  as_tibble() |>  
  mutate(upper = fit + se.fit,
         lower= fit - se.fit)

plot_pub <- preds_pub |> 
  ggplot() +
  geom_ribbon(aes(x = att, ymin = lower, ymax = upper, group = factor(pp)), 
              alpha = 0.4) +
  geom_line(aes(x = att, y = fit, 
                color = factor(pp)), 
            linewidth = 1) +
  labs(x = "Attitude about Birth Control",
       y = "Birth Control Use",
       color = "Peer Pressure") 
```

-----

```{r}
plot_pub
```


-----

## Two Quantitative Variables: Summary

For two quantitative IVs, you now know:   

- How to quantify and test *main* effects (effect of IV at mean of other IV).   
- How to quantify and test for interaction.   
- How to quantify and test *simple* effects of each IV at levels of other IV.   
- How to graphically display effects.    

In case it wasn’t obvious, this is the conceptual equivalent of a factorial ANOVA with two quantitative (rather than categorical) IVs.   
