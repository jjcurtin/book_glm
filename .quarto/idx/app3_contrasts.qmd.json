{"title":"Monte Carlo Simulation of Contrast Approaches","markdown":{"yaml":{"editor_options":{"chunk_output_type":"console"}},"headingText":"Monte Carlo Simulation of Contrast Approaches","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n```{r}\n#| echo: false\n\nlibrary(tidyverse)\nlibrary(broom)\n```\n\n### 3 Groups with No Group Differences (Type I Errors) \n\nSet up simulation characteristics for Null Findings.\n\nThis will allow us to determine Type I error rates because any signficant effect is a type I error given we have set the population effect to 0\n```{r}\n# simulate N experiments\nn_experiments <- 20000\n\n# group means\nm_1 <- 10\nm_2 <- 10\nm_3 <- 10\n\nsd <- 20 # sd for y\nn <- 50 # group size\n\n# set up x as factor\nx <-  factor(c(rep(\"a\", n), rep(\"b\", n), rep(\"c\", n)))  \n\nset.seed(1234567)\n```\n\n--------------------------------------------------------------------------------\n\n**1. POCs - all focal (separate research questions)**\n\n```{r}\n#| label: null_poc \n#| code-fold: true\n\nsimulate_poc <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(2, -1, -1,\n                         0,  1, -1), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05,\n         sig_c2 = results$p.value[3] < 0.05,\n         sig_any = any(results$p.value[2:3] < 0.05))\n}\n\ntype1_poc <- map(1:n_experiments, simulate_poc) |> \n  list_rbind()\n```\n\nResults (to make clear what function returns)\n```{r}\ntype1_poc |> head()\n```\n\nTest wise type I error for each contrast is 5%\n```{r}\nmean(type1_poc$sig_c1)\nmean(type1_poc$sig_c2)\n```\n\nThe results across contrasts are independent because they come from different families\n```{r}\ncor(type1_poc$sig_c1, type1_poc$sig_c2) |> round(2)\n```\n\nTo be clear, the family-wise type I error across the set is 10% BUT often not considered in same family so not important?\n```{r}\nmean(type1_poc$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Dummy contrasts from one model (3 levels; no protection).** \n\n```{r}\n#| label: null_dummy\n#| code-fold: true\n\nsimulate_dummy <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_any = any(c(results$p.value[2:3]) < 0.05))\n}\n\ntype1_dummy <- map(1:n_experiments, simulate_dummy) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is 5%\n```{r}\nmean(type1_dummy$sig_d1)\nmean(type1_dummy$sig_d2)\n```\n\nBut these are from same family (results of contrasts are related)\n```{r}\ncor(type1_dummy$sig_d1, type1_dummy$sig_d2) |> round(2)\n```\n\nTherefore, family-wise error rate is higher (but not 10% because contrasts are dependent/related)\n```{r}\nmean(type1_dummy$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**3. All (3) pairwise contrasts (no protection).** \n\n```{r}\n#| label: null_pair\n#| code-fold: true\n\nsimulate_pair <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n \n  # fit second model \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results_3$p.value[2] < 0.05,\n         sig_d2 = results_3$p.value[3] < 0.05,\n         sig_d3 = results_1$p.value[2] < 0.05,\n         sig_any = any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_pair <- map(1:n_experiments, simulate_pair) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is 5%\n```{r}\nmean(type1_pair$sig_d1)\nmean(type1_pair$sig_d2)\nmean(type1_pair$sig_d3)\n```\n\nBut these are from same family (results of contrasts are related)\n```{r}\ncor(type1_pair$sig_d1, type1_pair$sig_d2) |> round(2)\ncor(type1_pair$sig_d1, type1_pair$sig_d3) |> round(2)\ncor(type1_pair$sig_d2, type1_pair$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is higher (but not 15% because contrasts are dependent/related)\n```{r}\nmean(type1_pair$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**4. Fisher LSD with 3 pairwise comparisons**\n```{r}\n#| label: null_fish\n#| code-fold: true\n\nsimulate_fisher <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_any = sig_omnibus && any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_fish <- map(1:n_experiments, simulate_fisher) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is < 5% (too conservative!)\n```{r}\nmean(type1_fish$sig_d1)\nmean(type1_fish$sig_d2)\nmean(type1_fish$sig_d3)\n```\n\nThese are from same family (results of contrasts are even more related)\n```{r}\ncor(type1_fish$sig_d1, type1_fish$sig_d2) |> round(2)\ncor(type1_fish$sig_d1, type1_fish$sig_d3) |> round(2)\ncor(type1_fish$sig_d2, type1_fish$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is controlled at 5% \n```{r}\nmean(type1_fish$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**5. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: null_hb\n#| code-fold: true\n\nsimulate_hb <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_any = any(p_contrasts < 0.05))\n}\n\ntype1_hb <- map(1:n_experiments, simulate_hb) |> \n  list_rbind()\n```\n\nTest-wise Type I is well under 5% (too conservative!) \n```{r}\nmean(type1_hb$sig_d1)\nmean(type1_hb$sig_d2)\nmean(type1_hb$sig_d3)\n```\n\nThese are from same family (results of contrasts are related)\n```{r}\ncor(type1_hb$sig_d1, type1_hb$sig_d2) |> round(2)\ncor(type1_hb$sig_d1, type1_hb$sig_d3) |> round(2)\ncor(type1_hb$sig_d2, type1_hb$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is controlled at 5%\n```{r}\nmean(type1_hb$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with One Group Difference (Type II Errors)\n\nNow lets consider Type II errors.  This is too often neglected in these discussions.  However it is also complicated because there are LOTS of different ways that the population effects could be set up and its not necessarily true that the same method would be more powerful across these settings.  You should consider these simulations as only a start to comparing the power of these methods.\n\nHere we update the pattern of means such that one group is different from the other two but the other two group means are equal\n```{r}\nm_1 <- 10\nm_2 <- 10\nm_3 <- 20\n```\n\n--------------------------------------------------------------------------------\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis pattern of means is well-suited to using dummy codes with the third group as reference.  \nThat said, if we only tested these two contrasts, we couldnt conclude anything about differences between groups 1 and 2.\n\n```{r}\n#| label: eff_dummy\n#| code-fold: true\n\nsimulate_dummy_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_both = all(c(results$p.value[2:3]) < 0.05))\n}\n\ntype2_dummy <- map(1:n_experiments, simulate_dummy_2) |> \n  list_rbind()\n```\n\n\nHere is power for the two contrasts that should be significant and for finding both significant.\n\nPower is low (70%) for the individual tests but what we would expect given the effect size and sample size.  What we care about is relative power across the approaches. \n\nBut again, we should also note that this method doesnt inform us about differences between group 1 and 2\n```{r}\nmean(type2_dummy$sig_d1)\nmean(type2_dummy$sig_d2)\nmean(type2_dummy$sig_both)\n```\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n```{r}\n#| label: eff_fish \n#| code-fold: true\n\nsimulate_fish_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n\n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_both = sig_omnibus && all(c(results_3$p.value[2:3]) < 0.05))\n}\n\ntype2_fish <- map(1:n_experiments, simulate_fish_2) |> \n  list_rbind()\n```\n\n\nPower is lower for the two individual contrasts (63-64%) but we get the third contrasts to show that G1 and G2 are not difference (with 5% false alarm rate).\n```{r}\nmean(type2_fish$sig_d1)\nmean(type2_fish$sig_d2)\nmean(type2_fish$sig_both)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: eff_hb \n#| code-fold: true\n\nsimulate_hb_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_both = all(p_contrasts[1:2] < 0.05))\n} \n\ntype2_hb <- map(1:n_experiments, simulate_hb_2) |> \n  list_rbind()\n```\n\nHB is worse still on power for the individual contrasts (55-56%)\n```{r}\nmean(type2_hb$sig_d1)\nmean(type2_hb$sig_d2)\nmean(type2_hb$sig_both)\n```\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nPOCs don't fit perfectly to this setting.  However, if in this instance our theory predicts only group three different from groups 1 and 2, we could test only that contrast or we might test the second contrast to demonstrate it was NOT significant.\n\n```{r}\n#| label: eff_poc \n#| code-fold: true\n\nsimulate_poc_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(-1, -1, 2,\n                          1, -1, 0), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05)\n}\n\ntype2_poc <- map(1:n_experiments, simulate_poc_2) |> \n  list_rbind()\n```\n\nHere we only care about the power for the first effect.  Clearly, the best power (~81%) if this is sufficient.  We would likely want the second contrast to be non-significant to demonstrate that the effect is specific to group 3.  This would false alarm at 5%.\n```{r}\nmean(type2_poc$sig_c1)\n```\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with All Groups Different (Type II Errors)\n\n```{r}\nm_1 <- 10\nm_2 <- 20\nm_3 <- 30\n```\n\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis approach doesn't make sense because we want to test all three contrasts (assuming our theory correctly predicted all groups different)\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n```{r}\n#| label: eff_fish_3 \n#| code-fold: true\n\nsimulate_fish_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n\n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_all = sig_omnibus && all(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype2_fish_3 <- map(1:n_experiments, simulate_fish_3) |> \n  list_rbind()\n```\n\n\nPower is much better for G1 vs G3 (~99%) because its a bigger mean difference than for the other two (w/ ~ 70% power).  But again, its about relative power now.  We need to compare to other methods.\n```{r}\nmean(type2_fish_3$sig_d1)\nmean(type2_fish_3$sig_d2)\nmean(type2_fish_3$sig_d3)\nmean(type2_fish_3$sig_all)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: eff_hb_3 \n#| code-fold: true\n\nsimulate_hb_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_all = all(p_contrasts[1:3] < 0.05))\n} \n\ntype2_hb_3 <- map(1:n_experiments, simulate_hb_3) |> \n  list_rbind()\n```\n\nHB is a bit worse power for the the smaller individual contrasts (66-67%)\n```{r}\nmean(type2_hb_3$sig_d1)\nmean(type2_hb_3$sig_d2)\nmean(type2_hb_3$sig_d3)\nmean(type2_hb_3$sig_all)\n```\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nAgain, not clear exactly how to use POCs in this setting if we expect all groups to be different\n\n\n###  What about with 4 levels?\n\nHavent done this year but we will see that Type 1 control falls apart for all pairwise with fisher LSD because there are two many pairwise contrasts.   Without, that most wouldnt tolerate it.   HB will still handle 4 levels with go type 1 control.   Power will be lower still though because there will be bigger adjustments to all p-values.  POCs can really shine here but ONLY if the pattern of means works.   Otherwise, HB is the best bet.","srcMarkdownNoYaml":"\n\n# Monte Carlo Simulation of Contrast Approaches {.unnumbered}\n\n```{r}\n#| echo: false\n\nlibrary(tidyverse)\nlibrary(broom)\n```\n\n### 3 Groups with No Group Differences (Type I Errors) \n\nSet up simulation characteristics for Null Findings.\n\nThis will allow us to determine Type I error rates because any signficant effect is a type I error given we have set the population effect to 0\n```{r}\n# simulate N experiments\nn_experiments <- 20000\n\n# group means\nm_1 <- 10\nm_2 <- 10\nm_3 <- 10\n\nsd <- 20 # sd for y\nn <- 50 # group size\n\n# set up x as factor\nx <-  factor(c(rep(\"a\", n), rep(\"b\", n), rep(\"c\", n)))  \n\nset.seed(1234567)\n```\n\n--------------------------------------------------------------------------------\n\n**1. POCs - all focal (separate research questions)**\n\n```{r}\n#| label: null_poc \n#| code-fold: true\n\nsimulate_poc <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(2, -1, -1,\n                         0,  1, -1), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05,\n         sig_c2 = results$p.value[3] < 0.05,\n         sig_any = any(results$p.value[2:3] < 0.05))\n}\n\ntype1_poc <- map(1:n_experiments, simulate_poc) |> \n  list_rbind()\n```\n\nResults (to make clear what function returns)\n```{r}\ntype1_poc |> head()\n```\n\nTest wise type I error for each contrast is 5%\n```{r}\nmean(type1_poc$sig_c1)\nmean(type1_poc$sig_c2)\n```\n\nThe results across contrasts are independent because they come from different families\n```{r}\ncor(type1_poc$sig_c1, type1_poc$sig_c2) |> round(2)\n```\n\nTo be clear, the family-wise type I error across the set is 10% BUT often not considered in same family so not important?\n```{r}\nmean(type1_poc$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Dummy contrasts from one model (3 levels; no protection).** \n\n```{r}\n#| label: null_dummy\n#| code-fold: true\n\nsimulate_dummy <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_any = any(c(results$p.value[2:3]) < 0.05))\n}\n\ntype1_dummy <- map(1:n_experiments, simulate_dummy) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is 5%\n```{r}\nmean(type1_dummy$sig_d1)\nmean(type1_dummy$sig_d2)\n```\n\nBut these are from same family (results of contrasts are related)\n```{r}\ncor(type1_dummy$sig_d1, type1_dummy$sig_d2) |> round(2)\n```\n\nTherefore, family-wise error rate is higher (but not 10% because contrasts are dependent/related)\n```{r}\nmean(type1_dummy$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**3. All (3) pairwise contrasts (no protection).** \n\n```{r}\n#| label: null_pair\n#| code-fold: true\n\nsimulate_pair <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n \n  # fit second model \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results_3$p.value[2] < 0.05,\n         sig_d2 = results_3$p.value[3] < 0.05,\n         sig_d3 = results_1$p.value[2] < 0.05,\n         sig_any = any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_pair <- map(1:n_experiments, simulate_pair) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is 5%\n```{r}\nmean(type1_pair$sig_d1)\nmean(type1_pair$sig_d2)\nmean(type1_pair$sig_d3)\n```\n\nBut these are from same family (results of contrasts are related)\n```{r}\ncor(type1_pair$sig_d1, type1_pair$sig_d2) |> round(2)\ncor(type1_pair$sig_d1, type1_pair$sig_d3) |> round(2)\ncor(type1_pair$sig_d2, type1_pair$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is higher (but not 15% because contrasts are dependent/related)\n```{r}\nmean(type1_pair$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**4. Fisher LSD with 3 pairwise comparisons**\n```{r}\n#| label: null_fish\n#| code-fold: true\n\nsimulate_fisher <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_any = sig_omnibus && any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_fish <- map(1:n_experiments, simulate_fisher) |> \n  list_rbind()\n```\n\nTest-wise Type I for each contrast is < 5% (too conservative!)\n```{r}\nmean(type1_fish$sig_d1)\nmean(type1_fish$sig_d2)\nmean(type1_fish$sig_d3)\n```\n\nThese are from same family (results of contrasts are even more related)\n```{r}\ncor(type1_fish$sig_d1, type1_fish$sig_d2) |> round(2)\ncor(type1_fish$sig_d1, type1_fish$sig_d3) |> round(2)\ncor(type1_fish$sig_d2, type1_fish$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is controlled at 5% \n```{r}\nmean(type1_fish$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n**5. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: null_hb\n#| code-fold: true\n\nsimulate_hb <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_any = any(p_contrasts < 0.05))\n}\n\ntype1_hb <- map(1:n_experiments, simulate_hb) |> \n  list_rbind()\n```\n\nTest-wise Type I is well under 5% (too conservative!) \n```{r}\nmean(type1_hb$sig_d1)\nmean(type1_hb$sig_d2)\nmean(type1_hb$sig_d3)\n```\n\nThese are from same family (results of contrasts are related)\n```{r}\ncor(type1_hb$sig_d1, type1_hb$sig_d2) |> round(2)\ncor(type1_hb$sig_d1, type1_hb$sig_d3) |> round(2)\ncor(type1_hb$sig_d2, type1_hb$sig_d3) |> round(2)\n```\n\nFamily-wise error rate is controlled at 5%\n```{r}\nmean(type1_hb$sig_any)\n```\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with One Group Difference (Type II Errors)\n\nNow lets consider Type II errors.  This is too often neglected in these discussions.  However it is also complicated because there are LOTS of different ways that the population effects could be set up and its not necessarily true that the same method would be more powerful across these settings.  You should consider these simulations as only a start to comparing the power of these methods.\n\nHere we update the pattern of means such that one group is different from the other two but the other two group means are equal\n```{r}\nm_1 <- 10\nm_2 <- 10\nm_3 <- 20\n```\n\n--------------------------------------------------------------------------------\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis pattern of means is well-suited to using dummy codes with the third group as reference.  \nThat said, if we only tested these two contrasts, we couldnt conclude anything about differences between groups 1 and 2.\n\n```{r}\n#| label: eff_dummy\n#| code-fold: true\n\nsimulate_dummy_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_both = all(c(results$p.value[2:3]) < 0.05))\n}\n\ntype2_dummy <- map(1:n_experiments, simulate_dummy_2) |> \n  list_rbind()\n```\n\n\nHere is power for the two contrasts that should be significant and for finding both significant.\n\nPower is low (70%) for the individual tests but what we would expect given the effect size and sample size.  What we care about is relative power across the approaches. \n\nBut again, we should also note that this method doesnt inform us about differences between group 1 and 2\n```{r}\nmean(type2_dummy$sig_d1)\nmean(type2_dummy$sig_d2)\nmean(type2_dummy$sig_both)\n```\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n```{r}\n#| label: eff_fish \n#| code-fold: true\n\nsimulate_fish_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n\n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_both = sig_omnibus && all(c(results_3$p.value[2:3]) < 0.05))\n}\n\ntype2_fish <- map(1:n_experiments, simulate_fish_2) |> \n  list_rbind()\n```\n\n\nPower is lower for the two individual contrasts (63-64%) but we get the third contrasts to show that G1 and G2 are not difference (with 5% false alarm rate).\n```{r}\nmean(type2_fish$sig_d1)\nmean(type2_fish$sig_d2)\nmean(type2_fish$sig_both)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: eff_hb \n#| code-fold: true\n\nsimulate_hb_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_both = all(p_contrasts[1:2] < 0.05))\n} \n\ntype2_hb <- map(1:n_experiments, simulate_hb_2) |> \n  list_rbind()\n```\n\nHB is worse still on power for the individual contrasts (55-56%)\n```{r}\nmean(type2_hb$sig_d1)\nmean(type2_hb$sig_d2)\nmean(type2_hb$sig_both)\n```\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nPOCs don't fit perfectly to this setting.  However, if in this instance our theory predicts only group three different from groups 1 and 2, we could test only that contrast or we might test the second contrast to demonstrate it was NOT significant.\n\n```{r}\n#| label: eff_poc \n#| code-fold: true\n\nsimulate_poc_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(-1, -1, 2,\n                          1, -1, 0), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05)\n}\n\ntype2_poc <- map(1:n_experiments, simulate_poc_2) |> \n  list_rbind()\n```\n\nHere we only care about the power for the first effect.  Clearly, the best power (~81%) if this is sufficient.  We would likely want the second contrast to be non-significant to demonstrate that the effect is specific to group 3.  This would false alarm at 5%.\n```{r}\nmean(type2_poc$sig_c1)\n```\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with All Groups Different (Type II Errors)\n\n```{r}\nm_1 <- 10\nm_2 <- 20\nm_3 <- 30\n```\n\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis approach doesn't make sense because we want to test all three contrasts (assuming our theory correctly predicted all groups different)\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n```{r}\n#| label: eff_fish_3 \n#| code-fold: true\n\nsimulate_fish_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n\n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_all = sig_omnibus && all(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype2_fish_3 <- map(1:n_experiments, simulate_fish_3) |> \n  list_rbind()\n```\n\n\nPower is much better for G1 vs G3 (~99%) because its a bigger mean difference than for the other two (w/ ~ 70% power).  But again, its about relative power now.  We need to compare to other methods.\n```{r}\nmean(type2_fish_3$sig_d1)\nmean(type2_fish_3$sig_d2)\nmean(type2_fish_3$sig_d3)\nmean(type2_fish_3$sig_all)\n```\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n```{r}\n#| label: eff_hb_3 \n#| code-fold: true\n\nsimulate_hb_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_all = all(p_contrasts[1:3] < 0.05))\n} \n\ntype2_hb_3 <- map(1:n_experiments, simulate_hb_3) |> \n  list_rbind()\n```\n\nHB is a bit worse power for the the smaller individual contrasts (66-67%)\n```{r}\nmean(type2_hb_3$sig_d1)\nmean(type2_hb_3$sig_d2)\nmean(type2_hb_3$sig_d3)\nmean(type2_hb_3$sig_all)\n```\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nAgain, not clear exactly how to use POCs in this setting if we expect all groups to be different\n\n\n###  What about with 4 levels?\n\nHavent done this year but we will see that Type 1 control falls apart for all pairwise with fisher LSD because there are two many pairwise contrasts.   Without, that most wouldnt tolerate it.   HB will still handle 4 levels with go type 1 control.   Power will be lower still though because there will be bigger adjustments to all p-values.  POCs can really shine here but ONLY if the pattern of means works.   Otherwise, HB is the best bet."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["book.css"],"output-file":"app3_contrasts.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","bibliography":["refs.bib"],"callout-icon":false,"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}