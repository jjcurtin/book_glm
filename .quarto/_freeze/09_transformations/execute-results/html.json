{
  "hash": "b4218bb8c025beb15d83121b63fc9750",
  "result": {
    "engine": "knitr",
    "markdown": "--- \noutput: html_document \neditor_options:  \n  chunk_output_type: console\n--- \n\n\n \n\n# Unit 9: Dealing with Messy Data III - Transformations\n\n\n\n\n\n\n\n\n## Transformations: The Family of Power and Roots\n\nThe GLM makes **strong assumptions** about the structure of data, assumptions which often fail to hold in practice.\n\n- One solution is to abandon the GLM for more complicated models (generalized linear models; weighted least squares; robust regression).\n\n- Another solution is to transform the data (either the $X$s or $Y$) so that they conform more closely to the assumptions.  \n\n------------------------------------------------------------------------------\n\nA particularly useful group of transformations is the *family* of powers and roots: \n\n$X$ → $X^p$\n\nIf p is negative, then the transformation is an inverse power: \n\n- $X^{-1} = \\frac{1}{X}$, and $X^{−2} = \\frac{1}{X^2}$\n\nIf p is a fraction, then the transformation represents a root: \n\n- $X^{\\frac{1}{2}} = \\sqrt{X}$, and $X^{-\\frac{1}{2}} = \\frac{1}{\\sqrt{X}}$\n    \n    \n------------------------------------------------------------------------------\n\nThe Box-Cox family of transformations provides a comparable but more convenient form (in some cases*).\n\n- $X$ → $X^{(p)} \\equiv \\frac{X^p- 1}{p}$\n    \nSince $X^{(p)}$ is a linear function of $X^p$, the 2 transformations have the same essential effect on the data.\n\n- Dividing by p preserves the direction of $X$, which otherwise would be reversed when p is negative.  \n- This function also matches level and slope of curve at $X=1$.\n    \n------------------------------------------------------------------------------\n    \n\n\n::: {.cell}\n::: {.cell-output-display}\n![The Box-Cox family of modified power transformations, $X^{(p)}= (X^p - 1)/p$, for values of $p = -1, 0, 1, 2, 3$. When $p = 0, X^{(p)} = log_e X$.](09_transformations_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------------\n\nThe power transformation $X^{(0)}$ is useless, but the very useful log transformation is a kind of *zero* power:\n                   \nlim\n$p$ → $0$   $\\frac{X^p - 1}{p} = \\text{log}_eX$\n                   \nwhere $e \\approx 2.718$ is the base of the natural logarithms. Thus, we will take $X^{(0)} \\equiv log(X)$.\n\n\nIt is generally more convenient to use logs to the base 10 or base 2, which are more easily interpreted than logs to the base e.\n\n- Changing bases is equivalent to multiplying your variable by a constant. No effect on significance tests.\n\n------------------------------------------------------------------------------\n\nDescending the *ladder* of powers and roots from $p = 1$ (i.e., no transformation) towards $p = -2$ **compresses the large values of X and spreads out the small ones**.\n\nAscending the ladder of powers from $p = 1$ towards $p = 3$ has the **opposite effect**.\n\n-----\n\nPower transformations are sensible only when all of the values of $X$ are positive:\n\n- Some of the transformations, such as log ($p= 0$) and square root ($p= .5$), are undefined for negative or zero values of $X$.\n\n- Power transformations are not monotone (i.e., they change to order of scores) when there are both positive and negative values among the data.\n\n- We can add a positive constant (called a *start*) to each data value to make all of the values positive: \n\n  - $X$ → $(X + s)^{(p)}$\n    \n------------------------------------------------------------------------------\n\nPower transformations are effective only when the ratio of the biggest data values to the smallest ones is sufficiently large; if this ratio is close to 1, then power transformations are nearly linear.  For example:  \n\n- Power transformations will work well if range of $X = 1 – 100$.\n  \n- Power transformations will have little effect if range of $X = 1000 – 1100$\n    \nUsing a negative start can often increase the ratio of highest/lowest score.\n\nUsing reasonable starts, if necessary, an adequate power transformation can usually be found in the range $−2 \\le p \\le 3$.\n\n------------------------------------------------------------------------------\n\nPower transformations of $Y$ (or sometimes $X$) can correct problems with normality of errors.\n\nPower transformations of $Y$ (or sometimes $X$) can stabilize the variance of the errors.\n\nPower transformations of $X$ (or sometimes $Y$) can make many nonlinear relationships more nearly linear.\n\nYou can experiment with Box-Cox transformations of $X$ or $Y$ in R using `bcPower()` in the `car` package.\n\nIn many fields, $X^p$ rather than $X^{(p)}$ may be preferred. Particularly, if $p \\ge 0$. This is simply done algebraically.\n\n------------------------------------------------------------------------------\n\n## Transformations: Dealing with Skew\n\nTransforming $Y$ *down the ladder* can correct positive skew in the errors (most common problem).  \n\nTransforming $Y$ *up the ladder* corrects negative skew in the errors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(102030)\ny <- tibble(y_raw = rchisq(n=500, df=1),\n            y_.5 = car::bcPower(y_raw, .5),\n            y_.25 = car::bcPower(y_raw, .25),\n            y_0 = car::bcPower(y_raw, 0))    \n\nplot_raw <- y |> \n  ggplot(aes(x = y_raw)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-.5, 12), breaks = c(0, 2, 4, 6, 8, 10)) +\n  labs(title = \"Raw Y\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_raw)$bw, 4)))\n\nplot_.5 <- y |> \n  ggplot(aes(x = y_.5)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-3, 5), breaks = c(-2, 0, 2, 4)) +\n  labs(title = \"p = .5; sqrt(Y)\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_.5)$bw, 4)))\n\nplot_.25 <- y |> \n  ggplot(aes(x = y_.25)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-4.5, 4), breaks = c(-4, -2, 0, 2, 4)) +\n  labs(title = \"p = .25\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_.25)$bw, 4)))\n\nplot_0 <- y |> \n  ggplot(aes(x = y_0)) +\n  geom_density() + \n  scale_x_continuous(limits = c(-15, 4), breaks = c(-15, -10. -5, 0)) +\n  labs(title = \"p = 0; log(Y)\",\n       x = NULL,\n       caption = str_c(\"N = 500; bandwidth = \", round(density(y$y_0)$bw, 4)))\n\n\n(plot_raw + plot_.5) / (plot_.25 + plot_0)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------------\n\n## Transformations: Dealing with Heteroscedasticity\n\nTransforming $Y$ *down the ladder* can correct problems with increasing spread of errors as $Y$ increases (most common problem).  \n\nTransforming $Y$ *up the ladder* corrects decreasing spread. \n\nThe problems of unequal spread and skewness commonly occur together and can be corrected together. Therefore, transforming $Y$ down the ladder can correct both issues simultaneously.\n\n------------------------------------------------------------------------------\n\nLets return to the FPS example from the previous units and fit the 3 predictor model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_data <- \"data_lecture\"\ndata <- read_csv(here::here(path_data, \"09_transformations_fps.csv\"), \n                 show_col_types = FALSE) |> \n   mutate(sex_c = if_else(sex == \"female\", -.5, .5)) |> \n   # remove outliers to fit same model as last two units \n   filter(!subid %in% c(\"0125\", \"2112\"))\n\nm <- lm(fps ~ bac + ta + sex_c, data = data)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = fps ~ bac + ta + sex_c, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.419 -15.870  -3.955  14.992  81.050 \n\nCoefficients:\n              Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)   28.70270    6.44222   4.455 0.0000240 ***\nbac         -254.58936   72.18588  -3.527  0.000664 ***\nta             0.12306    0.02726   4.514 0.0000192 ***\nsex_c        -15.85499    5.69071  -2.786  0.006505 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.49 on 90 degrees of freedom\nMultiple R-squared:  0.3191,\tAdjusted R-squared:  0.2964 \nF-statistic: 14.06 on 3 and 90 DF,  p-value: 0.0000001355\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------------\n\nLet review model assumptions \n\n- Here is a QQ plot and a density plot of residuals\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncar::qqPlot(m, id = FALSE, simulate = TRUE,\n            main = \"Quantile-Comparison Plot to Assess Normality\", \n            ylab = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_density(aes(x = value), data = enframe(rstudent(m), name = NULL)) +\n  labs(title = \"Density Plot to Assess Normality of Residuals\",\n       x = \"Studentized Residual\") +\n  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),\n                                      y = dnorm(seq(-4, 4, length.out = 100), \n                                      mean=0, sd=sd(rstudent(m)))), \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------------\n\n- Here is a plot of residuals by predicted values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y_hat = predict(m),\n              res = rstudent(m)) |> \n  ggplot(aes(x = y_hat, y = res)) +\n  geom_point(alpha = .6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(title = \"Studentized Residuals vs. Predicted Values\",\n       x = \"Predicted Values\",\n       y = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n-------------------------------------------------------------------------------\n\n- And a spread level plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> car::spreadLevelPlot()\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSuggested power transformation:  0.2370356 \n```\n\n\n:::\n:::\n\n\n\n-------------------------------------------------------------------------------\n\n- And statistical test for non-constant variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> car::ncvTest()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 13.2232, Df = 1, p = 0.00027651\n```\n\n\n:::\n:::\n\n\n\n-------------------------------------------------------------------------------\n\n- And global tests of model assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> gvlma::gvlma()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = fps ~ bac + ta + sex_c, data = data)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n    28.7027    -254.5894       0.1231     -15.8550  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = m) \n\n                        Value p-value                   Decision\nGlobal Stat        9.39689053 0.05191    Assumptions acceptable.\nSkewness           3.60145982 0.05773    Assumptions acceptable.\nKurtosis           0.30834234 0.57870    Assumptions acceptable.\nLink Function      0.00002503 0.99601    Assumptions acceptable.\nHeteroscedasticity 5.48706334 0.01916 Assumptions NOT satisfied!\n```\n\n\n:::\n:::\n\n\n\n-------------------------------------------------------------------------------\n\n\n## Box Cox Transformation Function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  select(bac, ta, sex_c, fps) |> \n  skim() |> \n  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> \n  yank(\"numeric\")\n```\n\n::: {.cell-output-display}\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing|   mean|     sd|    min|    max|\n|:-------------|---------:|------:|------:|------:|------:|\n|bac           |         0|   0.06|   0.04|   0.00|   0.14|\n|ta            |         0| 145.91| 104.80|  10.00| 445.00|\n|sex_c         |         0|   0.01|   0.50|  -0.50|   0.50|\n|fps           |         0|  32.19|  32.77| -26.07| 136.82|\n\n\n\n:::\n:::\n\n\n\nWe are going to first add a constant so all response values are > 0\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |> \n  mutate(fps_27 = fps + 27)\n\nm_2 <- lm(fps_27 ~ bac + ta + sex_c, data = data)\n```\n:::\n\n\n\n-----\n\nNext we will pull the best lambda value from a plot of log-likelihood values by lambda power transformations of response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::boxCox(m_2)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n\n```{.r .cell-code}\nround(car::boxCox(m_2)$x[which.max(car::boxCox(m_2)$y)],2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.55\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\nLastly, we will use the best lambda value to conduct our power transformation and re-evaluate our model assumptions. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |> \n  mutate(fps_bc = car::bcPower(fps_27, .55))\n\nm_3 <- lm(fps_bc ~ bac + ta + sex_c, data = data)\n```\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncar::qqPlot(m_3, id = FALSE, simulate = TRUE, \n            main = \"Quantile-Comparison Plot to Assess Normality\",  \n            ylab = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_density(aes(x = value), data = enframe(rstudent(m_3), name = NULL)) +\n  labs(title = \"Density Plot to Assess Normality of Residuals\",\n       x = \"Studentized Residual\") +\n  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),\n                                      y = dnorm(seq(-4, 4, length.out = 100), \n                                      mean=0, sd=sd(rstudent(m_3)))), \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = fitted.values(m_3),\n       y = rstudent(m_3)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(title = \"Studentized Residuals vs. Fitted Values\",\n       x = \"Fitted Values\",\n       y = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::spreadLevelPlot(m_3)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSuggested power transformation:  -0.5446104 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncar::ncvTest(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 6.005664, Df = 1, p = 0.01426\n```\n\n\n:::\n:::\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ngvlma::gvlma(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = fps_bc ~ bac + ta + sex_c, data = data)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n   14.21548    -37.52098      0.01793     -2.70290  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = m_3) \n\n                       Value p-value                   Decision\nGlobal Stat        12.759981 0.01251 Assumptions NOT satisfied!\nSkewness            1.588199 0.20758    Assumptions acceptable.\nKurtosis            5.520612 0.01879 Assumptions NOT satisfied!\nLink Function       0.004907 0.94416    Assumptions acceptable.\nHeteroscedasticity  5.646263 0.01749 Assumptions NOT satisfied!\n```\n\n\n:::\n:::\n\n\n\n**Transformations often don't help!**\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  14.2      1.03        13.8  6.26e-24\n2 bac         -37.5     11.5         -3.25 1.62e- 3\n3 ta            0.0179   0.00436      4.11 8.62e- 5\n4 sex_c        -2.70     0.910       -2.97 3.82e- 3\n```\n\n\n:::\n:::\n\n\n\n-----\n\nWhite (1980) Heteroscedasticity-corrected SEs and Tests\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrected_ses <- sqrt(diag(car::hccm(m)))\n\nbroom::tidy(m) |> \n  select(term, estimate) |> \n  add_column(std.error = corrected_ses) |> \n  mutate(statistic = estimate/std.error,\n         p.value = 2*(pt(abs(statistic), df=m_2$df.residual, lower.tail=FALSE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   28.7      6.91        4.15 0.0000744\n2 bac         -255.      71.3        -3.57 0.000575 \n3 ta             0.123    0.0320      3.85 0.000224 \n4 sex_c        -15.9      5.87       -2.70 0.00826  \n```\n\n\n:::\n:::\n\n\n\n*Maybe* better choicer was untransformed Y (original model) with corrected SEs?    \n \n*Maybe* the problem wasn’t bad enough to do anything?\n\n-----\n\n## Transformations: Dealing with Non-linearity\n\nPower transformations can make simple monotone relationships more linear (Fig A).  Polynomial regression (or other transformations, e.g. logit) is often needed for more complex relationships (Figs, B & C).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/monotone_relationship.png)\n:::\n:::\n\n\n\n\n## Transformations: Mosteller and Tukey’s bulging rule\n\nSimple monotone relationships can be corrected by transforming $X$ or $Y$.     \n\n- $X$ is typical if only one $XY$ relationship is problematic.    \n\n- If all $X$s have similar non-linear relationship, transform $Y$.    \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/mosteller_tukey.png)\n:::\n:::\n\n\n\n-----\n\n::: {.callout-important}\n# Question\n\nWhat 2 transformations would make this relationship linear?\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n:::{.fragment}\n[Move $X$ down the ladder (e.g., $X^.5$).]{style=\"color:blue;\"}    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_nonlinear  |> \n  ggplot(aes(x = sqrt(X), y = Y)) +\n  geom_point(alpha = .4)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n\n:::{.fragment}\n[Move $Y$ up the ladder (e.g., $Y^2$).]{style=\"color:blue;\"}  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_nonlinear  |> \n  ggplot(aes(x = X, y = Y^2)) +\n  geom_point(alpha = .4)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n-----\n\n## General Transformations\n\n- In some fields, certain power transformations (sqrt, log, inverse) are common.\n\n- If we have a choice between transformations that perform roughly equally well, we may prefer one transformation to another because of interpretability:\n\n    - The log transformation has a convenient multiplicative interpretation (e.g., increasing  log2 (X) by 1 doubles X; increasing log10 (X) by 1 multiples X by 10).\n\n- Transformations are a big source of research dfs. Should be pre-registered.  You may know what your DV typically *needs* from prior data. Worst case, report with and without transformations and justify.\n\n-----\n\n## 5K Race Times\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_5k <- read_csv(here::here(path_data, \"09_transformations_5k.csv\"),\n                    show_col_types = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 80\nColumns: 4\n$ subid <chr> \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\"…\n$ time  <dbl> 24.91, 21.82, 21.54, 23.03, 25.35, 22.84, 30.16, 27.00, 16.42, 2…\n$ age   <dbl> 29, 25, 27, 25, 37, 31, 43, 44, 46, 53, 58, 30, 27, 36, 32, 45, …\n$ miles <dbl> 24.99984, 30.80905, 52.04042, 66.20958, 26.60005, 48.21255, 12.3…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_5k <- lm(time ~ age + miles, data = data_5k)\n\nbroom::tidy(m_5k)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   25.0      1.80       13.9  1.07e-22\n2 age            0.172    0.0333      5.17 1.77e- 6\n3 miles         -0.210    0.0246     -8.53 9.54e-13\n```\n\n\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncar::qqPlot(m_5k, id = FALSE, simulate = TRUE,\n            main = \"Quantile-Comparison Plot to Assess Normality\", \n            ylab = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_density(aes(x = value), data = enframe(rstudent(m_5k), name = NULL)) +\n  labs(title = \"Density Plot to Assess Normality of Residuals\",\n       x = \"Studentized Residual\") +\n  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),\n                                      y = dnorm(seq(-4, 4, length.out = 100), \n                                      mean=0, sd=sd(rstudent(m_5k)))), \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = fitted.values(m_5k),\n       y = rstudent(m_5k)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(title = \"Studentized Residuals vs. Fitted Values\",\n       x = \"Fitted Values\",\n       y = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::spreadLevelPlot(m_5k)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSuggested power transformation:  0.4894249 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::ncvTest(m_5k)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.8077899, Df = 1, p = 0.36877\n```\n\n\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::crPlots(m_5k, ask = FALSE)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngvlma::gvlma(m_5k)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = time ~ age + miles, data = data_5k)\n\nCoefficients:\n(Intercept)          age        miles  \n    24.9804       0.1724      -0.2097  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = m_5k) \n\n                      Value   p-value                   Decision\nGlobal Stat        13.01039 0.0112252 Assumptions NOT satisfied!\nSkewness            0.01435 0.9046389    Assumptions acceptable.\nKurtosis            0.07603 0.7827462    Assumptions acceptable.\nLink Function      12.82570 0.0003419 Assumptions NOT satisfied!\nHeteroscedasticity  0.09430 0.7587759    Assumptions acceptable.\n```\n\n\n:::\n:::\n\n\n\n-----\n\n\nLet's try transforming miles using `log2()`, a binary logarithm (base 2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_5k <- data_5k |> \n  mutate(log_miles = log2(miles))\n\nm_5k_tran <- lm(time ~ age + log_miles, data = data_5k)\n\nbroom::tidy(m_5k_tran)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   42.5      3.25       13.1  2.88e-21\n2 age            0.170    0.0320      5.32 9.97e- 7\n3 log_miles     -4.98     0.538      -9.26 3.73e-14\n```\n\n\n:::\n:::\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::crPlots(m_5k_tran, ask = FALSE)\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngvlma::gvlma(m_5k_tran)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = time ~ age + log_miles, data = data_5k)\n\nCoefficients:\n(Intercept)          age    log_miles  \n     42.519        0.170       -4.983  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = m_5k_tran) \n\n                     Value p-value                Decision\nGlobal Stat        1.83180  0.7667 Assumptions acceptable.\nSkewness           0.15584  0.6930 Assumptions acceptable.\nKurtosis           0.06402  0.8003 Assumptions acceptable.\nLink Function      1.58986  0.2073 Assumptions acceptable.\nHeteroscedasticity 0.02209  0.8819 Assumptions acceptable.\n```\n\n\n:::\n:::\n\n\n\n-----\n\n## Displaying Transformed Results With Fake Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_fake <- tibble(x = 3 * rchisq(200, df=3),\n                    x_sr = sqrt(x),\n                    y = 3 * x_sr + rnorm(200,mean=0,sd = 1))\n\nm_raw = lm(y ~ x, data = data_fake)\nbroom::tidy(m_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.67     0.148       24.9 7.86e-63\n2 x              0.519    0.0147      35.3 2.66e-87\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_fake |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_abline(intercept = coef(m_raw)[1], slope = coef(m_raw)[2], color = \"red\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_sr = lm(y ~ x_sr, data = data_fake)\nbroom::tidy(m_sr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   -0.151    0.175     -0.863 3.89e-  1\n2 x_sr           3.06     0.0624    49.1   9.18e-113\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_fake |> \n  ggplot(aes(x = x_sr, y = y)) +\n  geom_point(alpha = .4) +\n  geom_abline(intercept = coef(m_sr)[1], slope = coef(m_sr)[2], color = \"red\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n\n\n-----\n\nYou can display results from linear model but provide scale for raw $X$ instead of Sqrt($X$) or in addition to Sqrt($X$) on another axis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- tibble(x_sr = seq(min(data_fake$x_sr),max(data_fake$x_sr), by=.01))\ny_pred = predict(m_sr, newdata = preds)\n\nggplot() +\n  geom_point(aes(x = data_fake$x_sr, y = data_fake$y), alpha = .4) +\n  geom_line(aes(x = preds$x_sr, y = y_pred)) +\n  ylab(\"Y\") +\n  scale_x_continuous(name = \"SQRT(X)\", sec.axis = sec_axis(~.^2, name = \"Raw X\"))\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\n\n\n\n-----\n\nYou can display results from linear model in Raw $X$ Units and use `predict()` to get the $Y$s based on transformed $X$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- tibble(x_sr = sqrt(seq(min(data_fake$x),max(data_fake$x), by=.1)))\ny_pred = predict(m_sr, newdata = preds)\n\nggplot() +\n  geom_point(aes(x = data_fake$x, y = data_fake$y), alpha = .4) +\n  geom_line(aes(x = seq(min(data_fake$x),max(data_fake$x), by=.1), y = y_pred)) +\n  ylab(\"Y\") +\n  xlab(\"X\")\n```\n\n::: {.cell-output-display}\n![](09_transformations_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "09_transformations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}