{
  "hash": "abf1d3aba3bd6ce44beb7818b2c11279",
  "result": {
    "engine": "knitr",
    "markdown": "# Unit 7: Dealing with Messy Data I - Case Analysis\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Anscombe's Quartet\n\nAnscombe, Francis J. (1973) Graphs in statistical analysis. *American Statistician*, 27, 17–21. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n**Set 1**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_1 <- lm(y1 ~ x, data = Quartet)\nex_1 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0257 \n2 x              0.500     0.118      4.24 0.00217\n```\n\n\n:::\n\n```{.r .cell-code}\nex_1 |> sse() |> round(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.8\n```\n\n\n:::\n:::\n\n\n\n**Set 2**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nex_2 <- lm(y2 ~ x, data = Quartet)\nex_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)     3.00     1.13       2.67 0.0258 \n2 x               0.5      0.118      4.24 0.00218\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nex_2 |> sse() |> round(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.8\n```\n\n\n:::\n:::\n\n\n\n**Set 3**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nex_3 <- lm(y3 ~ x, data = Quartet)\nex_3 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x              0.500     0.118      4.24 0.00218\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nex_3 |> sse() |> round(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.8\n```\n\n\n:::\n:::\n\n\n\n**Set 4**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nex_4 <- lm(y4 ~ x4, data = Quartet)\nex_4 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x4             0.500     0.118      4.24 0.00216\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nex_4 |> sse() |> round(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.7\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note}\nSee the Quartet dataset in the `carData` package\n:::\n\n--------------------------------------------------------------------------------\n\n## Case Analysis\n\n- Goal is to identify any unusual or excessively influential data. \n\n- These data points may either bias results and/or reduce power to detect effects (inflate standard errors and/or decrease $R^2$).   \n\n- Three aspects of individual observations we attend to:\n  1. Leverage\n  2. Regression Outlier\n  3. Influence\n\n- Case Analysis also provides an important first step as you get to *know* your data.  \n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(here::here(path_data, \"07_three_predictors_fps.csv\"), \n                 show_col_types = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 96\nColumns: 5\n$ subid <chr> \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bac   <dbl> 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    <dbl> 208, 133, 120, 103, 97, 84, 34, 34, 296, 126, 200, 158, 26, 255,…\n$ fps   <dbl> 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n$ sex   <chr> \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"fem…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  subid   bac    ta    fps sex   \n  <chr> <dbl> <dbl>  <dbl> <chr> \n1 0011      0   208  19.5  female\n2 0012      0   133  48.4  female\n3 0013      0   120 -22.5  female\n4 0014      0   103   6.72 female\n5 0015      0    97  89.7  female\n6 0016      0    84  40.6  female\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe prefer to use text labels for our categorical variables (e.g., male/female vs. 0/1)\n\nIt is often easier to work with categorical variables if they are converted to factors\n\nWe can also create a regressor for `sex` that is unit-weighted and centered and a mean-centered regressor for `ta`, but leave `bac` as is.     \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |> \n  mutate(sex = fct(sex, levels = c(\"female\", \"male\")), \n         sex_c = if_else(sex == \"female\", -.5, .5),\n         ta_c = ta - mean(ta)) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(data$sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"female\" \"male\"  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  subid   bac    ta    fps sex    sex_c  ta_c\n  <chr> <dbl> <dbl>  <dbl> <fct>  <dbl> <dbl>\n1 0011      0   208  19.5  female  -0.5  60.4\n2 0012      0   133  48.4  female  -0.5 -14.6\n3 0013      0   120 -22.5  female  -0.5 -27.6\n4 0014      0   103   6.72 female  -0.5 -44.6\n5 0015      0    97  89.7  female  -0.5 -50.6\n6 0016      0    84  40.6  female  -0.5 -63.6\n```\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\nLets fit our new three predictor model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(fps ~ bac + ta_c + sex_c, data = data)\nm |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   41.4      5.77        7.17 1.86e-10\n2 bac         -165.      84.6        -1.95 5.45e- 2\n3 ta_c           0.152    0.0316      4.82 5.70e- 6\n4 sex_c        -16.0      6.67       -2.40 1.84e- 2\n```\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\n## Univariate Statistics\n\nThe first step of case analysis is to review the univariate statistics\n\nAs before, we can use the `skim()` function from the `skimr` package to get a quick overview of our data, but we like to customize the function a bit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(skimr)\nmy_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),\n                                n_missing = ~sum(is.na(.), na.rm = TRUE)),\n                     numeric = sfl(p25 = NULL,\n                                   p75 = NULL,\n                                   hist = NULL),\n                     character = sfl(min = NULL, max = NULL),\n                     factor = sfl(ordered = NULL))\n```\n:::\n\n\n\n---------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> my_skim(-subid)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |data |\n|Number of rows           |96   |\n|Number of columns        |7    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|factor                   |1    |\n|numeric                  |5    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: factor**\n\n|skim_variable | n_complete| n_missing| n_unique|top_counts       |\n|:-------------|----------:|---------:|--------:|:----------------|\n|sex           |         96|         0|        2|fem: 48, mal: 48 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_complete| n_missing|   mean|     sd|      p0|    p50|   p100|\n|:-------------|----------:|---------:|------:|------:|-------:|------:|------:|\n|bac           |         96|         0|   0.06|   0.04|    0.00|   0.06|   0.14|\n|ta            |         96|         0| 147.61| 105.73|   10.00| 119.00| 445.00|\n|fps           |         96|         0|  32.19|  37.54|  -98.10|  19.46| 162.74|\n|sex_c         |         96|         0|   0.00|   0.50|   -0.50|   0.00|   0.50|\n|ta_c          |         96|         0|   0.00| 105.73| -137.61| -28.61| 297.39|\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\n## Univariate Plots\n\nHistograms, density plots, and rug plots are all useful/common visualization for numeric variables.   \n\nAs always, lets by DRY by writing functions (and add to your personal function library?)\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_univariate <- function(data, x_name, n_bins = 10) {\n  data |> \n    ggplot(aes(x = !!sym(x_name))) +\n    geom_histogram(aes(y = after_stat(density)),\n                   color = \"black\", fill = \"light grey\", bins = n_bins) +\n    geom_density() +\n    geom_rug(color = \"red\")\n}\n```\n:::\n\n\n\n:::{.callout-tip}\n# Programming Tip\nAll of the case analysis functions can be sourced at once from a script in my lab_support repository.\n`devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/case_analysis.R?raw=true\")\n:::\n\n---------------------------------------------------------------------------\n\nAnd now use it (making four plots together with `patchwork`)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n(plot_univariate(data, \"fps\") + plot_univariate(data, \"bac\")) / \n  (plot_univariate(data, \"ta\") + plot_univariate(data, \"sex_c\"))\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n----------------------------------------------------------------------\n\nBox plots can also be a useful visualization tool for numeric variables.   \n\nBoxplots display:   \n\n1. Median as line\n2. 25th %ile and 75th %ile as hinges\n3. Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25th %ile and 75th %ile)\n4. Outliers outside of 1.5 * IQR\n\nHere is a simple boxplot function\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_box <- function(data, x_name) {\n  data |> \n    ggplot(aes(x = 1, y = !!sym(x_name))) +\n    geom_boxplot() +\n    theme(axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.x = element_blank())\n}\n```\n:::\n\n\n\n---------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_box(data, \"fps\") + plot_box(data, \"bac\") + plot_box(data, \"ta\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------\n\nWe could also overlay a violin plot over the box plot to clearly see the shape of the distribution and tails.  Here is an example for `fps`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_box(data, \"fps\") + \n  geom_violin(aes(x = 1), fill = \"green\", color = NA, alpha = .4)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\n\n## Bivariate Correlations\n\nThe next step in case analysis is to review the bivariate correlations among the predictors and the outcome.  \n\nThis should be limited to the numeric variables (and only one of the `ta` variables)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  select(where(is.numeric), -ta) |>\n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        bac   fps sex_c  ta_c\nbac    1.00 -0.19  0.06 -0.02\nfps   -0.19  1.00 -0.23  0.44\nsex_c  0.06 -0.23  1.00 -0.01\nta_c  -0.02  0.44 -0.01  1.00\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------\n\n## Bivariate Plots between Outcome and Numeric Predictors\n\nScatterplots are the preferred visualization when both variables (i.e., the predictor and outcome) are numeric.     \n\nWe also can add a simple line and a LOWESS line (Locally Weighted Scatterplot Smoothing) to help us consider the shape of the relationship.\n\nHow about another function?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_bivariate <- function(data, x_name, y_name, loess = TRUE) {\n  plot <- data |> \n    ggplot(aes(x = !!sym(x_name), y = !!sym(y_name))) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n  \n  if(loess) {\n    plot <- plot + geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\")\n  }\n  \n  return(plot)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_bivariate(data, \"bac\", \"fps\") + plot_bivariate(data, \"ta_c\", \"fps\") + \n  plot_bivariate(data, \"sex_c\", \"fps\", loess = FALSE)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\nGGally is a great package for creating a matrix of bivariate plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGGally::ggpairs(data, columns = c(\"fps\", \"bac\", \"ta_c\", \"sex_c\"))\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n## Bivariate Plots (Categorical Predictor)\n\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables.\n\nLets look at an example of this with sex.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n data |>\n    ggplot(aes(x = sex, y = fps)) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\n## Leverage\n\nLeverage is a property of the predictors (DV is not considered for leverage analysis). An observation will have increased *leverage* on the results as its distance from the mean of all predictors increases. \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-26-1.png){width=768}\n:::\n:::\n\n\n\n::: {.callout-important}\n# Question\nWhich colored points have the most leverage in the 1 predictor cartoon data example above?\n:::\n\n:::{.fragment}\n[Blue, Yellow AND Green]{style=\"color:blue;\"}\n:::\n\n---------------------------------------------------------------------------\n\nHat values ($h_i$) provide an index of leverage.   \n\nIn the one predictor case:  \n\n- $h_i = \\frac{1}{N} + \\frac{(X_i- \\overline X)^2}{\\sum(X_j- \\overline X)^2}$ (for $j=1$ to $N$ in summation)  \n\nWith multiple predictors, $h_i$ measures the distance from the centroid (point of means) of the $X$s. Hat values are bounded between $\\frac{1}{N}$ and 1. \n\n---------------------------------------------------------------------------\n\nThe mean Hat value in a sample is a function of $P$ and $N$\n\n- Mean Hat value = $\\frac{P}{N}$\n\nThere are coarse rules of thumb for identifying high leverage points\n\n- $h_i > 3* \\overline h$ for small samples ($N < 100$)\n- $h_i > 2* \\overline h$ for large samples   \n\n\nBUT... \n\n- Do not blindly apply rules of thumb. \n- Hat values should be separated from distribution of $h_i$. \n- View a histogram of $h_i$.\n\n:::{.callout-note}\n# Note\nMahalanobis (Maha) distance = $(N - 1)(h_i - \\frac{1}{N})$.   \nSPSS reports centered leverage ($h - \\frac{1}{N}$).\n:::\n\n---------------------------------------------------------------------------\n\nHigh leverage values are not always bad. In fact, in some cases they are good. Must also consider if they are regression outliers.  \n\n::: {.callout-important}\n# Question\nWhy?\n:::\n\n:::{.fragment}\n\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$  \n\n$\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}$\n\n[High leverage points that are fit well by the model increase the difference between $\\text{SSE}_{\\text{mean-only}}$ and $\\text{SSE}_a$, which increases $R^2$.]{style=\"color:blue;\"}   \n\n[High leverage points that are fit well also increase variance for predictor. This reduces the SE for predictors and yields more power.]{style=\"color:blue;\"}   \n\n[Well fit, high leverage points do **not** alter $b$s.]{style=\"color:blue;\"}    \n:::\n\n---------------------------------------------------------------------------\n\nTo identify high leverage points, we can add `hatvalues()` to the dataframe to see which observations have the most leverage\n\nYou could also filter to values with high hats\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncut_hat <- 2 * mean(hatvalues(m))\n\ndata |> \n  mutate(hat = hatvalues(m)) |> \n  arrange(desc(hat)) |> \n  filter(hat > cut_hat) |> \n  print() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 8\n  subid    bac    ta   fps sex    sex_c  ta_c    hat\n  <chr>  <dbl> <dbl> <dbl> <fct>  <dbl> <dbl>  <dbl>\n1 0126  0        417  85.4 male     0.5  269. 0.111 \n2 2016  0.075    445  30.5 female  -0.5  297. 0.107 \n3 1024  0.0455   410  68.7 female  -0.5  262. 0.0853\n4 3123  0.139    287  14.7 male     0.5  139. 0.0845\n```\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\nBUT we should view these values within the distribution of all the hats for the dataset\n\nHere is a function to do this (add to your library?)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_leverage <- function(model) {\n  tibble(x = hatvalues(model)) |> \n    ggplot(aes(x = x)) +\n      geom_histogram(fill = \"light grey\", color = \"black\", bins = 10) +\n      geom_rug(color = \"blue\") +\n      labs(x = \"hat values\",\n           y = \"Frequency\") +\n      geom_vline(xintercept = 2 * mean(hatvalues(model)), color = \"yellow\", linewidth = .75) +\n      geom_vline(xintercept = 3 * mean(hatvalues(model)), color = \"red\", linewidth = .75) +\n      labs(subtitle = \"\") +\n      labs(subtitle = \"Red: 3 * mean(hat) for N < 100;  Yellow: 2 * mean(hat) for N >= 100\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_leverage(m)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## Regression Outliers\n\nAn observation that is not adequately fit by the regression model (i.e., falls very far from the prediction line).  \n\nIn essence, a regression outlier is a discrepant score with a large residual ($e_i$).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-30-1.png){width=768}\n:::\n:::\n\n\n\n::: {.callout-important}\n# Question\nWhich point(s) are regression outliers?\n:::\n\n:::{.fragment}\n[Yellow, Red, and Blue in descending order of magnitude]{style=\"color:blue;\"}\n:::\n\n---------------------------------------------------------------------------\n\nThere are multiple quantitative indicators to identify regression outliers\n\n- raw residuals ($e_i$) \n- standardized residuals ($e'_i$) \n- studentized residuals ($t'_i$) \n\nThe preferred index is the **studentized residual**.   \n\n- $t'_i = \\frac{e_i}{(SE_{e(-i)}*\\sqrt{(1-h_i)})}$    \n- $t'_i$ follows a t-distribution with $n-P-1$ degrees of freedom.   \n\n:::{.callout-note}\n# NOTE \nSPSS calls these Studentized Deleted Residuals. \n\nCohen calls these Externally Studentized Residual\n:::\n\n---------------------------------------------------------------------------\n    \nRegression outliers are always bad but they can have two different types of bad effects.   \n\n::: {.callout-important}\n# Question\nWhat are these two bad effects and why?\n:::\n\n:::{.fragment}\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$  \n\n$\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}$\n\n[Regression outliers increase $\\text{SSE}_a$ which decreases $R^2$. Decreased $R^2$ leads to increased SEs for $b$s.]{style=\"color:blue;\"}   \n\n[If outlier also has leverage can alter (increase or decrease) $b$s.]{style=\"color:blue;\"}\n:::\n\n---------------------------------------------------------------------------\n\nYou can add the studentized residuals to the dataframe to see which observations are regression outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  mutate(residuals = rstudent(m)) |> \n  arrange(desc(abs(residuals))) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  subid    bac    ta   fps sex    sex_c  ta_c residuals\n  <chr>  <dbl> <dbl> <dbl> <fct>  <dbl> <dbl>     <dbl>\n1 0125  0        110 -98.1 male     0.5 -37.6     -4.31\n2 2112  0.073    345 163.  male     0.5 197.       3.77\n3 2013  0.0625   264 -26.1 female  -0.5 116.      -2.67\n4 0013  0        120 -22.5 female  -0.5 -27.6     -2.16\n5 1023  0.0375   362 137.  female  -0.5 214.       1.96\n6 3011  0.105    282 110.  female  -0.5 134.       1.84\n```\n\n\n:::\n:::\n\n\n\n\\\n\nThe `car` package has a function `outlierTest()` that can be used to quickly identify regression outliers (residuals with a Bonferroni p-value < .05).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::outlierTest(m, cutoff = .05, labels = data$subid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rstudent unadjusted p-value Bonferroni p\n0125 -4.307609        0.000041622    0.0039958\n2112  3.765010        0.000294880    0.0283080\n```\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\nBut as with Leverage, we should view these values within the distribution of all the residuals for the dataset\n\nHere is a function to do this (add to your library?)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_residuals <- function(model){\n  res <-tibble(x = rstudent(model))\n  t_cut_point <- qt(p = .025/nrow(res), \n                    df = nrow(res) - length(coef(model)) - 1 - 2, \n                    lower.tail = FALSE)\n  res |> \n      ggplot(aes(x = x)) +\n        geom_histogram(fill = \"light grey\", color = \"black\", bins = 10) +\n        geom_rug(color = \"blue\") +\n        labs(x = \"Studentized Residuals\",\n           y = \"Frequency\") +\n        geom_vline(xintercept = c(-1, 1) * t_cut_point, color = \"red\", linewidth = .75) +\n        labs(subtitle = \"Red cut: Bonferroni corrected p-value < 0.05\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_residuals(m)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n\n## Influence\n\nAn observation is *influential* if it substantially alters the fitted regression model (i.e., the coefficients and/or intercept). Two commonly used assessment methods:    \n\n- Cooks distance\n- dfBetas\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-35-1.png){width=768}\n:::\n:::\n\n\n\n\n::: {.callout-important}\n# Question\nWhich point(s) have the most influence?\n::: \n\n:::{.fragment}\n[Yellow and Blue because they will alter $b_1$ due to their combined leverage and influence.]{style=\"color:blue;\"}\n:::\n\n## Cook's Distance\n\nCook’s distance ($\\text{D}_i$) provides a single summary statistic to index how much influence each score has on the overall model.    \n\nCooks distance is based on both the *outlierness* (standardized residual) and leverage characteristics of the observation.   \n\n- $\\text{D}_i = \\frac{e'^2_i}{P} * \\frac{h_i}{1-h_i}$   \n\n- $\\text{D}_i > \\frac{4}{N-P}$ has been proposed as a very liberal cutoff (identifies a lot of influential points). \n\n- $\\text{D}_i > \\text{qf}(.5, P, N-P)$ has also been employed as very conservative.   \n\n\nAs with the other case analysis metrics, identification of problematic scores should be considered in the context of the overall distribution of $\\text{D}_i$.\n\n---------------------------------------------------------------------------\n\nWe can add Cook's distance to the dataframe to see which observations have the most influence on the overall model.\n\nYou might also want to see the leverage and residuals for these points\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncut_cooks <- 4/(nrow(data) - length(coef(m)))\n\ndata |> \n  mutate(hats = hatvalues(m),\n         residuals = rstudent(m),\n         cooks = cooks.distance(m)) |> \n  arrange(desc(cooks)) |> \n  filter(cooks > cut_cooks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 10\n  subid    bac    ta   fps sex    sex_c  ta_c   hats residuals  cooks\n  <chr>  <dbl> <dbl> <dbl> <fct>  <dbl> <dbl>  <dbl>     <dbl>  <dbl>\n1 2112  0.073    345 163.  male     0.5 197.  0.0598      3.77 0.197 \n2 0125  0        110 -98.1 male     0.5 -37.6 0.0450     -4.31 0.184 \n3 2016  0.075    445  30.5 female  -0.5 297.  0.107      -1.70 0.0851\n4 1023  0.0375   362 137.  female  -0.5 214.  0.0650      1.96 0.0650\n5 2013  0.0625   264 -26.1 female  -0.5 116.  0.0339     -2.67 0.0588\n6 3011  0.105    282 110.  female  -0.5 134.  0.0560      1.84 0.0489\n7 0013  0        120 -22.5 female  -0.5 -27.6 0.0409     -2.16 0.0480\n```\n\n\n:::\n:::\n\n\n\n---------------------------------------------------------------------------\n\nHere is a function to view the distribution of cooks distance (add to your library?)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_cooks <- function(model){\n  cooks <-tibble(x = cooks.distance(model))\n  cut_cooks <- 4/(nrow(cooks) - length(coef(model)))\n  \n  cooks |> \n    ggplot(aes(x = x)) +\n      geom_histogram(fill = \"light grey\", color = \"black\", bins = 10) +\n      geom_rug(color = \"blue\") +\n      labs(x = \"Cooks Distances\",\n           y = \"Frequency\") +\n      geom_vline(xintercept = 4/(nrow(data) - length(coef(model)) - 1 - 1),\n                 color = c(\"red\"), linewidth = .75) +\n      labs(subtitle = \"Cut: 4/(N-P)\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_cooks(m)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n\n\n## dfBeta and dfBetas\n\n$dfBeta_{ij}$ is an index of how much each regression coefficient ($j= 0 – k$) would change if the $i^{th}$ score was deleted.   \n\n- $\\text{dfBeta}_{ij} = b_j - b_{j(-i)}$\n\ndfBetas (preferred) is the standardized form of the index.   \n\n- $dfBetas = \\frac{dfBeta}{SE_{b_{j(-i)}}}$ \n\n- $|dfBetas|>2$ may be problematic.   \n\n- $|dfBetas|> \\frac{2}{\\sqrt{N}}$ in larger samples (Belsley et al., 1980).   \n\n\nAs always, consider distribution with histogram! Also can visualize with added variable plot.\n\nProblem is there can be many dfBetas (a set for each predictor and intercept). Most helpful when there is one *critical/focal effect.*\n\n---------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_dfbetas <- function(model, x_name){\n   dfb <- as_tibble(dfbetas(model)) |> \n     select(matches(x_name))\n   colnames(dfb) <- \"x\" \n  \n  dfb |> \n    ggplot(aes(x = x)) +\n      geom_histogram(fill = \"light grey\", color = \"black\", bins = 10) +\n      geom_rug(color = \"blue\") +\n      labs(x = \"dfBetas\",\n           y = \"Frequency\") +\n      geom_vline(xintercept = c(-2, 2),\n                 color = c(\"red\"), linewidth = .75) +\n      labs(title = x_name, subtitle = \"Cut at |2|\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_dfbetas(m, \"bac\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-40-1.png){width=960}\n:::\n:::\n\n\n\n\n## Added Variable Plot\n\nAn added variable plot (AKA partial regression plot), can be used to detect observations that are influencing the relationship between a specific $X$ and $Y$ \n\n- Residuals for $Y$ are plotted against the residuals of a focal $X$ after regressing out the other independent variables. \n- This helps to isolate the effect of that specific variable, making it easier to see its unique contribution to the model.\n- It is also easier to see observations that impact that effect.\n\nThe steps to create an added variable plot typically involve:\n\n1. Regressing $Y$ on all $X$ except for the focal $X$ to obtain the residuals for $Y$\n2. Regressing the focal $X$ on all other $X$ to obtain the residuals for that focal $X$\n3. Plotting the residuals of $Y$ against the residuals of $X$\n\n---------------------------------------------------------------------------\n\nThe `avPlots() function in the `car` package can be used to create added variable plots. \n\nBut we have preferences for how we set all the function parameters so we will write our own function to wrap around it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_av <- function(data, model, label_name = \"subid\"){\n \n  # avPlots() wants a traditional dataframe with rownames set to subid or \n  # other label for points\n  data <- as.data.frame(data)\n  rownames(data) <- pull(data, matches(label_name))\n\n  car::avPlots(model, intercept = TRUE,\n              id = list(method = list(abs(residuals(model, type = \"pearson\"))), \n              n = 5, cex = 1, col = 'red', location = \"lr\"))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> plot_av(model = m, label_name = \"subid\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n\n\n## Impact on SEs\n\nIn addition to altering regression coefficients (and reducing $R^2$), problematic scores can increase the SEs (i.e., precision of estimation) of the regression coefficients.    \n\nCOVRATIO is an index that indicates how individual scores affect the overall precision of estimation (joint confidence region for set of coefficients) of the regression coefficients.\n\nObservations that decrease the precision of estimation have COVRATIOS < 1.0.\n\nBelsley et al., (1980) proposed a cut off of:    \n\n$\\text{COVRATIO}_i < |3* \\frac{P}{N} - 1|$\n\n---------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_covratio <- function(model){\n   cr <-tibble(x = covratio(model)) \n  \n  cr |> \n    ggplot(aes(x = x)) +\n      geom_histogram(fill = \"light grey\", color = \"black\", bins = 10) +\n      geom_rug(color = \"blue\") +\n      labs(x = \"Cov Ratio\",\n           y = \"Frequency\") +\n      geom_vline(xintercept = abs((3 * (length(coef(model)))/nrow(cr) - 1)),\n                 color = \"red\", linewidth = .75) +\n      labs(subtitle = \"Cut: 3*(P/N) - 1\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_covratio(m)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\n\n\n## Four Examples with Fake Data\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-revealjs/unnamed-chunk-46-1.png){width=960}\n:::\n:::\n\n\n\n\n## Enter the Real World\n\n::: {.callout-important}\n# Question\nSo what do you do?\n:::\n\n\n-----\n\n## Overall Impact of Problem Scores: Real Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   41.4      5.77        7.17 1.86e-10\n2 bac         -165.      84.6        -1.95 5.45e- 2\n3 ta_c           0.152    0.0316      4.82 5.70e- 6\n4 sex_c        -16.0      6.67       -2.40 1.84e- 2\n```\n\n\n:::\n:::\n\n\n\nSSE = 97756\n\n-----\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rm_outliers <- data |> \n  filter(!subid %in% c(\"0125\", \"2112\"))\n\nm_2 <- lm(fps ~ bac + ta_c + sex_c, data = data_rm_outliers) \n\ntidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   45.1      4.96        9.08 2.29e-14\n2 bac         -229.      72.4        -3.16 2.14e- 3\n3 ta_c           0.126    0.0273      4.61 1.35e- 5\n4 sex_c        -15.5      5.70       -2.72 7.90e- 3\n```\n\n\n:::\n:::\n\n\n\nSSE = 68263\n\n-----\n\n## What to Do\n\nWhat to worry about\n\n- Don’t worry about leverage alone\n- Worry about model outliers always\n- Worry about influence (overall model or predictors by field)\n\n\nWhat to do\n\n- Drop, retain, or bring model outliers to the fence?\n- Drop or retain influential participants?\n  - Report both ways (in olden day…)\n  - Get with the program and pre-register instead (what you worry about, how you define it, and what you do)!",
    "supporting": [
      "07_case_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}