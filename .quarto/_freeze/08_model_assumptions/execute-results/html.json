{
  "hash": "07ac70aa8b34fcc713adbb017907b3e4",
  "result": {
    "engine": "knitr",
    "markdown": "--- \noutput: html_document \neditor_options:  \n  chunk_output_type: console\n--- \n\n\n \n\n# Unit 8: Dealing with Messy Data II - Model Assumptions\n\n\n\n\n\n\n\n## Goals of Unit\n\n- Understand 5 assumptions of GLMs.\n\n- Understand consequences of assumption violations.\n  - Inefficient standard errors (low power, type II errors).\n  - Inaccurate standard errors (incorrect statistical tests, type I errors).\n  - Inaccurate parameter estimates.\n  \n- Learn how to detect violations (statistical tests and visual diagnosis).\n\n- Recognize the range of options available when violations are detected. Implementation of these options will be covered over the semester.\n\n- Examination of assumptions will further inform you about your data.\n\n--------------------------------------------------------------------------------\n\n## Assessment of Assumptions for Sig. Tests\n\nAll GLM procedures commonly make the **5 assumptions below**. \n\nWhen these assumptions are met, OLS regression coefficients are MVUE (Minimum Variance Unbiased Estimators) and BLUE (Best Linear Unbiased Estimators). \n\nWith the exception of $\\#1$, these assumptions are expressed (and assessed) with respect to the residuals around the prediction line.\n\n1. **Exact X:** The IVs are assumed to be known exactly (i.e., without measurement error).\n2. **Independence:**  Residuals are independently distributed (prob. of obtaining a specific observation does not depend on other observations).\n3. **Normality:**  All residual distributions are normally distributed.\n4. **Constant variance:**  All residual distributions have a constant variance.\n5. **Linearity:**  All residual distributions (i.e., for each $\\hat{Y}$) are assumed to have means equal to zero.\n\n-------------------------------------------------------------------------------\n\n## Exact X\n\n\n\n\n\n\n\n\n\nViolations of the Exact X assumption lead to biased (i.e., inaccurate) estimates of regression coefficients.  \n\nViolations are caused by problems with reliability of measurement of your predictors.\n\n::: {.callout-important}\n# Question\n\nIn simple, bivariate regression, how will reducing reliability affect the regression model?\n:::\n\n:::{.fragment}\n[It will reduce $b_1$. We will underestimate the strength of relationship between $X$ and $Y$.]{style=\"color:blue;\"}    \n\n[In multiple predictor models, the bias can be either positive or negative based on the nature of the correlations among the predictors. **Use reliable variables!**.]{style=\"color:blue;\"}\n\n\n\n\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n::: {.callout-important}\n# Question\n\nWhat are the implications of unreliable $X$ for the use of covariates to *control* variables?\n:::\n\n:::{.fragment}\n[Covariates only *control* from the construct they measure to the degree that they are reliable (and valid) measures of that construct.]{style=\"color:blue;\"}   \n\n[Analysis that rely on unreliable covariates are not controlling the variance for the construct well.]{style=\"color:blue;\"}   \n:::\n\n--------------------------------------------------------------------------------\n\n## Independence\n\nViolations of the independence of residuals assumption can compromise the validity of our statistical tests (inaccurate standard errors).   \n\nViolations of residuals independence is a function of the research design caused by repeated measures on the same individual or related individuals/observations (participants in same family, school, etc).  \n\nOften difficult to detect in data but clear from research design.\n\nCan be fixed by a variety of approaches including repeated measures analyses or multi-level, mixed effects, and/or hierarchical linear models (**next semester**).\n\n--------------------------------------------------------------------------------\n\n## Assessment of Residuals: General Issues\n\nRemaining three assumptions (Normality distributed residuals with a mean of 0 and constant variance) can be assessed via examination of the residuals. \n\nUse of Graphical Methods is emphasized.\n\nStatistical tests of assumptions exist but should be used cautiously.\n\nAssessment of assumptions about residuals is an inexact science: Conclusions are tentative.\n\nThe process of examining residuals will increase your understanding of your data.\n\n\t- May suggest transformations of your data.\n\t- May suggest alternative analytic strategies.\n\t- Will increase your confidence in your conclusions.\n\n--------------------------------------------------------------------------------\n\n## Residuals by Predicted Y: Normal M = 0, S = Constant\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnorm_tbl <- tibble(y_hat = rep(1:25, 1000),\n                   e = rnorm(25000, mean = 0, sd = 1))\n\nnorm_tbl |>\n  ggplot(aes(x = y_hat,\n             y = e)) +\n    geom_jitter(alpha = .4, width = .75, height = 0, size = .5) +\n    geom_hline(yintercept = 0, color = \"blue\", linewidth = 1) +\n    geom_vline(xintercept = 10, color = \"red\", linewidth = 1) +\n    geom_vline(xintercept = 15, color = \"red\", linewidth = 1) +\n    geom_vline(xintercept = 20, color = \"red\", linewidth = 1) +\n    scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25))\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nhist_10 <- norm_tbl |> \n  filter(y_hat == 10) |> \n  ggplot(aes(x = e)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"y_hat = 10\")\n\nhist_15 <- norm_tbl |> \n  filter(y_hat == 15) |> \n  ggplot(aes(x = e)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"y_hat = 15\")\n\nhist_20 <- norm_tbl |> \n  filter(y_hat == 20) |> \n  ggplot(aes(x = e)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", \n                 fill = \"light grey\", bins = 10) +\n  geom_density() +\n  labs(title = \"y_hat = 20\")\n\nhist_10 + hist_15 + hist_20\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Normally Distributed Errors\n\nThe errors for each $\\hat{Y}$ are assumed to be normally distributed. Normally distributed errors are required for OLS regression coefficients to be MVUE but not BLUE.   \n\nCentral limit theory indicates that even with non-normal errors, **significance tests and confidence intervals are approximately correct with large $N$.**\n\nCoefficients are still best unbiased efficient estimators among linear solutions (i.e., BLUE) but **more efficient non-linear solutions** may exist (e.g., Generalized Linear Models such as Poisson regression for thick tailed distributions). \n\n**Mean may not be best measure of center** of a highly skewed distribution.\n\nMultimodal error distributions suggest the **omission of one or more categorical variables** that divide the data into groups.\n\nTransformations may correct shape of residuals (*Unit 9*).\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npath_data <- \"data_lecture\"\n\ndata <- read_csv(here::here(path_data, \"07_three_predictors_fps.csv\"), \n                 show_col_types = FALSE) |> \n   mutate(sex_c = if_else(sex == \"female\", -.5, .5))\n```\n:::\n\n\n\nLets refit our last model from the previous unit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rm_outliers <- data |> \n  filter(!subid %in% c(\"0125\", \"2112\"))\n\nm_2 <- lm(fps ~ bac + ta + sex_c, data = data_rm_outliers) \n```\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n### Density plot of residuals\n\nWe can use a density plot to plot the model's residuals against a normal distribution.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_density(aes(x = value), data = enframe(rstudent(m_2), name = NULL)) +\n  labs(title = \"Density Plot to Assess Normality of Residuals\",\n       x = \"Studentized Residual\") +\n  geom_line(aes(x = x, y = y), \n            data = tibble(x = seq(-4, 4, length.out = 100),\n                          y = dnorm(seq(-4, 4, length.out = 100), \n                                    mean = 0, sd = sd(rstudent(m_2)))), \n            linetype = \"dashed\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n### Quantile-Quantile Plot\nBetter still We can use `qqplot()` from the `car` package to assess normality of residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::qqPlot(m_2, id = FALSE, simulate = TRUE,\n            main = \"QQ Plot to Assess Normality\", \n            ylab = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nHere are three examples of qq plots for normal distributions of small samples (N = 100)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(102030)\ntibble(x = rnorm(100, mean=0, sd=1)) |> \n  pull(x) |>\n  car::qqPlot(id = FALSE, \n              main = \"QQ Plot for Random Normal (N = 100)\", \n       ylab = \"n1\") \n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = rnorm(100, mean=0, sd=1)) |> \n  pull(x) |>\n  car::qqPlot(id = FALSE, \n              main = \"QQ Plot for Random Normal (N = 100)\", \n              ylab = \"n2\")\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-11-2.png){width=960}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = rnorm(100, mean=0, sd=1)) |> \n  pull(x) |>\n  car::qqPlot(id = FALSE, \n              main = \"QQ Plot for Random Normal (N = 100)\", \n              ylab = \"n3\")\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-11-3.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nAnd here for the chi-squared (positive skewed)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\nAnd the t-distributions (which is heavy tailed)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Constant Variance\n\nThe errors for each $\\hat{Y}$ are assumed have a constant variance (homoscedasticity). This is necessary for the OLS estimated coefficients to be BLUE.\n\nIf the errors are heteroscedastic, the **coefficients remain unbiased** but the **efficiency (precision of estimation) is impaired** and the **coefficient SEs become inaccurate**. The degree of the problem depends on severity of violation and sample size.\n\nRough rule is that estimation is seriously degraded if the ratio of largest to smallest variance is 10 or greater (or more conservatively, 4 or greater)\n\n1.  Transformations may fix this issue (*next unit*).\n\n2.  Weighted Least Squares provides an alternative to estimation when heteroscedasticity exists (maybe next semester?).\n\n3.  Corrections also exist for SEs when errors are heteroscedastic (more on this in a moment).\n\n--------------------------------------------------------------------------------\n\n### Residual by Predicted Values\n\nCan look at plot of studentized residuals vs. predicted values\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = predict(m_2),\n       y = rstudent(m_2)) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"blue\", linewidth = 1) +\n  labs(title = \"Studentized Residuals vs.  Predicted Values\",\n       x = \"Predicted Values\",\n       y = \"Studentized Residuals\")\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n### Spread Level Plot\n\nA spread level plot is a plot of the log(abs(studentized residuals) vs. log(predicted values). \n\nPredicted values must all be positive to take log.  Can use `start` to handle negative predicted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin(predict(m_2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -6.428208\n```\n\n\n:::\n\n```{.r .cell-code}\nstart <- 7\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = log(predict(m_2) + start), # shift to positive values\n       y = log(abs(rstudent(m_2)))) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = .6) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE, \n              color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Spread-Level Plot for m_2\",\n       x = \"Predicted Values\",\n       y = \"|Studentized Residuals|\") +\n  scale_x_continuous(trans = \"exp\", \n                     breaks = scales::trans_breaks(\"exp\", function(x) log(x)),\n                     labels = scales::trans_format(\"exp\", \n                                                   format = scales::math_format(.x))) +\n  scale_y_continuous(trans = \"exp\", \n                     breaks = scales::trans_breaks(\"exp\", function(y) log(y)),\n                     labels = scales::trans_format(\"exp\", scales::math_format(.x)))\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n$1-b$ (from the regression line) is the suggested power transformation for $Y$ to stabilize variance.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - (lm(log(abs(rstudent(m_2))) ~ log(predict(m_2) + start))$coefficients[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlog(predict(m_2) + start) \n                0.5226341 \n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip}\nsee also: `car::spreadLevelPlot()`\n:::\n\n--------------------------------------------------------------------------------\n\n### Statistical Test\n\nTwo groups independently developed test for constant variance:\n\n- Breusch,T. S. and Pagan,A. R. (1979) A simple test for heteroscedasticity and random coefficient variation. *Econometrica*, 47,  1287-1294. \n- Cook & Weisberg (1983)^[Cook,R. D. and Weisberg,S. (1983) Diagnostics for heteroscedasticity in regression. *Biometrika*, 70, 1-10. \n\nAvailable as `ncvTest()` in `car` package. \n\nDo not use it blindly. \n\n- All statistical tests are designed to be sensitive to specific types of violations. \n- May miss other types of violations.\n- May also provide false positive due to other aspects of the distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::ncvTest(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 21.89294, Df = 1, p = 0.0000028829\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n### Correcting SEs\n\nStandard errors are inaccurate when variance of residuals is not constant. \n\nA procedure to provide White (1980) corrected SEs is described in Fox (2008), chapter 12, pp 275-276. \n\nSee: \n\n- White,H. (1980) A heterskedastic consistent covariance matrix estimator and a direct test of heteroskedasticity. *Econometrica* 48, 817–838.\n- Long,J.S. and Ervin,L.H. (2000) Using heteroscedasity consistent standard errors in the linear regression model. *The American Statistician* 54, 217–224.\n\nUncorrected Tests of Coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   26.6      6.44        4.13 0.0000825\n2 bac         -229.      72.4        -3.16 0.00214  \n3 ta             0.126    0.0273      4.61 0.0000135\n4 sex_c        -15.5      5.70       -2.72 0.00790  \n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWhite (1980) Heteroscedasticity-corrected SEs and Tests\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Heteroscedasticity-Corrected Covariance Matrices (hccm)\ncorrected_ses <- sqrt(diag(car::hccm(m_2)))\n\ntidy(m_2) |> \n  select(term, estimate) |> \n  add_column(std.error = corrected_ses) |> \n  mutate(statistic = estimate/std.error,\n         p.value = 2 * (pt(abs(statistic), \n                         df = m_2$df.residual, \n                         lower.tail=FALSE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   26.6      6.90        3.85 0.000220 \n2 bac         -229.      72.1        -3.18 0.00204  \n3 ta             0.126    0.0304      4.13 0.0000799\n4 sex_c        -15.5      5.70       -2.72 0.00791  \n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Linearity: Component + Residuals Plots\n\nIf Linearity assumption is not met, coefficients are biased.  \n\n- Plot partial residual ($e_{i(j)} = e_i + b_jX_{ij}$) by each predictor.  \n\n- Can include factors but can not include interactions with factors. Code regressors manually. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::crPlots(m_2, ask = FALSE)\n```\n\n::: {.cell-output-display}\n![](08_model_assumptions_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Global Test of Model Assumptions\n\nPena and Slate (2006) validated a global test of linear model assumptions.\n\n- Pena,E.A. and Slate,E.H. (2006). Global validation of linear model assumptions, *Journal of the American Statistical Association*, 101, 341-354.\n\nProvided by `gvlma()` using `gvlma` package\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngvlma::gvlma(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = fps ~ bac + ta + sex_c, data = data_rm_outliers)\n\nCoefficients:\n(Intercept)          bac           ta        sex_c  \n    26.5528    -228.8721       0.1256     -15.4874  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = m_2) \n\n                    Value  p-value                   Decision\nGlobal Stat        9.8263 0.043458 Assumptions NOT satisfied!\nSkewness           0.2921 0.588886    Assumptions acceptable.\nKurtosis           1.0393 0.307987    Assumptions acceptable.\nLink Function      0.1898 0.663091    Assumptions acceptable.\nHeteroscedasticity 8.3051 0.003953 Assumptions NOT satisfied!\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Some Solutions\n\n- Power transformations (*next unit*) are very useful for correcting problems with normality, constant, variance, and linearity of errors.\n\n- Polynomial regression (*710*) is useful when you have quadratic, cubic, etc. effects of $X$s on $Y$.\n\n- Generalized linear models (e.g., Logistic regression; *last unit*) are also available.\n\n\n--------------------------------------------------------------------------------\n\n## Summary of Violation Consequences and Solutions\n\n1. **Exact X**  \n\n    - Inaccurate (biased) parameter estimates.\n    - Use reliable measures of $X$s, use SEM with latent variables.\n\n2. **Independence**\n\n    - Inaccurate standard errors.\n    - Use repeated measures, use multi-level models.\n\n3. **Normally distributed errors**\n\n    - Inefficient standard errors.\n    - Consider omitted variables, use transformations, use generalized linear models.\n\n4. **Constant variance for errors**\n\n    - Inaccurate and inefficient standard errors.\n    - Use SE corrections (but still inefficient), use transformations, use weighted least squares.\n      \n5. **Linearity** (Error distributions all have mean of 0)    \n\n    - Inaccurate (biased) parameter estimates.\n    - Use transformations, use polynomial regression, use generalized linear models.",
    "supporting": [
      "08_model_assumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}