{
  "hash": "56ce8011494edb7fda1adee09be6268d",
  "result": {
    "engine": "knitr",
    "markdown": "# Unit 6: Inferences about two predictors (multiple regression without interaction)\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Multiple Regression 2+ Predictors\n\nIn this unit, we will consider the case of multiple predictors.\n\n- Consider how the concepts we have discussed so far generalize to the 2 predictor (3 parameter) model.  \n- We will start with 2 quantitative predictors example. Will continue with 1 quantitative and 1 dichotomous predictor example.  \n- Learn how to quantify, test, and interpret ‘partial’ effects:\n    - $b_j$\n    - $\\Delta R^2, \\eta_p^2$\n- Multicollinearity\n- Text, table and figure descriptions of results\n- Generalization to >2 predictors is straightforward.\n\n--------------------------------------------------------------------------------\n\n## Benefits of Multiple Predictors\n\n1. **Statistical power**:  Goal is to increase power to test focal predictor’s effect on DV by adding it to model that contains additional known predictors of DV.\n\n2. **Additional explanatory power**: Goal is to demonstrate that focal predictor adds explanatory power above and beyond other predictor(s) [Unique effect controlling for other predictors].\n\n3. **Efficiency**:  Can test focal effects of two predictors in one study (each benefiting from increased power per point 1).\n\n4. **Mediation**: We have identified a known cause of a DV.  We add a new focal predictor to test if the effect of our known causal IV on the DV is mediated by our focal predictor (i.e., identify “mechanism” of IV effect).\n\n5. **Better prediction**: If we are using the model to predict the DV for individuals, all real life DVs are the result of multiple causes.  Including them will improve prediction.\n\n--------------------------------------------------------------------------------\n\n## Alcohol and Stress Response Dampening (SRD)\n\nTest for Alcohol *Stress response dampening*   \n\nManipulate BAC (0.00% - 0.15%)  \n\nStressor Task (threat of unpredictable shock)  \n\nMeasure Stress Response (Fear potentiated startle)  \n\n--------------------------------------------------------------------------------\n\n## Two Parameter (1 Predictor) Model\n\nSet up environment and load data\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse) \nlibrary(broom)\nlibrary(patchwork)\ntheme_set(theme_classic()) \n\npath_data <- \"data_lecture\" \n\ndata <- read_csv(here::here(path_data, \"06_two_predictors_fps.csv\"),\n                 show_col_types = FALSE) \n```\n:::\n\n\n\nDefine `my_skim()` function\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(skimr)\nmy_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),\n                                n_missing = ~sum(is.na(.), na.rm = TRUE)),\n                     numeric = sfl(p25 = NULL,\n                                   p75 = NULL,\n                                   hist = NULL),\n                     character = sfl(min = NULL, max = NULL),\n                     factor = sfl(ordered = NULL))\n```\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nSkim the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> my_skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |data |\n|Number of rows           |96   |\n|Number of columns        |4    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |1    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_complete| n_missing| empty| n_unique| whitespace|\n|:-------------|----------:|---------:|-----:|--------:|----------:|\n|subid         |         96|         0|     0|       96|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_complete| n_missing|   mean|     sd|    p0|    p50|   p100|\n|:-------------|----------:|---------:|------:|------:|-----:|------:|------:|\n|bac           |         96|         0|   0.06|   0.04|   0.0|   0.06|   0.14|\n|ta            |         96|         0| 147.61| 105.73|  10.0| 119.00| 445.00|\n|fps           |         96|         0|  32.19|  37.54| -98.1|  19.46| 162.74|\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe can also write some functions to help us with calculations and effect sizes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate sum of squared errors for a model\nsse <- function(model) {\n  sum(residuals(model)^2)\n}\n\n# calculate pre for a model comparison\npre <- function(compact, augmented) {\n  (sse(compact) - sse(augmented)) / sse(compact)\n}\n```\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nNow, we fit the two parameter model to our data again (same as previous unit)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_2 <- lm(fps ~ 1 + bac, data = data)\n\nm_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDescribe the interpretation of $b_1$ (coefficient for BAC) and its significance test.\n:::\n\n:::{.fragment}\n[$b_1$ *describes* the relationship between BAC and FPS in the units of each measure. FPS will decrease by 184 µV for every 1% increase in BAC (It will decrease by 1.84µV for every .01 increase in BAC).]{style=\"color:blue;\"}\n\n[The significance test for $\\beta_1$ tests the null hypothesis that the population relationship between BAC and FPS is 0 (i.e., $\\beta_1$ = 0, no relationship). We fail to reject this $H_0$. Conclude that alcohol does not affect FPS.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n## Testing Inferences about $\\beta_1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\n$H_0: \\beta_1 = 0$  \n$H_a: \\beta_1 \\neq 0$  \n\nWhat could we change about the sampling distribution that would make this $b_1$ be less probable given $H_0$ so that we reject the Null?\n:::\n\n:::{.fragment}\n[If the standard deviation of the sampling distribution (its standard error) was smaller so that the distribution was narrower, $b_1$ would be less probable given $H_0$.]{style=\"color:blue;\"}`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual)) |> \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  labs(title = \"Sampling Distribution for b1\")\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-8-1.png){width=768}\n:::\n:::\n\n\n:::\n\n## Standard Errors of GLM Coefficients\n\nThe formula for the standard error for a coefficient $b_j$ in **multiple** regression (i.e., GLM with more than one regressor) is:  \n\n$SE_{bj} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R^2_j)}}$  \n\n- $R^2_j$ = variance in $X_j$ accounted for by all other regressors ($X$s) in the model (i.e., how redundant is $X_{a_j}$ in model?).\n\n- This is literally: predict $X_j$ as $Y$ with all other regressors as $X$s.  \n\n:::{.callout-note}\n# Note \nFormula for the standard error for $b_0$ is different than for $b_j$.  For the two parameter model $SE_{b0}$ is (need matrix notation for $P > 2$):  \n\n$\\sqrt {\\frac{SSE}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{X^2}{(N-1)s_x^2}}$\n:::\n\n--------------------------------------------------------------------------------\n\n## SE for b_j and R-Squared\n\n$SE_{b_j}= \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{(N-P)}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\nIf we increase $R_y^2$, we would decrease the SE for our regression coefficient.  \n\n--------------------------------------------------------------------------------\n\n## Model Comparison: Testing Inferences about Beta_1\n\n$H_0: \\beta_1 = 0$\n$H_a: \\beta_1 \\neq 0$  \n\n:::{.callout-important}\n# Question\nWhat two models are you comparing when you test hypotheses about $\\beta_1$ for BAC? Describe the logic.\n:::\n\n:::{.fragment}\n[Compact model:]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_c <- lm(fps ~ 1, data = data)\n```\n:::\n\n\n\n- $\\hat{FPS}_i = \\beta_0+0*BAC_i$\n- $SSE_c=$ 133888, $P_c=1$     \n\n[Augmented model:]{style=\"color:blue;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_a <- lm(fps ~ 1 + bac, data = data)\n```\n:::\n\n\n\n- $\\hat{FPS}_i = \\beta_0+\\beta_1*BAC_i$    \n- $SSE_a=$ 128837, $P_a=2$   \n\n$F(P_a-P_c, N-P_a) = \\frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m_c, m_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: fps ~ 1\nModel 2: fps ~ 1 + bac\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     95 133888                              \n2     94 128837  1    5051.2 3.6854 0.05792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n$F(P_a-P_c, N-P_a) = \\frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$  \n\n:::{.callout-important}\n# Question\nWhat could you change from this model comparison perspective to increase $F$ and probability to reject the $H_0$ about $\\beta_1$?\n:::\n\n:::{.fragment}\n[Make $SSE_a$ smaller by explaining more variance in $Y_i$.]{style=\"color:blue;\"}\n\n[Of course, $R^2 = \\frac{SSE_{mean-only} - SSE_a}{SSE_{mean-only}}$]{style=\"color:blue;\"}\n\n[If you decrease $SSE_a$ or increase model $R^2$, you will have more power to reject $H_0$ regarding parameter estimates.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n## Two Parameter (1 Regressor) Model (Continued)\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat can we do analytically to decrease SSE (increase model $R^2$) in any model?\n:::\n\n:::{.fragment}\n[Include another regressor ($X$) in the model that accounts for additional variance in $Y$ (reduces SSE).]{style=\"color:blue;\"}\n\n- [When we add $X$s to the model to reduce error or increase $R^2$, rather than to explicitly test questions about these $X$s, we call these $X$s covariates.]{style=\"color:blue;\"}\n\n- [Ideally, covariates should be orthogonal (uncorrelated) with the other $X$s.]{style=\"color:blue;\"}\n\n- [In this experiment, I could have measured another predictor of stress response, Trait Anxiety (TA). TA might be expected to be a robust predictor of FPS. It also should be uncorrelated with BAC because I manipulated BAC.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\nHere are the distributions of $Y$ and our two potential $X$s\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_fps <- data |> \n  ggplot(aes(x = fps)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +\n  geom_rug(color = \"red\")\n\nplot_bac <- data |> \n  ggplot(aes(x = bac)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0,\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\")\n\nplot_ta <- data |> \n  ggplot(aes(x = ta)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\nplot_fps + plot_bac + plot_ta\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe can look at correlations among these variables using `corr.test()` from the `psych` package (you may need to install that package)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- data |> \n  select(where(is.numeric)) |> \n           psych::corr.test()\n\n# corr.test() is not tidy and does not return a dataframe.  Annoying!!\nr$r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            bac          ta        fps\nbac  1.00000000 -0.01720719 -0.1942346\nta  -0.01720719  1.00000000  0.4350457\nfps -0.19423458  0.43504572  1.0000000\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe can also visualize these bivariate relationships using scatterplots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = ta, y = fps)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = bac, y = ta)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n\n## The Two Predictor and General Linear Models\n\nThe two (and $k$) are simple generalizations of the models you have already learned\n\n- Simply add more $X$s to the model \n- Estimate parameters for these $X$s\n\nDATA = MODEL + ERROR\n\n**Two Predictor Model for Sample Data**  \n\n- $Y_i=b_0+b_1X_1+b_2X_2+e_i$  \n\n- $\\hat{Y_i}=b_0+b_1X_1+b_2X_2$  \n\n\n**$k$ Predictor Model for Sample Data**   \n\n- $Y_i=b_0+b_1X_1+...+b_kX_k+e_i$  \n\n- $\\hat{Y_i}=b_0+b_1X_1+...+b_kX_k$  \n\n--------------------------------------------------------------------------------\n\n## Testing BAC in a Three Parameter Model (2 Predictors)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_3 <- lm(fps ~ 1 + bac + ta, data = data)\n\nm_3 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n$\\hat{FPS} = 19.4 + -177 * BAC + 0.2 * TA$\n\n:::{.callout-important}\n# Question\nWhat parameter estimate is used to test our research question about the effect of BAC? What are our $H_0$ and $H_a$ for the associated population parameter?\n:::\n\n:::{.fragment}\n[$H_0:\\beta_1=0; H_a: \\beta_1 \\neq 0$]{style=\"color:blue;\"}\n\n[We use $b_1$ (-177) to test our hypothesis about the population effect of BAC ($\\beta_1$).]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDescribe conclusion and logic of the test of $H_0:\\beta_1=0$ from sampling distribution perspective.\n:::\n\n:::{.fragment}\n[If $H_0$ is true, we expect a sampling distribution for $b_1$ to have a mean of 0 and an SE of 86.6 (red curve below).]{style=\"color:blue;\"}\n\n[A sample $b_1$ = -177.0 is unlikely (about 2 standard deviations below mean; p = .0437). Therefore, we reject our $H_0$ and conclude that $\\beta_1 \\neq 0$.]{style=\"color:blue;\"}\n\n[Conclusion is that BAC affects FPS.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(tidy(m_3), term == \"bac\")$std.error, m_3$df.residual)) |> \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line(color = \"red\") +\n  geom_vline(xintercept = subset(tidy(m_3), term == \"bac\")$estimate, \n             color = \"blue\") +\n  geom_vline(xintercept = -1 * subset(tidy(m_3), term == \"bac\")$estimate, \n             color = \"green\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nDescribe conclusion and logic of the test of $H_0:\\beta_1=0$ from model comparison perspective.\n:::\n\n:::{.fragment}\n[$H_0: \\beta_1 = 0; H_a: \\beta_1 \\neq 0$]{style=\"color:blue;\"}\n\n[**Compact Model:**]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_c <- lm (fps ~ 1 + ta, data = data)\n```\n:::\n\n\n\n- $\\hat{FPS}=\\beta_0 + 0*BAC+\\beta_2*TA$    \n- $SSE_c =$ 108547.9, $P_c = 2$   \n\n[**Augmented Model:**]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_a <- lm(fps ~ 1 + bac + ta, data = data)\n```\n:::\n\n\n\n- $\\hat{FPS}=\\beta_0 + \\beta_1*BAC+\\beta_2*TA$    \n- $SSE_a =$ 103877.2, $P_a = 3$ \n\n$F(P_a - P_c, N - P_a) = \\frac{(SSE_c - SSE_a)/(P_a-P_c)}{SSE_a/(N-P_a)}$   \n$F(1, 93) =$ 4.18, $p =$ 0.0442\n:::\n\n--------------------------------------------------------------------------------\n\n**Two parameter model test of BAC**   \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 <- lm(fps ~ 1 + bac, data = data)\n\ntidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n$SSE =$ 128837.1\n\n**Three parameter model test of BAC**   \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 <- lm(fps ~ 1 + bac + ta, data = data)\ntidy(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n$SSE =$ 103877.2\n\n:::{.callout-important}\n# Question\nWhat changed about test of $\\beta_1$ (BAC effect) and why?\n:::\n\n:::{.fragment}\n[**Two parameter model test of BAC**]{style=\"color:blue;\"}    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_2) |> pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03772707\n```\n\n\n:::\n:::\n\n\n\n[**Three parameter model test of BAC**]{style=\"color:blue;\"}   \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_3) |> pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2241502\n```\n\n\n:::\n:::\n\n\n\n[SSE decreased and $R^2$ increased.]{style=\"color:blue;\"}\n\n[As a result, SE for BAC decreased leading to a larger t-statistic]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n## Standard Error of Partial Regression Coefficient ($b_j$)\n\n$t(N-P) = \\frac{b_j - 0}{SE_{b_j}}$  \n\n$SE_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n:::{.callout-important}\n# Question\nWhat happens to $SE_{b_j}$ as model $R^2$ ($R^2_y$) increases (holding other factors constant)?\n:::\n\n:::{.fragment}\n[$SE_{b_j}$ decreases as model $R^2$ increases. In other words, the sampling distribution get narrower.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat happens to significance test of $b_j$ as $\\text{SE}_{b_j}$ decreases (holding other factors constant)?\n:::\n\n:::{.fragment}\n[$t$ increases and associated p-value decreases (More Power!).]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n## Sampling Distributions and Power\n\n$t(N-P) = \\frac{b_j - 0}{SE_{b_j}}$  \n\n\n**Two parameter model test of BAC**   \n\n- $t(96-2) = \\frac{-184 - 0}{95.9}$    \n- $t(94) = -1.92$  \n- $p = .0579$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n**Three parameter model test of BAC**   \n\n- $t(96-3) = \\frac{-177 - 0}{86.6}$    \n- $t(93) = -2.04$  \n- $p = .0437$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntidy(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndistr_data <- tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_3), term == \"bac\")$std.error, m_3$df.residual),\n       group = \"3 parameter model\") |> \n  bind_rows(tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual),\n       group = \"2 parameter model\"))\n\ndistr_data |> \n  ggplot(aes(x = b1, y = probability, color = group)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  geom_vline(xintercept = subset(broom::tidy(m_3), term == \"bac\")$estimate, \n             color = \"blue\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Sampling Distributions and Precision\n\n$CI_b=b \\pm t(\\alpha; N-P)*SE_b$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %    97.5 %\n(Intercept)   29.45597 55.457721\nbac         -374.49261  6.308724\n```\n\n\n:::\n:::\n\n\n\n$\\Delta$ 380.801\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %     97.5 %\n(Intercept)    4.22130089 34.6411333\nbac         -348.98099457 -5.1177113\nta             0.08891558  0.2177329\n```\n\n\n:::\n:::\n\n\n\n$\\Delta$ 343.863\n\n## Standard Error of Partial Regression Coefficient ($b_j$; Continued)\n\n$t(N-P) = \\frac{b_j - 0}{SE_{b_j}}$  \n\n$SE_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_Y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n$R^2_j$ = variance in $X_j$ accounted for by all other predictors in model (i.e., how redundant is $X_j$ in model?).  \n\n--------------------------------------------------------------------------------\n\n$SE_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_Y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n:::{.callout-important}\n# Question\nWhat other factors affect SE for regression coefficients and how?\n:::\n\n:::{.fragment}\n- [Increasing $N$ decreases SE (increases power).]{style=\"color:blue;\"}\n\n- [Increasing $P$ increases SE (decreases power)]{style=\"color:blue;\"}\n\n- [Increasing $s_y$ increases SE (decreases power).]{style=\"color:blue;\"}\n\n- [Increasing $s_j$ decreases SE (increases power).]{style=\"color:blue;\"}\n\n- [Increasing $R_j^2$ increases SE (decreases power).]{style=\"color:blue;\"}\n:::\n\n## Power and SSE in Two and Three Parameter Models\n\n**Two parameter model test of BAC**   \n\n- Compact Model: \n  - $\\hat{FPS}= 32.2 + 0*{BAC}$   \n  - $SSE_c =$ 133888.3, $P_c$ = 1  \n\n- Augmented Model: \n  - $\\hat{FPS}= 42.5 + -184.1 *BAC$   \n  - $SSE_a =$ 128837.1, $P_a$ = 2  \n\n\n**Three parameter model test of BAC**   \n\n- Compact Model: \n  - $\\hat{FPS}= 9.4 + 0*BAC + 0.2 * TA$   \n  - $SSE_c =$ 108547.9, $P_c$ = 2  \n\n- Augmented Model: \n  - $\\hat{FPS}= 19.4 + -177 *BAC + 0.2 *TA$   \n  - $SSE_a =$ 103877.2, $P_a$ = 3  \n\n\n$F(P_a - P_c, N - P_a) = \\frac{(SSE_c - SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$ \n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nHow can you see the increase in power from the model comparison perspective (look back at last slide)?\n:::\n\n:::{.fragment}\n**Two parameter model test of BAC** \n\n- $F(2-1, 96-2) = \\frac{(133888.3 - 128837.1)/(2-1)}{128837.1 / (96-2)}$\n\n- $F(1, 94) = \\frac{(5051.2)/(1)}{1370.6}$\n\n- $F (1, 94) = 3.69, p = 0.0579$\n\n**Three parameter model test of BAC** \n\n- $F(3-2, 96-3) = \\frac{(108547.9 - 103877.2)/(3-2)}{103877.2 / (99-3)}$\n\n- $F(1, 93) = \\frac{(4670.7)/(1)}{1117}$\n\n- $F (1, 93) = 4.18, p = 0.0442$\n\n[Decreased $SSE_a$ in three parameter model. Flip side of increased model $R^2$.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n$F(P_a - P_c, N - P_a) = \\frac{(SSE_c = SSE_a) / (P_a-P_c)}{SSE_a/(N-P_a)}$   \n\n- Impact of $N$ on $P_a$ also clear.\n\n- Impact of $s_y$ and $s_{x_j}$ and multicollinearity less clear in formula.\n\n- Connection to precision of parameter estimation less clear in formula.\n\n--------------------------------------------------------------------------------\n\n## R-Squared_j and Multicollinearity\n\n$SE_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_Y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n- $t(N-P) = \\frac{b-0}{SE_b}$   \n- $CI_b= b \\pm t(\\alpha; N-P)*SE_b$   \n\nThis decrease in **power and precision** for model parameter estimates (regression coefficients) associated with redundancy among the predictors is called the **problem of Multicollinearity**.  \n\n--------------------------------------------------------------------------------\n\nWhen there are more than two `X`s in the model, it is **not** sufficient to examine only bivariate correlations among $X$s. \n\nTo determine if a problem exists, calculate Variance Inflation Factors (VIF) for each $X$.  \n\n- $VIF_j = \\frac{1}{(1-R^2_j)}$   \n\n- VIF tells you how much $SE_{b_j}$ is increased because of redundancy. VIFs $\\ge$ 5 are considered problematic (SE increased by factor of 2.2).  \n\n--------------------------------------------------------------------------------\n\nWe can use `car::vif()` to calculate VIFs in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     bac       ta \n1.000296 1.000296 \n```\n\n\n:::\n:::\n\n\n\n\\\n\nA related concept is **tolerance** \n\n- $X_i= 1- R^2_j$ \n- Tolerance decreases toward 0 as multicollinearity increases.\n\n--------------------------------------------------------------------------------\n\nSolutions for Multicollinearity include:  \n\n- Drop redundant variable.\n\n- Factor analysis (e.g., PCA) to produce factors that reflect major sources of variance among the redundant predictors.\n\n- This is only a problem for the `X`s in the model with high VIFs. If you don't care about testing them, this is not a problem. Generally, you only care about VIFs for your focal `X`(s).\n\n## Interpretation of Multiple Regression Coefficients\n\n:::{.callout-important}\n# Question\nWhat did the value of $b_1$ tell us in a regression model with one $X$?\n:::\n\n:::{.fragment}\n[The change in $Y$ associated with a one unit increase in $X_1$.]{style=\"color:blue;\"}\n\n[For every 1 unit increase in $X_1$, there will be a $b_1$ unit increase in $Y$.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat about $b_j$ with multiple (e.g., 2) $X$s?\n:::\n\n:::{.fragment}\n[The change in $Y$ associated with a one unit increase in $X_j$ **controlling for all other $X$s in the model**. \"Controlling for\" means holding constant.]{style=\"color:blue;\"}\n\n[For every 1 unit increase in $X_j$, there will be a $b_j$ unit increase in $Y$ holding all other $X$s in the model constant.]{style=\"color:blue;\"}\n:::\n\n## A Second Example\n\nNow lets switch gears to a new example with new data.\n\n- Do hours of studying per week affect exam performance in 610?\n- How might we test this question if we also knew students IQs?\n\n--------------------------------------------------------------------------------\n\nHere are some data to test our question\n\n- Lets make centered `X`s for both of our quantitative predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam <- read_csv(here::here(path_data, \"06_two_predictors_exam.csv\"), \n                 show_col_types = FALSE) |> \n  mutate(study_hours_c = study_hours - mean(study_hours),\n         iq_c = iq - mean(iq))\n```\n:::\n\n\n\n- And take a quick look at the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  subid study_hours    iq  exam study_hours_c  iq_c\n  <dbl>       <dbl> <dbl> <dbl>         <dbl> <dbl>\n1     1        9.20 106.   79.7        -0.802  6.39\n2     2       10.9  103.   59.6         0.897  3.03\n3     3       10.5   98.1  70.2         0.508 -1.89\n4     4       12.3  110.   59.3         2.30  10.4 \n5     5        8.79  93.4  55.8        -1.21  -6.64\n6     6       11.5  114.   73.0         1.53  14.3 \n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n- And descriptives\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam |> my_skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |          |\n|:------------------------|:---------|\n|Name                     |data_exam |\n|Number of rows           |200       |\n|Number of columns        |6         |\n|_______________________  |          |\n|Column type frequency:   |          |\n|numeric                  |6         |\n|________________________ |          |\n|Group variables          |None      |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_complete| n_missing|   mean|    sd|     p0|    p50|   p100|\n|:-------------|----------:|---------:|------:|-----:|------:|------:|------:|\n|subid         |        200|         0| 100.50| 57.88|   1.00| 100.50| 200.00|\n|study_hours   |        200|         0|  10.00|  2.00|   4.15|  10.05|  14.92|\n|iq            |        200|         0| 100.00| 15.00|  61.60| 100.10| 132.48|\n|exam          |        200|         0|  62.14| 11.66|  25.99|  62.44|  93.69|\n|study_hours_c |        200|         0|   0.00|  2.00|  -5.85|   0.05|   4.92|\n|iq_c          |        200|         0|   0.00| 15.00| -38.40|   0.10|  32.48|\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nStart by fitting the two parameter model (testing just `study_hours`) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_exam_2 <- lm(exam ~ 1 + study_hours_c, data = data_exam)\n\ntidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat is the interpretation of $b_1$ in this model?\n:::\n\n:::{.fragment}\n[There is a significant effect of study hours on exam scores.  For every one hour of studying per week, students' exam scores increase by 2.3 points]{style=\"color:blue;\"}\n:::\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat is the interpretation of $b_0$?\n:::\n\n:::{.fragment}\n[The expected exam score for a student who studies the mean number of hours per week (from the sample) is 62.1.  This is significantly different from zero, but that test is probably not that meaningful.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\nMaybe study hours are related to exam scores only because intelligent students study more (learned early good study habits) and intelligent students do better on exams.  \n\n:::{.callout-important}\n# Question\nWhat would you expect about the relationships (i.e., correlations) among study hours, iq, and exam scores if this were true?\n:::\n\n:::{.fragment}\n[They would all be positively correlated with each other.  This is, in fact, what we see in these data (because they are fake!)]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam |> \n  select(study_hours, iq, exam) |> \n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            study_hours   iq exam\nstudy_hours         1.0 0.50 0.40\niq                  0.5 1.00 0.54\nexam                0.4 0.54 1.00\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nHow could you assess the unique effect of `study_hours`, controlling for `iq`?\n:::\n\n:::{.fragment}\n[Model exam scores as a function of both `study_hours` and `iq`. $b_1$ (effect of study hours) in this model is the unique effect of study_hours, controlling for IQ.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_exam_3 <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)\n```\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     62.1      0.687      90.4  2.28e-162\n2 study_hours_c    1.04     0.398       2.61 9.77e-  3\n3 iq_c             0.348    0.0530      6.56 4.62e- 10\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDo hours of studying per week affect performance in 610 after controlling for student IQ?</span>   \n:::\n\n:::{.fragment}\n[Yes, for every one hour of studying per week, students' exam scores increase by 1 point, controlling for IQ.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     62.1      0.687      90.4  2.28e-162\n2 study_hours_c    1.04     0.398       2.61 9.77e-  3\n3 iq_c             0.348    0.0530      6.56 4.62e- 10\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat is the interpretation of $b_0$ in this model?\n:::\n\n:::{.fragment}\n[It is the predicted value for exam scores for someone of mean IQ who studies the mean number of hours per week.  This is descriptively useful but the statistical test against 0 is not very meaningful.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat would change in this model if we did not center `study_hours` and `iq`?\n:::\n\n:::{.fragment}\n[The intercept would be the expected exam score for a student who studies 0 hours per week and has an IQ of 0.]{style=\"color:blue;\"}\n\n[There would be no change in either $b_1$ or $b_2$.  These are still the unique effects of each of their associated $X$s, holding the other $X$ constant in the model.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(m_exam_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     62.1      0.687      90.4  2.28e-162\n2 study_hours_c    1.04     0.398       2.61 9.77e-  3\n3 iq_c             0.348    0.0530      6.56 4.62e- 10\n```\n\n\n:::\n:::\n\n\n\n\\\n\nLook at the effect of `study_hours` across the two models\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhy did the effect of `study_hours` get smaller after controlling for IQ (see last slide and think about pattern of correlations among predictors)?\n:::\n\n:::{.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam |> \n  select(study_hours, iq, exam) |> \n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            study_hours   iq exam\nstudy_hours         1.0 0.50 0.40\niq                  0.5 1.00 0.54\nexam                0.4 0.54 1.00\n```\n\n\n:::\n:::\n\n\n\n[1. When study_hours increases, IQ increases.]{style=\"color:blue;\"}\n\n[2. When IQ increases, exam scores increase.]{style=\"color:blue;\"}\n\n[3. The partial effect of `study_hours` on exam scores is smaller if IQ is not allowed to increase.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n## Causal Models\n\nLets think about a causal model for the relationship between study hours and exam scores\n\n:::{.callout-important}\n# Question\nHow do we get the total (overall) effect of study hours on exam scores, not controlling for any other variables?\n:::\n\n:::{.fragment}\n[To get the total effect of study_hours on exam score, predict exam scores from study hours, without controlling for any other variables.  This is just the two parameter model we fit earlier.]{style=\"color:blue;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(exam ~ 1 + study_hours_c, data = data_exam) |> \n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\nThis is a \"path diagram\" of our causal model at this point\n\nWe can show this total effect of `study_hours` on `exam` using the parameter estimate from the one predictor model.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-f58585aeefc578560836\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f58585aeefc578560836\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"2.34\\\", fontsize = 5]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n:::{.callout-note}\n# Note\nIt is common to use standardized parameter estimates in path diagrams, but for instructive purposes, we will use raw parameter estimates\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nBut what if we had a more complicated model that acknowledged that IQ may also affect exam scores.  How do we get the direct (unique, partial) effect of study hours on exam scores, controlling for IQ?  And how do we get the direct effect of IQ on exam scores, controlling for study hours?\n:::\n\n:::{.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(exam ~ 1 + study_hours_c + iq_c, data = data_exam) |> \n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     62.1      0.687      90.4  2.28e-162\n2 study_hours_c    1.04     0.398       2.61 9.77e-  3\n3 iq_c             0.348    0.0530      6.56 4.62e- 10\n```\n\n\n:::\n:::\n\n\n\n[These are the parameter estimates for these two `X`s, from the three parameter model.  Each of these parameter estimates is the effect of that $X$, controlling for the other $X$ in the model]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\nAnd here is the path diagram for our updated causal model\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nDiagrammeR::grViz(\"\ndigraph{\n  graph[rankdir=LR]\n  node [shape = box, fontsize = 5, width = .5, height = .2]\n    A [label = 'study_hours']\n    B [label = 'iq']\n    Y [label = 'exam']\n    A->Y [label = '1.04', fontsize = 5]\n    B->Y [label = '.35', fontsize = 5]\n    A->B [label = '???', fontsize = 5]\n{ rank = same; A; B }\n}\n\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-666695e630571755035d\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-666695e630571755035d\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    B [label = \\\"iq\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"1.04\\\", fontsize = 5]\\n    B->Y [label = \\\".35\\\", fontsize = 5]\\n    A->B [label = \\\"???\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n:::{.callout-tip}\n# Programming Tip\nWe left the code for this figure above (folded). You can unfold to see how we used the `Diagrammer` package to create this figure.\n:::\n\n--------------------------------------------------------------------------------\n\nThe total effect of study hours on exam scores is the sum of the direct effect of study hours on exam scores and the indirect or spurious* effect of study hours on exam scores through IQ.  That is currently missing from our model\n\n- This would be considered an indirect effect if we believe that study hours causes IQ\n- This would be a spurious effect if we believe that IQ causes study hours or a third variable (not displayed) causes both study hours and IQ\n- The decision about which is the most plausible explanation is a theoretical one and/or a function of your research design\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-290b294e0462f4d88e25\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-290b294e0462f4d88e25\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    B [label = \\\"iq\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"1.04\\\", fontsize = 5]\\n    B->Y [label = \\\".35\\\", fontsize = 5]\\n    A->B [label = \\\"???\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nTo calculate this indirect/spurious effect, we first need to calculate the direct effect of study hours on IQ and add it to the path diagram.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_iq <- lm(iq_c ~ 1 + study_hours_c, data = data_exam)\n\nm_iq |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   3.88e-15     0.921  4.21e-15 1.00e+ 0\n2 study_hours_c 3.75e+ 0     0.462  8.12e+ 0 4.77e-14\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-6e13cc8a15914508f73b\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-6e13cc8a15914508f73b\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    B [label = \\\"iq\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"1.04\\\", fontsize = 5]\\n    B->Y [label = \\\".35\\\", fontsize = 5]\\n    A->B [label = \\\"3.75\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n**Total effect = Direct + Indirect/Spurious**\n\n2.34 = 1.04 +  (3.75 * 0.35)\n\n2.34 = 1.04 +  1.3\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-edfe3c4910b63f7c9b23\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-edfe3c4910b63f7c9b23\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"2.34\\\", fontsize = 5]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-2274f62786d7223e8318\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2274f62786d7223e8318\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    B [label = \\\"iq\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"1.04\\\", fontsize = 5]\\n    B->Y [label = \\\".35\\\", fontsize = 5]\\n    A->B [label = \\\"3.75\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nIn what situation would $b_j$ for a focal predictor (e.g., $X1$ below) not change when you added an additional $X$ (e.g., $X2$)? \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-dcf7fed3f3d4eb3f7319\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-dcf7fed3f3d4eb3f7319\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"X1\\\"]\\n    Y [label = \\\"Y\\\"]\\n    A->Y [label = \\\"b1\\\", fontsize = 5]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-8eb14ae402613fdd05f0\" style=\"width:576px;height:384px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-8eb14ae402613fdd05f0\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"X1\\\"]\\n    B [label = \\\"X2\\\"]\\n    Y [label = \\\"Y\\\"]\\n    A->Y [label = \\\"b1\\\", fontsize = 5]\\n    B->Y [label = \\\"b2\\\", fontsize = 5]\\n    A->B [label = \\\"?\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n[1. If X2 was completely uncorrelated (orthogonal) with the focal predictor (X1) in those sample data, there would be no change in the parameter estimate when you added X2. The direct effect of X1 would equal its total effect because the indirect/suprious effect through X2 would be zero.]{style=\"color:blue;\"}\n\n[2. This is why uncorrelated predictors/covariates are considered easier to interpret when trying to increase power. If they are related to the DV, they will increase power (< SE) to test your focal variable but they will not change your estimate of the magnitude of the focal variables parameter estimate.]{style=\"color:blue;\"}\n\n[3. Completely orthogonal variables are typically only observed in experimental designs. However, small/trivial, nonsystematic sample $r$ will occur when population $r=0$.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-137627264d3091620e12\" style=\"width:960px;height:480px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-137627264d3091620e12\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"X1\\\"]\\n    Y [label = \\\"Y\\\"]\\n    A->Y [label = \\\"b1\\\", fontsize = 5]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-eb9f6a2afd5c88e695b9\" style=\"width:960px;height:480px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-eb9f6a2afd5c88e695b9\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"X1\\\"]\\n    B [label = \\\"X2\\\"]\\n    Y [label = \\\"Y\\\"]\\n    A->Y [label = \\\"b1\\\", fontsize = 5]\\n    B->Y [label = \\\"b2\\\", fontsize = 5]\\n    A->B [label = \\\"0\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Visualizing Total and Direct/Unique Effects\n\nIn the previous slides, we displayed the relationships between correlated predictors and $Y$ in a path model\n\nWe can also visualize these relationships between correlated predictors and $Y$ using Venn diagrams\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-59-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Total and Direct/Unique Effects\n\n- The overlap among the circles represent the correlations among the variables\n- The numbers represent variability in $Y$ attributed to the $X$s.  \n- This variability can be partitioned into unique and shared variance among the predictors\n- And there will also be unexplained variability in $Y$ that cannot be explained by either predictor\n- We will build up to understanding this display conceptually over the next several (many) slides!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-60-1.png){width=960}\n:::\n:::\n\n\n\n## The Relevant Models\n\nAll of these variabilities can be defined in terms of differences in SSE or $var(\\hat{Y_i})$ between various models.  Lets fit all the relevant models here\n\n- Mean only model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_mo <- lm(exam ~ 1, data = data_exam)\n```\n:::\n\n\n\n- Study hours one predictor (two parameter) model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_sh <- lm(exam ~ 1 + study_hours_c, data = data_exam)\n```\n:::\n\n\n\n- IQ one predictor (two parameter) model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_iq <- lm(exam ~ 1 + iq_c, data = data_exam)\n```\n:::\n\n\n\n- Full two predictor (three parameter) model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_full <- lm(exam ~ 1 + study_hours_c + iq_c, data = data_exam)\n```\n:::\n\n\n\n## Total Variability in Y \n\nThe formula for the variance of $Y$ is\n\n- $s^2 = \\frac{\\Sigma{(Y_i - \\bar{Y})^2}}{N - 1}$\n\nThe numerator of this term is the SSE of the mean-only model for $Y$\n\n- $SSE_{mean-only} = \\Sigma{(Y_i - \\hat{Y})^2}$\n- because $\\hat{Y} = \\bar{Y}$ in the mean-only model\n\nSo we can think of SSE from the mean only model as an index of the total variability in $Y$ (just without the $N-1$)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(var(data_exam$exam) * (200 - 1)) |> round() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27044\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsse(m_mo) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27044\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe can represent our understanding of the variability in `exam` scores from the mean-only model as a single circle of a Venn diagram.  \n\n- There are no circles other than for `exam` because there are no predictors in the mean-only model\n- The full variability of `exam` is currently unexplained.  The mean-only model predicts a single value for all observations so it cannot explain why the scores on `exam` differ.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-67-1.png){width=960}\n:::\n:::\n\n\n\n## The One Predictor Study Hours Model \n\nNow we can add the variability in `exam` explained by `study_hours` to the Venn diagram\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-68-1.png){width=5in height=5in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the unexplained variability in `exam` from this one predictor model?\n:::\n\n:::{.fragment}\n[This is just the SSE from this one predictor `study_hours` model]{style=\"color:blue;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse(m_sh) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22675\n```\n\n\n:::\n:::\n\n\n\n[And, of course, this is just the numerator of the variance of the errors in the one predictor model]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(var(residuals(m_sh)) * (200 -1)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22675\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-71-1.png){width=5in height=5in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nThere are two ways to get the variability in `exam` explained by `study_hours` in this one predictor model.  What are they?\n:::\n\n:::{.fragment}\n[1.  A model comparison between the mean-only model and the one predictor study hours model.  Error is reduced in the study hours model because study hours explained variability that was previously unexplained in the mean-only model.]{style=\"color:blue;\"}\n\n- $SSE_{m\\_mo} - SSE_{m\\_sh}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_mo) - sse(m_sh)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4369\n```\n\n\n:::\n:::\n\n\n\n[2. The variability in `exam` explained by `study_hours` is also contained in the variance of that model's predicted values.]{style=\"color:blue;\"}\n\n\n- $Data = Model + Error$\n- $Var(Y_i) = Var(\\hat{Y_i}) + Var(\\epsilon_i)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(var(predict(m_sh)) * (200 -1)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4369\n```\n\n\n:::\n:::\n\n\n:::\n\n## The One Predictor IQ Model \n\nWe can do the same thing for the one predictor model with IQ\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-74-1.png){width=5in height=5in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the unexplained variability in `exam` in the one predictor model `iq` model?\n:::\n\n:::{.fragment}\n[The SSE from the one predictor `iq` model]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse(m_iq) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 19252\n```\n\n\n:::\n:::\n\n\n\n[Or the numerator of the variance of the errors in this one predictor model]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(var(residuals(m_iq)) * (200 -1)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 19252\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-77-1.png){width=5in height=5in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nThere are two ways to get the variability in `exam` explained by `iq` in this one predictor model.  What are they?\n:::\n\n:::{.fragment}\n[1.  A model comparison between the mean only model and the one predictor study hours model. Error is reduced in the iq model because iq explains variability that was previously unexplained in the mean-only model.]{style=\"color:blue;\"}\n\n- $SSE_{m\\_mo} - SSE_{m\\_sh}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_mo) - sse(m_sh)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4369\n```\n\n\n:::\n:::\n\n\n\n[2. The variability in `exam` explained by `iq` is also contained in the variance of that model's predicted values.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(var(predict(m_iq)) * (200 -1)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7791\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## The Two Predictor Model\n\nWe can represent how `study_hours` and `iq` combine to explain variability in `exam`\n\n- `study_hours` and `iq` each explain some variance in `exam` that can not be explained by the other  (their unique, partial, direct effects)\n- But given the correlation between them, they also both predict some of the same variability in `exam`.  This shared variance in `exam` can be predicted by either of these predictors.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-80-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-81-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the unexplained variability in `exam` in the two predictor model?\n:::\n\n:::{.fragment}\n[The SSE from the two predictor model]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse(m_full) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18609\n```\n\n\n:::\n:::\n\n\n:::\n\n## The Two Predictor Model: Unique Effect of Study Hours\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-83-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the unique variability in `exam` explained by `study_hours`, controlling for `iq` by comparing SSE across two models?\n:::\n\n:::{.fragment}\n[The SSE from the one predictor model with `iq` minus SSE from the two predictor model. This indexs how much new (unique) variability can be explained by `study_hours` above and beyond what was already explained by `iq` (i.e., the incremental variability explained by `study_hours`)]{style=\"color:blue;\"}\n\n- $SSE_{m\\_iq} - SSE_{m\\_full}$\n- [This is also the model comparison that is used to test the significance of the effect of `study_hours` in the two predictor model.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_iq) - sse(m_full)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 643\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-85-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat is a second way to get the unique variability in `exam` explained by `study_hours` controlling for `iq`?\n:::\n\n:::{.fragment}\n[Using the variability of predicted scores from the two predictor model minus the variability of the predicted scores from the one predictor model with `iq`.]{style=\"color:blue;\"}\n\n- $(Var(\\hat{Y}_{m\\_full}) - Var(\\hat{Y}_{m\\_iq})) * (N - 1)$\n- [Same model comparison but in terms of the variance of the predicted values (corrected for N-1 put in units of sums of squares)]{style=\"color:blue;\"}\n- [We simply focus on the explained vs. unexplained variability.  The variability in the predicted values is sometimes called $SS_{regression}$ or if $X$ is categorical, $SS_{between}$ in ANOVA.]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n((var(predict(m_full)) - var(predict(m_iq))) * (200 - 1))  |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 643\n```\n\n\n:::\n:::\n\n\n:::\n\n## The Two Predictor Model: Unique Effect of IQ\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-87-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the unique variability in `exam` explained by `iq`, controlling for `study_hours` using SSEs from a model comparison?\n:::\n\n:::{.fragment}\nThe SSE from the one predictor model with `study_hours` minus the SSE from the two predictor model.\n\n- $SSE_{m\\_sh} - SSE_{m\\_full}$\n- This is also the model comparison that is used to test the significance of the effect of `iq` in the two predictor model.\n- Of course, we could also get this value focusing on the variability of the predicted scores from the two models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_sh) - sse(m_full)) |> round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4066\n```\n\n\n:::\n:::\n\n\n:::\n\n## The Two Predictor Model: Shared effects\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-89-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nHow do we get the shared variability in `exam` explained by both `study_hours` and `iq` using SSEs?\n:::\n\n:::{.fragment}\nThere are many different ways to get this value. The most straightforward (IMO) is to subtract down this area.\n\n- Start with SSE of the mean-only model (the total variability in `exam`)\n- Subtract the SSE of the two predictor model (the unexplained variability in `exam` from that model)\n- Then subtract the unique variances for `study_hours` and `iq` that we calculated on the previous slides\n:::\n\n## Effect Sizes: Raw Parameter Estimates\n\nYou can describe the effects of your predictors in raw units of change or as proportions of explained variability in $Y$\n\nThe raw parameter estimates describe the effects of the predictors in the original units of $X$ and $Y$\n\n- The parameter estimates from the one predictor models describe the overall/total effects of each predictor\n- The parameter estimates from the full model describe the unique/partial/direct effects of each predictor\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_sh |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_iq |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   62.1      0.697      89.1  9.31e-162\n2 iq_c           0.417    0.0466      8.95 2.55e- 16\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_full |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     62.1      0.687      90.4  2.28e-162\n2 study_hours_c    1.04     0.398       2.61 9.77e-  3\n3 iq_c             0.348    0.0530      6.56 4.62e- 10\n```\n\n\n:::\n:::\n\n\n\n## Effect Sizes: Proportions of Explained Variability\n\nThere are several options to consider for proportions of explained variability in $Y$ \n\n- These options differ on whether they focus on the total variability explained by the full model vs. unique variability explained by each predictor\n- When focusing on unique predictor variability, they differ on the denominator of the proportion (total variability in $Y$ vs. previously unexplained variability in $Y$)\n\n## Effect Sizes: Model R-Squared\n\nYou can quantify and test the overall performance of the model (with all $X$s) using the coefficient of determination ($R^2$).\n\n- $R^2 = \\frac{\\text{Explained Variability in Y}}{\\text{Total Variability in Y}}$\n- $R^2$ is the proportion of variability in $Y$ explained by the set of all model predictors (all $X$s) relative to the total variability in $Y$\n- In our example, $R^2$ describes the combined effect of `study_hours` and `iq`. \n- In more complex models, $R^2$ will always be predictive strength of the set of all $X$s derived from the predictors. \n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-93-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n$R^2 = \\frac{\\text{Total Explained Variability in Y}}{\\text{Total Variability in Y}}$\n\n:::{.callout-important}\n# Question\nUse SS in the Venn diagram to calculate $R^2$\n:::\n\n:::{.fragment}\n$\\frac{643 + 4066 + 3726}{643 + 4066 + 3726 + 18609}$\n:::\n\n--------------------------------------------------------------------------------\n\n$R^2 = \\frac{\\text{Total Explained Variability in Y}}{\\text{Total Variability in Y}}$\n\nThe numerator of this formula can be calculated as a model comparison of SSEs\n\n- $SSE_{mean-only} - SSE_{full}$   \n\nThe denominator can be obtained directly from the SSE of mean-only model.  This yields another definition of the $R^2$ in terms of SSEs\n\n- $R^2 = \\frac{SSE_{mean-only} - SSE_{full}}{SSE_{mean-only} }$   \n\n--------------------------------------------------------------------------------\n\nGiven that the numerator of $R^2$ can be defined as a model comparison, we can use the model comparison approach to test hypotheses about it\n\n- $H_0: R^2 = 0$\n- $H_a: R^2 > 0$\n\n[Compact model: Mean-only model]{style=\"color:blue;\"}\n\n- $\\hat{Exam}_i = \\beta_0+0*StudyHours_i + 0*IQ_i$\n- $P_c=1$     \n\n[Augmented model: Full model]{style=\"color:blue;\"} \n\n- $\\hat{Exam}_i = \\beta_0+\\beta_1*StudyHours_i + \\beta_2*IQ_i$\n- $P_a=3$     \n\n$F(P_a-P_c, N-P_a) = \\frac{SSE_c-SSE_a/(P_a-P_c)}{SSE_a/(N-P_a)}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m_mo, m_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: exam ~ 1\nModel 2: exam ~ 1 + study_hours_c + iq_c\n  Res.Df   RSS Df Sum of Sq      F                Pr(>F)    \n1    199 27044                                              \n2    197 18609  2    8434.5 44.645 < 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n$R^2 = \\frac{\\text{Total Explained Variability in Y}}{\\text{Total Variability in Y}}$\n\nRemember: $var(Y_i) = var(\\hat{Y_i}) + var(e_i)$\n\n- $var(Y_i)$ is total variance in $Y$\n- $variance(\\hat{Y_i})$ is variance in predicted values\n- $variance(e_i)$ is variance in residuals\n\nTherefore, we can get $R^2$ by dividing the variance of the predicted values by the variance of the observed values\n\n$R^2 = \\frac{var(\\hat{Y_i})}{var(Y_i)}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(predict(m_full)) / var(data_exam$exam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3118857\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nThe easiest way to get $R^2$ for any model is the `glance()` function from the `broom` package.  \n\n- Pass in the full model\n- `glance()` provides a variety of full model statistics\n- You can either `pull()`, `select()` or use `$r.squared` to get the numeric value or a tibble with the value\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_full) |> pull(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3118857\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_full) |> select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.312\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_full)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3118857\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-99-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDo the $R^2$ from the two one predictor models (`study_hours` & `iq`) models add up to the $R^2$ of the two predictor model?\n:::\n\n:::{.fragment}\nNo, if you add the 2 one predictor model $R^2$s, it will overestimate the full model $R^2$ because you will double count the shared variance!\n\n$R^2_{sh} = \\frac{643 + 3726}{643 + 3726 + 4066 + 18609}$\n\n$R^2_{iq} = \\frac{4066 + 3726}{643 + 3726 + 4066 + 18609}$\n\n$R^2_{full} = \\frac{643 + 4066 + 3726}{643 + 3726 + 4066 + 18609}$\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-100-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nIs there any situation where the $R^2$ from 2 one predictor models will add up to the $R^2$ of the two predictor model including both predictors?\n:::\n\n:::{.fragment}\nYes, if the two $X$s are uncorrelated so that there is no shared variance in $Y$ predicted by both.\n:::\n\n\n## Effect Size: Predictor Effects\n\nThere are two proportion of variance effect sizes for the specific $X$s in your model\n\n- You have already learned about Partial eta-squared ($\\eta_p^2$), which Judd et al refer to as PRE.  We will return to this in a moment\n\n- You will first learn about Delta $R^2$ ($\\Delta R^2$).\n\n## Effect Size: Delta R-Squared\n\nAs the name implies, $\\Delta R^2$ is the change (increase) in model $R^2$ for the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.\n\n- We can calculate $\\Delta R^2$ for each $X$ (or later set of $X$s) in the model\n- We need to specify the appropriate compact and augmented model\n- Then: $\\Delta R^2 = R^2_a - R^2_c$\n\nfor `study_hours`\n\n- compact model: $\\hat{y}_i=\\beta_0+0*study\\_hours_i+ \\beta_2*iq_i$\n- augmented model: $\\hat{y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\nFor `iq`\n\n- compact Model: $\\hat{Y}_i=\\beta_0+*\\beta_1*study\\_hours_i+ 0*iq_i$\n- augmented Model: $\\hat{Y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\n--------------------------------------------------------------------------------\n\nConceptually, $\\Delta R^2$ for an $X$ is the unique (incremental) variability in $Y$ explained by $X$ relative to the total variability in $Y$\n\n- $\\Delta R^2 = \\frac{\\text{Unique Variability in Y Explained by X}}{\\text{Total Variability in Y}}$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-101-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\nUse SS in the Venn diagram to calculate $\\Delta R^2$ for `study_hours` and `iq`\n:::\n\n:::{.fragment}\nfor `study_hours`\n\n- $\\frac{643}{643 + 3726 + 4066 + 18609}$\n\nfor `iq`\n\n- $\\frac{4066}{643 + 3726 + 4066 + 18609}$\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-102-1.png){width=4in height=4in}\n:::\n:::\n\n\n\nFollowing this conceptual understanding of $\\Delta R^2$, we can use SSEs from model comparisons to get $\\Delta R^2$ for `study_hours` and `iq`\n\n- $\\Delta R^2 = \\frac{SSE_c - SSE_a}{SSE_{\\text{mean-only}}}$\n- Use the compact and augmented model associated with the test of the parameter estimate\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-103-1.png){width=4in height=4in}\n:::\n:::\n\n\n\nfor `study_hours`\n\n- compact model: $\\hat{y}_i=\\beta_0+0*study\\_hours_i+ \\beta_2*iq_i$\n- augmented model: $\\hat{y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_iq) - sse(m_full)) / sse(m_mo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02378012\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-105-1.png){width=4in height=4in}\n:::\n:::\n\n\n\nfor `iq`\n\n- compact model: $\\hat{y}_i=\\beta_0+\\beta_1*study\\_hours_i+ 0*iq_i$\n- augmented model: $\\hat{y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_sh) - sse(m_full)) / sse(m_mo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1503416\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-107-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDo the $\\Delta R^2$ each of the predictors (`study_hours` & `iq`) add up to the $R^2$ of the two predictor model?\n:::\n\n:::{.fragment}\nNo, if you add $\\Delta R^2$s for the two predictors, it will underestimate the full model $R^2$ because you will miss counting the shared variance!\n\n$\\Delta R^2_{sh} = \\frac{643}{643 + 3726 + 4066 + 18609}$\n\n$\\Delta R^2_{iq} = \\frac{4066}{643 + 3726 + 4066 + 18609}$\n\n$R^2_{full} = \\frac{643 + 4066 + 3726}{643 + 3726 + 4066 + 18609}$\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-108-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nIs there any situation where the $\\Delta R^2$ for each of the two predictors will add up to the $R^2$ of the two predictor model including both predictors?\n:::\n\n:::{.fragment}\nYes, if the two $X$s are uncorrelated so that there is no shared variance in $Y$ predicted by both.\n:::\n\n## Effect Size: Partial Eta-Squared or PRE\n\n$\\eta_p^2$ or PRE describes how much SSE was reduced (proportionally) in the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.  \n\n$PRE = \\frac{SSE_c - SSE_a}{SSE_c}$\n\n- We can calculate PRE for each $X$ (or later set of $X$s) in the model\n- We need to specify the appropriate compact and augmented model\n\n--------------------------------------------------------------------------------\n\n$PRE = \\frac{SSE_c - SSE_a}{SSE_c}$\n\nFor `study_hours`\n\n- Compact Model: $\\hat{Y}_i=\\beta_0+0*study\\_hours_i+ \\beta_2*iq_i$\n- Augmented Model: $\\hat{Y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_iq) - sse(m_full)) / sse(m_iq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.033404\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nFor `iq`\n\n- Compact Model: $\\hat{Y}_i=\\beta_0+*\\beta_1*study\\_hours_i+ 0*iq_i$\n- Augmented Model: $\\hat{Y}_i=\\beta_0+\\beta_1*study\\_hours_i+ \\beta_2*iq_i$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sse(m_sh) - sse(m_full)) / sse(m_sh)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1793077\n```\n\n\n:::\n:::\n\n\n\n## Comparing Delta R-Squared and PRE\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-111-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nDefine $\\Delta R^2$ and $PRE$ for `study_hours` in the two predictor model\n:::\n\n:::{.fragment}\n[The numerator is the same.  The denominator is different!]{style=\"color:blue;\"}\n\n- $\\Delta R^2_{sh} = \\frac{643}{643 + 3726 + 4066 + 18609}$\n- $PRE_{sh} = \\frac{643}{643 + 18609}$\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-112-1.png){width=4in height=4in}\n:::\n:::\n\n\n\n:::{.callout-important}\nDefine $\\Delta R^2$ and $PRE$ for `iq` in the two predictor model\n:::\n\n:::{.fragment}\nThe numerator is the same.  The denominator is different!\n\n- $\\Delta R^2_{iq} = \\frac{4066}{643 + 3726 + 4066 + 18609}$\n- $PRE_{iq} = \\frac{4066}{4066 + 18609}$\n:::\n\n## Comparing Variance Based Effect Sizes\n\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}} - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$\n\n$\\Delta R^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$\n\n$\\eta_p^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}$\n\n--------------------------------------------------------------------------------\n\n$R^2$  \n\n- Describes proportion of explained variance in $Y$ explained by full model relative to total variance in $Y$.   \n- **Not** used for a specific predictor but instead for the full model\n- Can use F test with model comparison (full vs. mean-only) to test if the model as a whole predicts variance in $Y$ (multi-df test)\n- Not used frequently in Psychology but useful in other fields.   \n\n$\\Delta R^2$  \n\n- Describes proportion of unique variance in $Y$ explained by $X_j$ relative to total variance in $Y$.  \n- If $X$s are orthogonal $\\Delta R^2$ will sum to $R^2$.   \n- Anchored to total $Y$ variance.  \n- Same denominator for all $X$s.  \n- Test is statistically equivalent to test of parameter estimate\n\n$\\eta_p^2$  \n\n- Describes proportion of reduction of unexplained variance (SSE) by adding $X_j$.  \n- Anchored to unexplained variance from compact model.\n- Stable in experimental designs when additional IVs are added.\n- Test is statistically equivalent to test of parameter estimate\n\n## Multiple Predictors: Dichotomous Predictor\n\n**Example:** Evaluate the effects of a new intervention for depression.   \n\n- $N=200$ participants, randomly assigned to receive new intervention vs. standard of care control.  \n- Measure depression with CES-D pre- (baseline) and post-intervention.    \n\n\\\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_int <- read_csv(here::here(path_data, \"06_two_predictors_intervention.csv\"),\n                     show_col_types = FALSE)\n```\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_int |> my_skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |data_int |\n|Number of rows           |200      |\n|Number of columns        |4        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|numeric                  |4        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: numeric**\n\n|skim_variable      | n_complete| n_missing|   mean|    sd|   p0|    p50|   p100|\n|:------------------|----------:|---------:|------:|-----:|----:|------:|------:|\n|subid              |        200|         0| 100.50| 57.88| 1.00| 100.50| 200.00|\n|intervention_group |        200|         0|   0.50|  0.50| 0.00|   0.50|   1.00|\n|depress_base       |        200|         0|  29.16| 10.09| 0.68|  29.57|  53.70|\n|depress_post       |        200|         0|  33.90| 10.86| 0.52|  35.67|  59.18|\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_int |> head(30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 × 4\n   subid intervention_group depress_base depress_post\n   <dbl>              <dbl>        <dbl>        <dbl>\n 1     1                  0         45.2         48.7\n 2     2                  0         41.6         49.6\n 3     3                  0         17.0         32.0\n 4     4                  0         48.1         42.4\n 5     5                  0         35.2         36.6\n 6     6                  0         27.7         40.7\n 7     7                  0         18.5         30.8\n 8     8                  0         36.8         51.1\n 9     9                  0         22.4         26.1\n10    10                  0         17.5         20.2\n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nHow do you evaluate the effect of the new intervention in a one predictor model?\n:::\n\n:::{.fragment}\n1. [Code one regressor for intervention group using dummy or zero-centered coefficients. How different?]{style=\"color:blue;\"}\n\n2. [Regress depression scores from post-intervention on intervention group.]{style=\"color:blue;\"}\n\n3. [Test if $b_1$ for Intervention group is non-zero.]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_ig <- lm(depress_post ~ intervention_group, data = data_int)\nm_ig |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question \nWhat do you conclude? What do you report?\n:::\n\n:::{.fragment}\n[The new intervention reduced CES-D depressions scores by approximately 3.6 units relative to the standard of care control group, $b = -3.6, t(198) = -2.40, p= .0175.$]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat else do you report?\n:::\n\n:::{.fragment}\n- [PRE or $\\Delta R^2$ (they are the same in the one predictor model and also equal to $R^2$!)]{style=\"color:blue;\"} \n- [Means/SD of two groups]{style=\"color:blue;\"}\n- [Figure with means, standard error, and raw (or residual) data points]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_mo <- lm(depress_post ~ 1, data = data_int)\nglance(m_ig)$r.squared - glance(m_mo)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02818089\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat is a **better** analysis to address this question about the effect of the intervention and why is it better?\n:::\n\n:::{.fragment}\n[Control for baseline depression scores to increase power]{style=\"color:blue;\"}\n\n[Baseline depression should be uncorrelated (in the population) with intervention group because participants were randomly assigned to group. Therefore, including baseline depression should not systematically change $b_1$.]{style=\"color:blue;\"}\n\n[However, baseline scores are likely a strong predictor of post-intervention scores. This will increase model $R^2$, reduce SSE, and therefore reduce the SE for intervention group. More power!]{style=\"color:blue;\"}\n:::\n\n--------------------------------------------------------------------------------\n\nReview parameter estimates and their tests.  Does this make sense to you?\n\n**Previous one predictor model for IG**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_ig |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n```\n\n\n:::\n:::\n\n\n\n**Two predictor model**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata_int <- data_int |> \n  mutate(depress_base_c = depress_base - mean(depress_base))\n\nm_full <- lm(depress_post ~ intervention_group + depress_base_c, data = data_int)\n\nm_full |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nReview $R^2$s.  Does this make sense to you?\n\n**Previous one predictor model for IG**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nglance(m_ig)$r.squared \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02818089\n```\n\n\n:::\n:::\n\n\n\n**Two predictor model**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nglance(m_full)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3130173\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nReview $\\Delta R^2$ for `intervention_grouup` across two models.  Do they make sense to you?\n\n**One predictor IG model**\n\n\n\n::: {.cell cold-fold='true'}\n\n```{.r .cell-code}\n(sse(m_mo) - sse(m_ig)) / sse(m_mo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02818089\n```\n\n\n:::\n:::\n\n\n\n**Two predictor model**\n\n\n\n::: {.cell cold-fold='true'}\n\n```{.r .cell-code}\nm_base <- lm(depress_post ~ depress_base_c, data = data_int)\n(sse(m_base) - sse(m_full)) / sse(m_mo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03807968\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nReview $PRE$ for `intervention_group` across two models.  Do they make sense to you? What about the differences between $\\Delta R^2$ and $PRE$ for the two predictor model?\n\n**One predictor IG model**\n\n\n\n::: {.cell cold-fold='true'}\n\n```{.r .cell-code}\n(sse(m_mo) - sse(m_ig)) / sse(m_mo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02818089\n```\n\n\n:::\n:::\n\n\n\n**Two predictor model**\n\n\n\n::: {.cell cold-fold='true'}\n\n```{.r .cell-code}\nm_base <- lm(depress_post ~ depress_base_c, data = data_int)\n(sse(m_base) - sse(m_full)) / sse(m_base)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05251917\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Visualizing the Model\n\nThis is NOT how we would present this in a paper but it is a useful way to understand the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npreds_ig <- c(0,1) \npreds_depress_base <- seq(min(data_int$depress_base_c), max(data_int$depress_base_c), length.out = 100)\n\npreds <- expand.grid(intervention_group = preds_ig, depress_base_c = preds_depress_base) |> \n  as_tibble()\n\npreds <- preds |> \n  mutate(depress_post = predict(m_full, newdata = preds))\n\npreds |> \n  ggplot(aes(x = depress_base_c, y = depress_post, group = as.factor(intervention_group), color = as.factor(intervention_group))) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-126-1.png){width=960}\n:::\n:::\n\n\n\n## Plot Option 1: Raw Data\n\nGet predicted values for the two groups controlling for baseline depression\n\nWe want predictions setting `depress_base_c = 0` for everyone\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- tibble(intervention_group = c(0,1),\n                depress_base_c = 0) \n\npreds <- preds|> \n  bind_cols(predict(m_full, preds, se.fit = TRUE) |>\n  as_tibble() |>\n  mutate(upper = fit + se.fit,\n         lower= fit - se.fit)) |> \n  select(-df, - residual.scale) # remove unnecessary columns\n\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  intervention_group depress_base_c   fit se.fit upper lower\n               <dbl>          <dbl> <dbl>  <dbl> <dbl> <dbl>\n1                  0              0  36.0  0.905  36.9  35.1\n2                  1              0  31.8  0.905  32.7  30.9\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\nWe want \n\n- Bar plot of means\n- Some display of uncertainty/sampling error (e.g. the standard error)\n- Individual data points? (raw or rawish?)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_col(aes(x = as.factor(preds$intervention_group), y = preds$fit),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(data_int$intervention_group), \n              y = data_int$depress_post), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds$lower, \n                    ymax = preds$upper, \n                    x = as.factor(preds$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-128-1.png){width=576}\n:::\n:::\n\n\n\n## Plot Option 2: Residuals\n\nInstead of the raw scores, we can plot depression scores after removing the variance due to baseline individual differences\n\n- start with original scores for `intervention_group`\n- add a variable for baseline depression set to mean for everyone\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- tibble(intervention_group = data_int$intervention_group, \n              depress_base_c = 0)\nres |> slice_sample(n = 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 2\n   intervention_group depress_base_c\n                <dbl>          <dbl>\n 1                  0              0\n 2                  0              0\n 3                  0              0\n 4                  1              0\n 5                  1              0\n 6                  1              0\n 7                  0              0\n 8                  0              0\n 9                  1              0\n10                  1              0\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n- get predicted values for each person based on their group but with baseline depression set to the mean\n- add in the residuals (errors) from each model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- res |>\n  mutate(depress_post_res = predict(m_full, res),\n         depress_post_res = depress_post_res + residuals(m_full))\nres |> slice_sample(n = 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 3\n   intervention_group depress_base_c depress_post_res\n                <dbl>          <dbl>            <dbl>\n 1                  0              0             24.3\n 2                  1              0             28.0\n 3                  1              0             32.7\n 4                  1              0             50.5\n 5                  1              0             31.2\n 6                  1              0             34.3\n 7                  0              0             41.5\n 8                  1              0             33.1\n 9                  1              0             12.8\n10                  1              0             24.0\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nHow are these residualized `depress_post` scores different from the raw scores?\n:::\n\n:::{.fragment}\n[These scores contain the variance from being in different `intervention_groups` and the unexplained variance from the full model (the residuals)]{style=\"color:blue;\"}\n\n[However, the variance associated with individual differences on baseline depression have been removed because everyone has predicted scores assuming they were at mean baseline depression]{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(data_int$depress_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 117.9451\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(res$depress_post_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 85.52947\n```\n\n\n:::\n:::\n\n\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_col(aes(x = as.factor(preds$intervention_group), y = preds$fit),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(res$intervention_group), \n                  y = res$depress_post_res), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds$lower, \n                    ymax = preds$upper, \n                    x = as.factor(preds$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-132-1.png){width=576}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Visualizing the Model\n\nLets return to the BAC and Trait Anxiety data\n\n$\\hat{FPS}= 19.4 + -177 *BAC + 0.2 *TA$ \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(plot3D)\n\npreds_bac <- seq(min(data$bac), max(data$bac), length.out = 100)\npreds_ta <- seq(min(data$ta), max(data$ta), length.out = 100)\n\npreds_x <- expand.grid(bac = preds_bac, ta = preds_ta)\n\npreds_y <- matrix(predict(m_3, newdata = preds_x),\n                 nrow = 100,\n                 ncol = 100)\n\nscatter3D(data$bac, data$ta, data$fps, pch = 19, col = \"black\", ticktype = \"detailed\",\n          xlab = \"BAC\", ylab = \"TA\", zlab = \"FPS\",\n          surf = list(x = preds_bac, y = preds_ta, z = preds_y, alpha = .9, border = \"light grey\"))\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-133-1.png){width=960}\n:::\n:::\n\n\n\n## Plot Option 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = mean(preds_ta)) \n\npreds <- preds |> \n  bind_cols(predict(m_3, preds, interval = \"confidence\", level = .95) |>\n  as_tibble()) \n\n\npreds |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          bac    ta      fit      lwr      upr\n1 0.000000000 227.5 54.31248 41.56158 67.06337\n2 0.001404040 227.5 54.06389 41.49181 66.63598\n3 0.002808081 227.5 53.81531 41.41991 66.21071\n4 0.004212121 227.5 53.56672 41.34580 65.78765\n5 0.005616162 227.5 53.31814 41.26938 65.36690\n6 0.007020202 227.5 53.06956 41.19054 64.94857\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = preds$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +\n  labs(x = \"BAC\",\n       y = \"FPS\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-135-1.png){width=576}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question    \nHow could I modify the previous plot to still focus on only BAC but have the raw data also control for TA\n:::\n\n:::{.fragment}\nI could residualize fps for TA and then plot the residuals instead of the raw data.  This would retain variance in FPS due to BAC and to unexplained error but remove TA variance.   I prefer this method.\n:::\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nHow might I modify the plot if TA was also a focal variable?\n:::\n\n:::{.fragment}\nI could plot separate lines for selected values of TA.\n:::\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = c(round(mean(preds_ta)), round(mean(preds_ta)-(sd(preds_ta)*1.5)), \n                              round(mean(preds_ta)+(sd(preds_ta)*1.5))))\n\npreds <- preds |> \n  bind_cols(predict(m_3, preds) |>\n  as_tibble()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$value, color = as.factor(preds$ta)),\n              linewidth = 1) +\n  labs(x = \"BAC\",\n       y = \"FPS\",\n       color = \"TA\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-revealjs/unnamed-chunk-137-1.png){width=960}\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat do you report and why?\n:::\n\n--------------------------------------------------------------------------------\n\nLets mean-center TA and fit a final full model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata <-  data |> \n  mutate(ta_c = ta - mean(ta))\nm_3_c <- lm(fps ~ 1 + bac + ta_c, data = data)\nm_3_c |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   42.1      5.91        7.12 2.28e-10\n2 bac         -177.      86.6        -2.04 4.37e- 2\n3 ta_c           0.153    0.0324      4.73 8.07e- 6\n```\n\n\n:::\n:::\n\n\n\nWhat would have changed from previous model with uncentered TA?\n\n--------------------------------------------------------------------------------\n\n$\\eta_p^2$ effect size for BAC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_bac <- sum(residuals(lm(fps ~ ta_c, data = data))^2)\nsse_a <- sum(residuals(m_3_c)^2)\n\n(p_eta_bac <- (sse_c_bac - sse_a)/sse_c_bac)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04302933\n```\n\n\n:::\n:::\n\n\n\n$\\eta_p^2$ effect size for TA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_ta <- sum(residuals(lm(fps ~ bac, data = data))^2)\n\n(p_eta_ta <- (sse_c_ta - sse_a)/sse_c_ta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.193732\n```\n\n\n:::\n:::\n\n\n\n$\\eta_p^2$ effect size for intercept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_int <- sum(residuals(lm(fps ~ bac + ta - 1, data = data))^2)\n\n(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06472535\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n95% CI\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_3_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %     97.5 %\n(Intercept)   30.32490549 53.8033111\nbac         -348.98099457 -5.1177113\nta_c           0.08891558  0.2177329\n```\n\n\n:::\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n## Describing Model Results\n\nWe regressed fear-potentiated startle (FPS) on Blood alcohol concentration (BAC). We included trait anxiety (mean-centered) as a covariate in this model to increase power to test substantive questions about BAC. We tested partial effects, controlling for all other predictors in the model, from the full model that included both predictors. We provide raw regression coefficients, 95% confidence intervals for these coefficients, and partial eta squared ($\\eta_p^2$) to quantify effect sizes for each predictor in Table 1.\n\nFPS was 42.1 $\\mu V$ for participants with 0.00% BAC and average trait anxiety, $t(93) = 7.12, p< .001$, indicating that our threat manipulation successfully increased FPS above zero when sober. As expected, the effect of the trait anxiety covariate was significant and reduced error variance by approximately 19%, $t(93)= 4.73, p< .001$.  FPS increased by 0.2 $\\mu V$ for every 1 unit increase in trait anxiety.\n\nAs predicted, the effect of BAC was significant and reduced error variance by approximately 4%, $t(93)= -2.04, p= .044$.  FPS decreased 1.8 $\\mu V$ for every .01% increase in BAC (see Figure 1).\n\n--------------------------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef <- m_3_c |> \n  broom::tidy() |> \n  select(term, estimate, statistic, p.value) |> \n  mutate(p.value = if_else(p.value < .001, \"<.001\", as.character(round(p.value, 2))))\n\nci <- confint(m_3_c) |> \n  round(2) |> \n  as_tibble() |> \n  unite(ci, `2.5 %`, `97.5 %`, sep = \", \") |> \n  mutate(ci = str_c(\"(\", ci, \")\"))\n\np_eta <- tibble (peta = c(p_eta_int, p_eta_bac, p_eta_ta)) \n\ntable <- coef |> \n  bind_cols(ci, p_eta) |> \n  mutate(` ` = factor(term, levels = c(\"(Intercept)\", \"bac\", \"ta_c\"),\n                     labels = c(\"Intercept\", \"Blood Alcohol Concentration\", \"Trait Anxiety\"))) |> \n  select(` `, \n         b = estimate,\n         `95% CI (b)` = ci,\n         `Partial eta squared` = peta,\n         t = statistic,\n         p = p.value) |> \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"))\n```\n:::\n\n\n\n--------------------------------------------------------------------------------\n\n**Table 1**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntable\n```\n\n::: {.cell-output-display}\n\n\n|                            |    b    |    95% CI (b)    | Partial eta squared |   t   |   p   |\n|:---------------------------|:-------:|:----------------:|:-------------------:|:-----:|:-----:|\n|Intercept                   |  42.06  |  (30.32, 53.8)   |        0.06         | 7.12  | <.001 |\n|Blood Alcohol Concentration | -177.05 | (-348.98, -5.12) |        0.04         | -2.04 | 0.04  |\n|Trait Anxiety               |  0.15   |   (0.09, 0.22)   |        0.19         | 4.73  | <.001 |\n\n\n:::\n:::\n\n\n\nNotes:   \n$R^2 = 0.224, F(2,93) = 13.44, p < .001$***    \nTrait anxiety was mean-centered   \n\n--------------------------------------------------------------------------------\n\n:::{.callout-important}\n# Question\nWhat other table should you consider in your results?\n:::\n\n:::{.fragment}\n[Table of simple correlations between variables. Can summarize with other important info fairly concisely. Could also include reliability, skewness, kurtosis, etc.}{style=\"color:blue;\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(` ` = c(\"Trait Anxiety\", \"Blood Alcohol Concentration\", \"Mean\", \"SD\"),\n       `Fear Potentiated Startle` = c(cor(data$ta_c, data$fps), cor(data$bac, data$fps),\n                                      mean(data$fps), sd(data$fps)),\n       `Trait Anxiety` = c(NA, cor(data$bac, data$ta_c), mean(data$ta_c), sd(data$ta_c)),\n       `Blood Alcohol Concentration` = c(NA, NA, mean(data$bac), sd(data$bac))) |> \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\"))\n```\n\n::: {.cell-output-display}\n\n\n|                            | Fear Potentiated Startle | Trait Anxiety | Blood Alcohol Concentration |\n|:---------------------------|:------------------------:|:-------------:|:---------------------------:|\n|Trait Anxiety               |           0.44           |      NA       |             NA              |\n|Blood Alcohol Concentration |          -0.19           |     -0.02     |             NA              |\n|Mean                        |          32.19           |     0.00      |            0.06             |\n|SD                          |          37.54           |    105.73     |            0.04             |\n\n\n:::\n:::\n\n\n\n\n## Adding a Third Predictor\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_full |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-important}\n# Question\nWhat would you do and why if you had also measured sex at birth in any of these examples? What would change?\n:::\n\n--------------------------------------------------------------------------------\n\n## Summary: New and Familiar Concepts\n\n- Interpretation of $b_0$, $b_1$, and $b_2$, in 2+ predictor model with continuous and dichotomous focal predictor.  \n- Impact of centering $X_1$ or $X_2$ on $b_0$, $b_1$, and $b_2$.  \n- What affects standard errors of $b_j$.  \n- What is Multicollinearity, how to detect, what are implications if high, and what are solutions.  \n- Model effect size ($R^2$).  \n- Effect sizes for $X$s ($b_j$s, $\\Delta R^2$, $\\eta_p^2$).  \n- Text, table, and figure descriptions of results.  ",
    "supporting": [
      "06_two_predictors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"06_two_predictors_files/libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"06_two_predictors_files/libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"06_two_predictors_files/libs/viz-1.8.2/viz.js\"></script>\n<link href=\"06_two_predictors_files/libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"06_two_predictors_files/libs/grViz-binding-1.0.11/grViz.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}