{
  "hash": "42c302fae86aa392f18a7c581a6531df",
  "result": {
    "engine": "knitr",
    "markdown": "--- \noutput: html_document \neditor_options:  \n  chunk_output_type: console\n--- \n\n\n\n \n\n# Dealing with Messy Data I: Case Analysis\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Anscombe's Quartet\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nAnscombe, Francis J. (1973) Graphs in statistical analysis. *American Statistician*, 27, 17–21. \n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(lm(y1 ~ x, data = Quartet)) #<1>\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0257 \n2 x              0.500     0.118      4.24 0.00217\n```\n\n\n:::\n:::\n\n\n\n\n1. See Quartet dataframe in `car` package.\n\n\nSSE = 13.76\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(lm(y2 ~ x, data = Quartet))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)     3.00     1.13       2.67 0.0258 \n2 x               0.5      0.118      4.24 0.00218\n```\n\n\n:::\n:::\n\n\n\n\nSSE = 13.78\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(lm(y3 ~ x, data = Quartet))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x              0.500     0.118      4.24 0.00218\n```\n\n\n:::\n:::\n\n\n\n\nSSE = 13.76\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(lm(y4 ~ x4, data = Quartet))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.00      1.12       2.67 0.0256 \n2 x4             0.500     0.118      4.24 0.00216\n```\n\n\n:::\n:::\n\n\n\n\nSSE = 13.74\n\n-----\n\n## Case Analysis\n\n- Goal is to identify any unusual or excessively influential data. These data points may either bias results and/or reduce power to detect effects (inflate standard errors and/or decrease $R^2$).   \n\n- Three aspects of individual observations we attend to:\n  1. Leverage\n  2. Regression Outlier\n  3. Influence\n\n- Case Analysis also provides an important first step as you get to *know* your data.  \n\n-----\n\n## Case Analysis: Unusual and Influential Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(car)\n\ntheme_set(theme_classic()) \n\ndata <- read_csv(here::here(\"data_lecture/7_three_predictors_fps.csv\"), \n                 show_col_types = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 96\nColumns: 5\n$ subid <chr> \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bac   <dbl> 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    <dbl> 208, 133, 120, 103, 97, 84, 34, 34, 296, 126, 200, 158, 26, 255,…\n$ fps   <dbl> 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n$ sex   <chr> \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"fem…\n```\n\n\n:::\n:::\n\n\n\n\nNotice sex is of character class type. We can't use character stings in most of our analyes so we will want to handle character predictors immediately. For example, with our sex variable we could turn it into a factor, dummy code it, center it, or use any other contrast/combination of the above.  \n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |> \n  mutate(sex_c = if_else(sex == \"female\", -.5, .5),\n         sex = factor(sex, \n                      levels = c(\"female\", \"male\"))) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 96\nColumns: 6\n$ subid <chr> \"0011\", \"0012\", \"0013\", \"0014\", \"0015\", \"0016\", \"0021\", \"0022\", …\n$ bac   <dbl> 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    <dbl> 208, 133, 120, 103, 97, 84, 34, 34, 296, 126, 200, 158, 26, 255,…\n$ fps   <dbl> 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n$ sex   <fct> female, female, female, female, female, female, female, female, …\n$ sex_c <dbl> -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5…\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_1 <- lm(fps ~ bac + ta + sex_c, data = data)\n\nbroom::tidy(m_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   18.9      7.47        2.53 0.0132    \n2 bac         -165.      84.6        -1.95 0.0545    \n3 ta             0.152    0.0316      4.82 0.00000570\n4 sex_c        -16.0      6.67       -2.40 0.0184    \n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Univariate Statistics\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  select(-subid, -sex) |> \n  pivot_longer(everything(), names_to = \"var\") |>\n  group_by(var) |> \n  summarize( n = n(),\n             mean = mean(value), \n             sd = sd(value), \n             min = min(value), \n             max = max(value)) |> \n  mutate(across(mean:max, ~round(.x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 6\n  var       n   mean     sd   min    max\n  <chr> <int>  <dbl>  <dbl> <dbl>  <dbl>\n1 bac      96   0.06   0.04   0     0.14\n2 fps      96  32.2   37.5  -98.1 163.  \n3 sex_c    96   0      0.5   -0.5   0.5 \n4 ta       96 148.   106.    10   445   \n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Univariate Plots\n\nHistograms are a useful/common visualization for numeric variables.   \n\nHere we have histograms with density as y-axis overlayed with density and rug plots (which you have seen earlier in the course!).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_fps <- data |> \n  ggplot(aes(x = fps)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +\n  geom_rug(color = \"red\")\n\nplot_bac <- data |> \n  ggplot(aes(x = bac)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0,\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\")\n\nplot_ta <- data |> \n  ggplot(aes(x = ta)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\nplot_sex <- data |> \n  ggplot(aes(x = sex_c)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n\n\n(plot_fps + plot_bac) / (plot_ta + plot_sex)\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\nBox plots can also be a useful visualization tool for numeric variables.   \n\nBoxplots display:   \n\n1. Median as line\n2. 25th %ile and 75th %ile as hinges\n3. Highest and lowest points within 1.5 * IQR (interquartile-range: difference between scores at 25th %ile and 75th %ile)\n4. Outliers outside of 1.5 * IQR\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox_fps <-  data |>\n    ggplot(aes(x = fps)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nbox_bac <-  data |>\n    ggplot(aes(x = bac)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nbox_ta <-  data |>\n    ggplot(aes(x = ta)) +\n    geom_boxplot() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank(),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox_fps + box_bac + box_ta\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\nWe could also overlay a violin plot over the box plot to clearly see the shape of the distribution and tails.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(box_fps +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL)) +\n\n(box_bac +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL)) +\n\n(box_ta +\n  geom_violin(aes(y = 0), fill = \"green\", color = NA, alpha = .4)  +\n  ylab(NULL)) \n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n\n## Bivariate Correlations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  select(-subid, -sex) |> \n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        bac    ta   fps sex_c\nbac    1.00 -0.02 -0.19  0.06\nta    -0.02  1.00  0.44 -0.01\nfps   -0.19  0.44  1.00 -0.23\nsex_c  0.06 -0.01 -0.23  1.00\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Bivariate Plots (Numeric Predictor)\n\nScatterplots are the preferred visualization when both variables (i.e., the predictor and outcome) are numeric.     \n\nWe also can add a simple line and a LOWESS line (Locally Weighted Scatterplot Smoothing) to help us consider the shape of the relationship.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbivar_bac <- data |>\n    ggplot(aes(x = bac, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n\nbivar_ta <- data |>\n    ggplot(aes(x = ta, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"green\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n\nbivar_sex <- data |>\n    ggplot(aes(x = sex_c, y = fps)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    theme(axis.text.x = element_text(size = 11),\n          axis.text.y = element_text(size = 11))\n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbivar_bac + bivar_ta + bivar_sex\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n## Bivariate Plots (Categorical Predictor)\n\nA grouped version of the combined box and violin plot is our preferred visualization for relationship between categorical and numeric variables.\n\nLets look at an example of this with sex.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n data |>\n    ggplot(aes(x = sex, y = fps)) +\n    geom_violin(fill = \"green\", color = NA) +\n    geom_boxplot(width = .1, fill = NA, lwd = 1.1, fatten = 1.1) +\n    theme(axis.text.x = element_text(angle = 90, \n                                     vjust = 0.5, hjust = 1),\n          axis.text.y = element_text(size = 11))\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n\n## Leverage (Cartoon Data)\n\n**Check for high Leverage points**   \n\nLeverage is a property of the predictors (DV is not considered for leverage analysis). An observation will have increased *leverage* on the results as its distance from the mean of all predictors increases. \n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n<span style=\"color: red;\">Question: Which colored points have the most leverage in the 1 predictor example above?</span> \n\n-----\n\n## Leverage\n\nHat values ($h_i$) provide an index of leverage.   \n\nIn the one predictor case:  \n$h_i = \\frac{1}{N} + \\frac{(X_i- \\overline X)^2}{\\sum(X_j- \\overline X)^2}$ (for $j=1$ to $N$ in summation)  \n\nWith multiple predictors, $h_i$ measures the distance from the centroid (point of means) of the $X$s. Hat values are bounded between $\\frac{1}{N}$ and 1. \n\n**Mean Hat value = $\\frac{P}{N}$.**  \n\nRules of thumb:  \n\n- $h_i > 3* \\overline h$ for small samples ($N < 100$)\n- $h_i > 2* \\overline h$ for large samples   \n\n\n**Do not blindly apply rules of thumb. Hat values should be separated from distribution of $h_i$. View a histogram of $h_i$.**\n\nNote: Mahalanobis (Maha) distance = $(N - 1)(h_i - \\frac{1}{N})$.   \nSPSS reports centered leverage ($h - \\frac{1}{N}$).\n\n-----\n\n## Leverage (Cartoon Data; Continued)\n\n\nHigh leverage values are not always bad. In fact, in some cases they are good. Must also consider if they are regression outliers.  \n\n<span style=\"color: red;\">Question: Why?</span> \n\n-----\n\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$  \n\n$\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}$\n\n\n- <span style=\"color: blue;\">High leverage points that are fit well by the model increase the difference between $\\text{SSE}_{\\text{mean-only}}$ and $\\text{SSE}_a$, which increases $R^2$.</span>\n\n- <span style=\"color: blue;\">High leverage points that are fit well also increase variance for predictor. This reduces the SE for predictors and yields more power.</span>\n\n- <span style=\"color: blue;\">Well fit, high leverage points do **not** alter $b$s.</span>\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n## Leverage (Real Data)\n\nThere is a function available for helping you identify observations with high leverage.  \n\nSource the function with the following code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/case_analysis.R?raw=true\")\n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"hatvalues\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n## Regression Outlier (Cartoon data)\n\n**Check for regression outliers**\n\nAn observation that is not adequately fit by the regression model (i.e., falls very far from the prediction line).  \n\nIn essence, a regression outlier is a discrepant score with a large residual ($e_i$).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n\n<span style=\"color: red;\">Question: Which point(s) are regression outliers?</span> \n\n-----\n\n## Regression Outlier\n\nThere are multiple quantitative indicators to identify regression outliers, including raw residuals ($e_i$), standardized residuals  ($e'_i$), and studentized residuals ($t'_i$). The preferred index is the **studentized residual**.   \n\n$t'_i = \\frac{e_i}{(\\text{SE}_{e(-i)}*\\sqrt{(1-h_i)})}$    \n\n$t'_i$ follows a t-distribution with n-P-1 degrees of freedom.   \n\n\nNOTE: SPSS calls these Studentized Deleted Residuals. Cohen calls these Externally Studentized Residual\n\n-----\n    \n\n## Regression Outlier (Cartoon data; Continued)\n\nRegression outliers are always bad but they can have two different types of bad effects.   \n\n<span style=\"color: red;\">Question: Why?</span> \n\n-----\n\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}}- \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$  \n\n$\\text{SE}_{bi} = \\frac{s_y}{s_i} = \\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N - k - 1)}}*\\frac{1}{\\sqrt{(1-R^2_i)}}$\n\n\n- <span style=\"color: blue;\">Regression outliers increase $\\text{SSE}_a$ which decreases $R^2$. Decreased $R^2$ leads to increased SEs for $b$s.</span>\n\n- <span style=\"color: blue;\">If outlier also has leverage can alter (increase or decrease) $b$s.</span>\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n## Regression Outlier (Real data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"residuals\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::outlierTest(m_1, cutoff = .05, labels = data$subid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rstudent unadjusted p-value Bonferroni p\n0125 -4.307609        0.000041622    0.0039958\n2112  3.765010        0.000294880    0.0283080\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n## Influence (Cartoon data)\n\nAn observation is *influential* if it substantially alters the fitted regression model (i.e., the coefficients and/or intercept). Two commonly used assessment methods:    \n\n- Cooks distance\n- dfBetas\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n\n\n<span style=\"color: red;\">Question: Which point(s) have the most influence?</span> \n\n\n## Cook's Distance\n\nCook’s distance ($\\text{D}_i$) provides a single summary statistic to index how much influence each score has on the overall model.    \n\nCooks distance is based on both the *outlierness* (standardized residual) and leverage characteristics of the observation.   \n\n$\\text{D}_i = \\frac{e'^2_i}{P} * \\frac{h_i}{1-h_i}$   \n\n$\\text{D}_i > \\frac{4}{N-P}$ has been proposed as a very liberal cutoff (identifies a lot of influential points). \n\n$\\text{D}_i > \\text{qf}(.5, P, N-P)$ has also been employed as very conservative.   \n\n\nIdentification of problematic scores should be considered in the context of the overall distribution of $\\text{D}_i$.\n\n\n## Cook's Distance (Real data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"cooksd\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)\n```\n\n\n:::\n:::\n\n\n\n\n## Influence Bubble Plot (Real data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"influenceplot\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Rownames\ncharacter(0)\n\n$Values\nnumeric(0)\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: What are the expected effects of each of these points on the model?</span> \n\n## dfBetas\n\n$\\text{dfBeta}_{ij}$ is an index of how much each regression coefficient ($j= 0 – k$) would change if the $i^{\\text{th}}$ score was deleted.   \n\n$\\text{dfBeta}_{ij} = b_j - b_{j(-i)}$\n\ndfBetas (preferred) is the standardized form of the index.   \n\n$\\text{dfBetas} = \\frac{\\text{dfBeta}}{\\text{SE}_{b_{j(-i)}}}$ \n\n$|{\\text{dfBetas}}|>2$ may be problematic.   \n\n$|{\\text{dfBetas}}|> \\frac{2}{\\sqrt{N}}$ in larger samples (Belsley et al., 1980).   \n\nConsider distribution with histogram! Also can visualize with added variable plot.\n\nProblem is there can be many dfBetas (a set for each predictor and intercept). Most helpful when there is one *critical/focal effect.*\n\n-----\n\n\n## dfBetas (Real data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"dfbetas\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n## Added Variable Plot (Real data)\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n## Impact on SEs\n\nIn addition to altering regression coefficients (and reducing $R^2$), problematic scores can increase the SEs (i.e., precision of estimation) of the regression coefficients.    \n\nCOVRATIO is an index that indicates how individual scores affect the overall precision of estimation (joint confidence region for set of coefficients) of the regression coefficients.\n\nObservations that decrease the precision of estimation have COVRATIOS < 1.0.\n\nBelsley et al., (1980) proposed a cut off of:    \n\n$\\text{COVRATIO}_i < |3* \\frac{P}{N} - 1|$\n\n-----\n\n## Impact on SEs (Real data)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_analysis(m_1, Type = \"covratio\")\n```\n\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Rownames\ncharacter(0)\n\n$Values\nnamed numeric(0)\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Four Examples with Fake Data\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_case_analysis_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n-----\n\n\n## Enter the Real World\n\n<span style=\"color: red;\">Question: So what do you do?</span> \n\n\n-----\n\n## Overall Impact of Problem Scores: Real Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(m_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   18.9      7.47        2.53 0.0132    \n2 bac         -165.      84.6        -1.95 0.0545    \n3 ta             0.152    0.0316      4.82 0.00000570\n4 sex_c        -16.0      6.67       -2.40 0.0184    \n```\n\n\n:::\n:::\n\n\n\n\n**SSE = 97756.43**\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rm_outliers <- data |> \n  filter(!subid %in% c(\"0125\", \"2112\"))\n\nm_2 <- lm(fps ~ bac + ta + sex_c, data = data_rm_outliers) \n\nbroom::tidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   26.6      6.44        4.13 0.0000825\n2 bac         -229.      72.4        -3.16 0.00214  \n3 ta             0.126    0.0273      4.61 0.0000135\n4 sex_c        -15.5      5.70       -2.72 0.00790  \n```\n\n\n:::\n:::\n\n\n\n\n**SSE = 68262.98**\n\n-----\n\n## What to Do\n\n- <span style=\"color: blue;\">Don’t worry about leverage alone.</span>\n\n- <span style=\"color: blue;\">Worry about model outliers always.</span>\n\n- <span style=\"color: blue;\">Worry about influence (overall model or predictors by field).</span>\n\n- <span style=\"color: blue;\">Drop, retain, or bring model outliers to the fence.</span>\n\n- <span style=\"color: blue;\">Drop or retain influential participants.</span>\n\n- <span style=\"color: blue;\">Report both ways (in olden day…).</span>\n\n- <span style=\"color: blue;\">Get with the program and pre-register instead (what you worry about, how you define it, and what you do)!</span>\n\n\n",
    "supporting": [
      "07_case_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}