{
  "hash": "c8d16c9e96b351c4d6bc83ae5883e56b",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Simulate Error Rates for Contrast Approaches {.unnumbered}\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n### 3 Groups with No Group Differences (Type I Errors) \n\nSet up simulation characteristics for Null Findings.\n\nThis will allow us to determine Type I error rates because any signficant effect is a type I error given we have set the population effect to 0\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate N experiments\nn_experiments <- 20000\n\n# group means\nm_1 <- 10\nm_2 <- 10\nm_3 <- 10\n\nsd <- 20 # sd for y\nn <- 50 # group size\n\n# set up x as factor\nx <-  factor(c(rep(\"a\", n), rep(\"b\", n), rep(\"c\", n)))  \n\nset.seed(1234567)\n```\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**1. POCs - all focal (separate research questions)**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_poc <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(2, -1, -1,\n                         0,  1, -1), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05,\n         sig_c2 = results$p.value[3] < 0.05,\n         sig_any = any(results$p.value[2:3] < 0.05))\n}\n\ntype1_poc <- map(1:n_experiments, simulate_poc) |> \n  list_rbind()\n```\n:::\n\n\n\n\nResults (to make clear what function returns)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype1_poc |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 4\n    sim sig_c1 sig_c2 sig_any\n  <int> <lgl>  <lgl>  <lgl>  \n1     1 FALSE  FALSE  FALSE  \n2     2 FALSE  FALSE  FALSE  \n3     3 TRUE   FALSE  TRUE   \n4     4 FALSE  FALSE  FALSE  \n5     5 FALSE  FALSE  FALSE  \n6     6 FALSE  FALSE  FALSE  \n```\n\n\n:::\n:::\n\n\n\n\nTest wise type I error for each contrast is 5%\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_poc$sig_c1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0491\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_poc$sig_c2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0545\n```\n\n\n:::\n:::\n\n\n\n\nThe results across contrasts are independent because they come from different families\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(type1_poc$sig_c1, type1_poc$sig_c2) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01\n```\n\n\n:::\n:::\n\n\n\n\nTo be clear, the family-wise type I error across the set is 10% BUT often not considered in same family so not important?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_poc$sig_any)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1002\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**2. Dummy contrasts from one model (3 levels; no protection).** \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_dummy <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_any = any(c(results$p.value[2:3]) < 0.05))\n}\n\ntype1_dummy <- map(1:n_experiments, simulate_dummy) |> \n  list_rbind()\n```\n:::\n\n\n\n\nTest-wise Type I for each contrast is 5%\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_dummy$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0508\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_dummy$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.051\n```\n\n\n:::\n:::\n\n\n\n\nBut these are from same family (results of contrasts are related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(type1_dummy$sig_d1, type1_dummy$sig_d2) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.15\n```\n\n\n:::\n:::\n\n\n\n\nTherefore, family-wise error rate is higher (but not 10% because contrasts are dependent/related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_dummy$sig_any)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0919\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**3. All (3) pairwise contrasts (no protection).** \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_pair <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n \n  # fit second model \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results_3$p.value[2] < 0.05,\n         sig_d2 = results_3$p.value[3] < 0.05,\n         sig_d3 = results_1$p.value[2] < 0.05,\n         sig_any = any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_pair <- map(1:n_experiments, simulate_pair) |> \n  list_rbind()\n```\n:::\n\n\n\n\nTest-wise Type I for each contrast is 5%\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_pair$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0501\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_pair$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0498\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_pair$sig_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0497\n```\n\n\n:::\n:::\n\n\n\n\nBut these are from same family (results of contrasts are related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(type1_pair$sig_d1, type1_pair$sig_d2) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.14\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_pair$sig_d1, type1_pair$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.14\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_pair$sig_d2, type1_pair$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.12\n```\n\n\n:::\n:::\n\n\n\n\nFamily-wise error rate is higher (but not 15% because contrasts are dependent/related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_pair$sig_any)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.12315\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**4. Fisher LSD with 3 pairwise comparisons**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_fisher <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_any = sig_omnibus && any(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype1_fish <- map(1:n_experiments, simulate_fisher) |> \n  list_rbind()\n```\n:::\n\n\n\n\nTest-wise Type I for each contrast is < 5% (too conservative!)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_fish$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02485\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_fish$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02475\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_fish$sig_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02555\n```\n\n\n:::\n:::\n\n\n\n\nThese are from same family (results of contrasts are even more related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(type1_fish$sig_d1, type1_fish$sig_d2) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_fish$sig_d1, type1_fish$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.31\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_fish$sig_d2, type1_fish$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3\n```\n\n\n:::\n:::\n\n\n\n\nFamily-wise error rate is controlled at 5% \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_fish$sig_any)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0509\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**5. Holm-Bonferroni correction with 3 pairwise comparisons**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_hb <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_any = any(p_contrasts < 0.05))\n}\n\ntype1_hb <- map(1:n_experiments, simulate_hb) |> \n  list_rbind()\n```\n:::\n\n\n\n\nTest-wise Type I is well under 5% (too conservative!) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_hb$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01865\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_hb$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01765\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type1_hb$sig_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01845\n```\n\n\n:::\n:::\n\n\n\n\nThese are from same family (results of contrasts are related)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(type1_hb$sig_d1, type1_hb$sig_d2) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.19\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_hb$sig_d1, type1_hb$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.18\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(type1_hb$sig_d2, type1_hb$sig_d3) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2\n```\n\n\n:::\n:::\n\n\n\n\nFamily-wise error rate is controlled at 5%\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type1_hb$sig_any)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04365\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with One Group Difference (Type II Errors)\n\nNow lets consider Type II errors.  This is too often neglected in these discussions.  However it is also complicated because there are LOTS of different ways that the population effects could be set up and its not necessarily true that the same method would be more powerful across these settings.  You should consider these simulations as only a start to comparing the power of these methods.\n\nHere we update the pattern of means such that one group is different from the other two but the other two group means are equal\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_1 <- 10\nm_2 <- 10\nm_3 <- 20\n```\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis pattern of means is well-suited to using dummy codes with the third group as reference.  \nThat said, if we only tested these two contrasts, we couldnt conclude anything about differences between groups 1 and 2.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_dummy_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  # fit first model\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results <- lm(y ~ x) |> \n    tidy()\n \n  # extract and organize key results \n  tibble(sim = i,\n         sig_d1 = results$p.value[2] < 0.05,\n         sig_d2 = results$p.value[3] < 0.05,\n         sig_both = all(c(results$p.value[2:3]) < 0.05))\n}\n\ntype2_dummy <- map(1:n_experiments, simulate_dummy_2) |> \n  list_rbind()\n```\n:::\n\n\n\n\n\nHere is power for the two contrasts that should be significant and for finding both significant.\n\nPower is low (70%) for the individual tests but what we would expect given the effect size and sample size.  What we care about is relative power across the approaches. \n\nBut again, we should also note that this method doesnt inform us about differences between group 1 and 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_dummy$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70465\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_dummy$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7037\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_dummy$sig_both)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.56145\n```\n\n\n:::\n:::\n\n\n\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_fish_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n\n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_both = sig_omnibus && all(c(results_3$p.value[2:3]) < 0.05))\n}\n\ntype2_fish <- map(1:n_experiments, simulate_fish_2) |> \n  list_rbind()\n```\n:::\n\n\n\n\n\nPower is lower for the two individual contrasts (63-64%) but we get the third contrasts to show that G1 and G2 are not difference (with 5% false alarm rate).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_fish$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6385\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_fish$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6322\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_fish$sig_both)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5464\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_hb_2 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_both = all(p_contrasts[1:2] < 0.05))\n} \n\ntype2_hb <- map(1:n_experiments, simulate_hb_2) |> \n  list_rbind()\n```\n:::\n\n\n\n\nHB is worse still on power for the individual contrasts (55-56%)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_hb$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.56085\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_hb$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5684\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_hb$sig_both)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4326\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nPOCs don't fit perfectly to this setting.  However, if in this instance our theory predicts only group three different from groups 1 and 2, we could test only that contrast or we might test the second contrast to demonstrate it was NOT significant.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_poc_2 <- function(i) {\n  # vector of y for three groups\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n contrasts(x)<- matrix(c(-1, -1, 2,\n                          1, -1, 0), \n                       ncol = 2,\n                       dimnames = list(levels(x),\n                                       c(\"a_v_bc\", \"b_v_c\")))\n\n  # fit model\n  results <- lm(y ~ x) |> \n    tidy()\n  \n  # extract and organize key results\n  tibble(sim = i,\n         sig_c1 = results$p.value[2] < 0.05)\n}\n\ntype2_poc <- map(1:n_experiments, simulate_poc_2) |> \n  list_rbind()\n```\n:::\n\n\n\n\nHere we only care about the power for the first effect.  Clearly, the best power (~81%) if this is sufficient.  We would likely want the second contrast to be non-significant to demonstrate that the effect is specific to group 3.  This would false alarm at 5%.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_poc$sig_c1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.81675\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n### 3 Groups with All Groups Different (Type II Errors)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_1 <- 10\nm_2 <- 20\nm_3 <- 30\n```\n:::\n\n\n\n\n\n**1. Dummy contrasts from one model (3 levels; no protection).** \n\nThis approach doesn't make sense because we want to test all three contrasts (assuming our theory correctly predicted all groups different)\n\n**2. Fisher LSD with 3 pairwise comparisons**\n\nIf we wanted all three contrasts, we could use Fisher LSD.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_fish_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n\n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n  \n  sig_omnibus <- anova(lm(y ~ x))$`Pr(>F)`[1] < 0.05\n  \n  tibble(sim = i,\n         sig_d1 = sig_omnibus && results_3$p.value[2] < 0.05,\n         sig_d2 = sig_omnibus && results_3$p.value[3] < 0.05,\n         sig_d3 = sig_omnibus && results_1$p.value[2] < 0.05,\n         sig_all = sig_omnibus && all(c(results_3$p.value[2:3], results_1$p.value[2]) < 0.05))\n}\n\ntype2_fish_3 <- map(1:n_experiments, simulate_fish_3) |> \n  list_rbind()\n```\n:::\n\n\n\n\n\nPower is much better for G1 vs G3 (~99%) because its a bigger mean difference than for the other two (w/ ~ 70% power).  But again, its about relative power now.  We need to compare to other methods.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_fish_3$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.99605\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_fish_3$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.69935\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_fish_3$sig_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.69805\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_fish_3$sig_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4333\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**2. Holm-Bonferroni correction with 3 pairwise comparisons**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_hb_3 <- function(i) {\n  y <- c(rnorm(n, m_1, sd), rnorm(n, m_2, sd), rnorm(n, m_3, sd))\n  contrasts(x) <- contr.treatment(levels(x), base = 3) \n  results_3 <- lm(y ~ x) |> \n    tidy()\n  \n  contrasts(x) <- contr.treatment(levels(x), base = 1) \n  results_1 <- lm(y ~ x) |> \n    tidy()\n\n  p_contrasts <- p.adjust(c(results_3$p.value[2:3], results_1$p.value[2]), method = \"holm\")\n    \n  tibble(sim = i,\n         sig_d1 = p_contrasts[1] < 0.05,\n         sig_d2 = p_contrasts[2] < 0.05,\n         sig_d3 = p_contrasts[3] < 0.05,\n         sig_all = all(p_contrasts[1:3] < 0.05))\n} \n\ntype2_hb_3 <- map(1:n_experiments, simulate_hb_3) |> \n  list_rbind()\n```\n:::\n\n\n\n\nHB is a bit worse power for the the smaller individual contrasts (66-67%)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(type2_hb_3$sig_d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.99555\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_hb_3$sig_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6619\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_hb_3$sig_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.66925\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(type2_hb_3$sig_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4247\n```\n\n\n:::\n:::\n\n\n\n\n--------------------------------------------------------------------------------\n\n**4. POCs - Assuming we were right about the pattern of means**\n\nAgain, not clear exactly how to use POCs in this setting if we expect all groups to be different\n\n\n###  What about with 4 levels?\n\nHavent done this year but we will see that Type 1 control falls apart for all pairwise with fisher LSD because there are two many pairwise contrasts.   Without, that most wouldnt tolerate it.   HB will still handle 4 levels with go type 1 control.   Power will be lower still though because there will be bigger adjustments to all p-values.  POCs can really shine here but ONLY if the pattern of means works.   Otherwise, HB is the best bet.",
    "supporting": [
      "app3_contrasts_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}