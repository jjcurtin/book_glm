{
  "hash": "f2decd1271a619312ff971e2db898861",
  "result": {
    "engine": "knitr",
    "markdown": "--- \noutput: html_document \neditor_options:  \n  chunk_output_type: console\n--- \n\n \n# Unit 3: Inferences About a Single Mean (1 Parameter Models)  {.unnumbered}\n\n\n## Assignments\n\n- [Slide deck](https://jjcurtin.quarto.pub/03_single_mean/) on QuartoPub\n\n- Read\n  - Judd et al. Chapter 4. Simple Models: Statistical Inferences about Parameter Values\n\n- Week 2 Application Assignment (due 9/18 @ 1:30 via Canvas)\n- Week 3 Application Assignment (due 9/15 @ 1:30 via Canvas)\n\n\n\n::: {.cell}\n\n:::\n\n\n-----\n\n## Units 3-4 Organization\n\n\n- First, consider details of simplest model (one parameter estimate; mean-only model; no $X$s)\n\n- Next, examine simple (bivariate) regression (two parameter estimates; one $X$ for one quantitative predictor)\n\n- These provide a critical foundation for all linear models\n\n- Subsequent units will generalize to one dichotomous variable (**Unit 5**), multiple predictors (**Units 6-7**), and beyond...\n\n\n-----\n\n\n## Linear Models as Models\n\nLinear models (including regression) are **models**.\n\n$DATA = MODEL + ERROR$\n\n\\\n\\\n\nThree general uses for models:  \n\n1. **Describe** and summarize DATA ($Y$s) in a simpler form using MODEL.\n2. **Predict** DATA ($Y$s) from MODEL.\n3. **Understand** (test inferences about) complex relationships between individual regressors ($X$s) in MODEL and the DATA ($Y$s). How precise are estimates of relationship?\n\n\\\n\\\n\n-----\n\n$DATA = MODEL + ERROR$\n\n\\\n\\\n\nMODELS are simplifications of reality \n\n- As such, there is ERROR \n- They also make assumptions that must be evaluated\n\n\n-----\n\n## Fear Potential Startle\n\nWe were interested in producing anxiety in the laboratory\n\n- To do this, we developed a procedure where we expose people to periods of unpredictable electric shock administration alternating with periods of safety\n\n- We measure their startle response in the shock and safe periods\n \n- We use the difference between their startle during shock – safe to determine if they are anxious\n\n- This is called **Fear potentiated startle (FPS)**. Our procedure works if FPS > 0.  We need a model of FPS scores to determine if FPS > 0\n\n------\n\n## Fear Potentiated Startle: One parameter model\n\nA very simple model for the population of FPS scores would predict the same value for everyone in the population.\n\n$\\hat{Y}_i=\\beta_0$\n\n\\\n\nWe would like this value to be the **best** prediction.\n\n\\\n\n<span style=\"color: red;\">Question: In the context of DATA = MODEL + ERROR, how can we quantify **best**?</span>\n\n-----\n\n<span style=\"color: red;\">Question: In the context of DATA = MODEL + ERROR, how can we quantify **best**?</span>\n\n\\\n\n<span style=\"color: blue;\">We want to predict some characteristic about the population of FPS scores that minimizes the ERROR from our model.</span>   \n\n\\\n\n<span style=\"color: blue;\">ERROR = DATA – MODEL</span>   \n\n\\\n\n$\\varepsilon_i=Y_i-\\hat{Y}_i$\n\n<span style=\"color: blue;\">There is an error ($\\varepsilon_i$) for each population score.</span>\n\n-----\n\n## Total Error\n\n<span style=\"color: red;\">Question: How can we quantify total model error?</span>\n\n-----\n\n\n<span style=\"color: red;\">Question: How can we quantify total model error?</span>\n\n\\\n\n<span style=\"color: blue;\">**Sum of errors**</span> across all scores in the population isn’t ideal because positive and negative errors will tend to cancel each other out.\n\n$\\sum(Y_i-\\hat{Y}_i)$\n\n\\\n\n<span style=\"color: blue;\">**Sum of absolute values of errors**</span> could work. If we selected $\\beta_0$ to minimize the sum of the absolute value of errors, $\\beta_0$ would equal the median of the population.\n\n$\\sum(|Y_i-\\hat{Y}_i|)$\n\n\\\n\n<span style=\"color: blue;\">**Sum of squared errors (SSE)**</span> could work. If we selected $\\beta_0$ to minimize the sum of squared errors, $\\beta_0$ would equal the mean of the population.\n\n$\\sum(Y_i-\\hat{Y}_i)^2$\n\n-----\n\n## One Parameter Model for FPS\n\nFor the moment, lets assume we prefer to minimize SSE (more on that in a moment). You should predict the population mean FPS for everyone.  \n\n\\\n$\\hat{Y}_i=\\beta_0$  ...  where $\\beta_0=\\mu$\n\n\n\\\n<span style=\"color: red;\">Question: What is the problem with this model?</span>\n\n-----\n\n<span style=\"color: red;\">Question: What is the problem with this model?</span>\n\n$\\hat{Y}_i=\\beta_0$  ...  where $\\beta_0=\\mu$\n\n\\\n\n<span style=\"color: blue;\">We don't know the population mean for FPS scores ($\\mu$).</span>\n\n\\\n\\\n\n<span style=\"color: red;\">Question: How could we solve this problem to develop a model?</span>\n\n-----\n\n<span style=\"color: red;\">Question: How could we solve this problem to develop a model?</span>\n\n\\\n\n<span style=\"color: blue;\">We can collect a sample from the population and use the sample mean ($\\overline{X}$) as an estimate of the population mean ($\\mu$).</span> \n\n\\\n\n<span style=\"color: blue;\">$\\overline{X}$ is an unbiased estimate for $\\mu$.</span>\n\n-----\n\n## Model Parameter Estimation\n\nDATA = MODEL + ERROR\n\n\\\n\\\n\n**Population model**    \n\n$Y_i=\\beta_0+\\varepsilon_i$   \n\n$\\hat{Y}_i=\\beta_0$ ... where $\\beta_0=\\mu$  \n\n\\\n\\\n\n**Estimate population parameters from sample**\n\n$Y_i=b_0+e_i$   \n\n$\\hat{Y}_i=b_0$ ... where $b_0=\\overline{X}$  \n\n\n-----\n\n## Least Squares Criterion\n\nIn ordinary least squares (OLS) regression and other least squares linear models, the model **parameter estimates (e.g., $b_0$) are calculated such that they minimize the sum of squared errors (SSE**) in the sample in which you estimate the model.  \n\n\\\n\n$\\text{SSE}=\\sum(Y_i-\\hat{Y}_i)^2$   \n\n\\\n\n$\\text{SSE}=\\sum e_i^2$   \n\n-----\n\n## Properties of Parameter Estimates\n\nThere are three properties that make a parameter estimate attractive.  \n\n1. **Unbiased:** Mean of the sampling distribution for the parameter estimate is equal to the value for that parameter in the population.   \n\n\\\n\n2. **Efficient:** The sample estimates are close to the population parameter.  In other words, the narrower the sampling distribution for any specific sample size $N$, the more efficient the estimator. Efficient means small SE for parameter estimate.    \n\n\\\n\n3. **Consistent:** As the sample size increases, the sampling distribution becomes narrower (more efficient). Consistent means as $N$ increases, SE for parameter estimate decreases\n\n-----\n\n## Least Squares Criterion (Continued)\n\nIf the $\\varepsilon_i$ are normally distributed, both the median and the mean are ***unbiased*** and ***consistent*** estimators.   \n\n\\\n\nThe variance of the sampling distribution for the mean is:   \n\n$\\frac{\\sigma^2}{N}$   \n\n\\\n\nThe variance of the sampling distribution for the median is:   \n\n$\\frac{\\pi\\sigma^2}{2N}$   \n\n\\\n\\\n\nTherefore, the mean is the **more efficient** parameter.   \n\nFor this reason, we tend to prefer to estimate our models by minimizing the sum of squared errors.   \n\n-----\n\n## Fear-potentiated Startle During Threat of Shock\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(conflicts.policy = \"depends.ok\") \nlibrary(tidyverse)\ntheme_set(theme_classic()) \npath_data <- \"data_lecture\"  \n\ndata <- read_csv(here::here(path_data, \"3_single_mean_fps.csv\"), \n                 show_col_types = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 96\nColumns: 2\n$ subid <dbl> 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 111, 112, 113, 1…\n$ fps   <dbl> 19.4909278, 48.4069444, -22.5285000, 6.7237833, 89.6587222, 40.5…\n```\n\n\n:::\n:::\n\n\n-----\n\n## Descriptives and Univariate Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  summarise(n = n(),\n            mean = mean(fps),\n            sd = sd(fps),\n            min = min(fps),\n            max = max(fps))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n      n  mean    sd   min   max\n  <int> <dbl> <dbl> <dbl> <dbl>\n1    96  32.2  37.5 -98.1  163.\n```\n\n\n:::\n:::\n\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = fps)) +\n  geom_histogram(color = \"black\", fill = \"light grey\", bins = 20) + \n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) \n```\n\n::: {.cell-output-display}\n![](03_single_mean_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## FPS Experiment: The Inference Details\n\n**Goal:** Determine if our shock threat procedure is effective at potentiating startle (increasing startle during threat relative to safe).  \n\n\\\n\n- Create a simple model of FPS scores in the population\n  - $\\text{FPS}=\\beta_0$   \n\n- Collect sample of $N=96$ to estimate $\\beta_0$\n\n- Calculate sample parameter estimate ($b_0$) that minimizes SSE in sample\n\n- Use $b_0$ to test hypotheses\n\n  - $H_0: \\beta_0 = 0$    \n  - $H_a: \\beta_0 \\neq 0$\n\n\n-----\n\n## Estimating a One Parameter Model in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(fps ~ 1, data = data) #<1>\n```\n:::\n\n\n1. Here we are fitting a linear model with FPS regressed on the intercept. In other words, we are fitting a model that predicts FPS using only the mean.    \n\n-----\n\nWe can pull out the errors ($e_i=Y_i-\\hat{Y}i$) for each observation in the sample using `residuals()`\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           1            2            3            4            5            6 \n -12.6999127   16.2161040  -54.7193405  -25.4670572   57.4678817    8.3829373 \n           7            8            9           10           11           12 \n  -2.7175738  -16.8541238   19.6643817   58.6873817   78.7543262   35.0963817 \n          13           14           15           16           17           18 \n -29.4627960   72.7258928  -31.7275460   36.5672151   19.1260706  -30.9964738 \n          19           20           21           22           23           24 \n   1.5669373   11.7176040    9.3662151  -25.3710072 -130.2886183   53.1913817 \n          25           26           27           28           29           30 \n  29.8681317   59.8164373  -14.1219516   34.7095484   17.9774928   47.3338484 \n          31           32           33           34           35           36 \n  61.4058262   67.7537262  104.6339928   36.5526595   14.2658262  -16.7506349 \n          37           38           39           40           41           42 \n -29.6592294   12.9909373   20.9858817  -29.1695572  -24.1598966  -19.2076849 \n          43           44           45           46           47           48 \n  11.7108262  -25.2434516  -18.4250627  -20.3317905   -8.4337683  -18.0094960 \n          49           50           51           52           53           54 \n -12.7704849    3.9210484  -58.2597294  -35.5108960  -32.0183927   -1.7377294 \n          55           56           57           58           59           60 \n   0.3123817  -35.5405016  -12.5921183   25.0772151  -20.6439405   37.4066428 \n          61           62           63           64           65           66 \n   9.3974373  130.5457706    5.2138262  -13.0036627   -9.8150183  -27.4784549 \n          67           68           69           70           71           72 \n  17.0578817   27.5951151  -28.0089794  -28.5735072  -23.4260627    4.5087151 \n          73           74           75           76           77           78 \n  77.8639373  -21.4575572  -18.5716738  -17.1700072   27.4325484  -26.4386960 \n          79           80           81           82           83           84 \n -18.1054016    6.1488262  -14.5139683    1.6943262  -21.4997294  -25.3833322 \n          85           86           87           88           89           90 \n -26.9358794  -17.5872294  -25.7722738    4.8073817  -26.9565572  -32.1845627 \n          91           92           93           94           95           96 \n -31.0086183  -34.0540127  -17.4630572  -31.4756127  -31.8114616  -15.9328183 \n```\n\n\n:::\n:::\n\n\n-----\n\nWe can also easily calculate the SSE with the following code:\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(residuals(m)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 133888.3\n```\n\n\n:::\n:::\n\n\n\\\n\n- This tells us about how well the model fits the data.  \n- Specifically it is the sum of the squared differences between the predicted values and the actual participant scores\n\n-----\n\nWe can get the predicted value for each individual in the sample using this model with the function `predict()`.   \n\n$\\hat{Y}=32.19$\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6        7        8 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n       9       10       11       12       13       14       15       16 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      17       18       19       20       21       22       23       24 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      25       26       27       28       29       30       31       32 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      33       34       35       36       37       38       39       40 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      41       42       43       44       45       46       47       48 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      49       50       51       52       53       54       55       56 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      57       58       59       60       61       62       63       64 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      65       66       67       68       69       70       71       72 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      73       74       75       76       77       78       79       80 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      81       82       83       84       85       86       87       88 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n      89       90       91       92       93       94       95       96 \n32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 32.19084 \n```\n\n\n:::\n:::\n\n\n-----\n\nWe also may want to look at the parameter estimates \n\n- We will also call these model (or regression) coefficients (In this case we are only looking at the intercept)\n- We can use the `tidy()` function from the `broom` package to do this\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     32.2      3.83      8.40 4.26e-13\n```\n\n\n:::\n:::\n\n\n\\\n\nThe estimate is $b_0$, the unbiased sample estimate of $\\beta_0$, and its standard error.  \n\nIt is also called the intercept in regression (more on this later).   \n\n$\\hat{Y}_i=b_0=32.2$\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     32.2      3.83      8.40 4.26e-13\n```\n\n\n:::\n:::\n\n\n\\\n\nThe statistic is the t-statistic to test the $H_0$ that $\\beta_0=0$.   \n\nThe probability (p-value) of obtaining a sample $b_0=32.2$ if $H_0$ is true ($\\beta_0=0$) is < .0001.\n\n\n\\\n\n-----\n\n## Sampling Distribution: Testing Inferences About $\\beta_0$   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     32.2      3.83      8.40 4.26e-13\n```\n\n\n:::\n:::\n\n\n\\\n\n<span style=\"color: red;\">Describe the logic of how the standard error, t statistic and p-value were determined given your understanding of sampling distributions.</span>\n\n-----\n\n<span style=\"color: red;\">Describe the logic of how the standard error, t statistic and p-value were determined given your understanding of sampling distributions.</span>\n\n\\\n\n<span style=\"color: blue;\">1. Establish null and alternative hypotheses.</span>\n\n$H_0: \\beta_0 = 0; H_a: \\beta_0 \\neq 0$\n\n\\\n\n<span style=\"color: blue;\">2. If $H_0$ is true, the sampling distribution for $\\beta_0$ will have a mean of 0. We can estimate standard deviation of the sampling distribution with SE for $b_0$.</span>    \n\n\\\n\n<span style=\"color: blue;\">3. $b_0$ is approximately 8 standard deviations above the expected mean of the distribution if $H_0$ is true.</span>  \n\n$t(df=N-P)=\\frac{b_0-0}{\\text{SE}_{b_0}}=\\frac{32.2-0}{3.8}=8.40$   \n\\\n\n[continue on next slide]\n\n-----\n\n<span style=\"color: blue;\">4. We can use `pt()` to calculate the exact $p$ value for this parameter estimate given $H_0$.</span>\n\n\\\n\n<span style=\"color: blue;\">The probability of obtaining a sample $b_0$ = 32.2 (or more extreme) if $H_0$ is true is very low (< .05). Therefore we reject $H_0$ and conclude that $\\beta_0 \\neq 0$ and $b_0$ is our best (unbiased) estimate of it.</span>\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\npt(8.40,95,lower.tail=FALSE) * 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0000000000004293253\n```\n\n\n:::\n:::\n\n\n\\\n\n<span style=\"color: blue;\">Of courese, we don't need to calculate the p-value with `pt()` because we got it directly from `tidy()`</span>\n\n-----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(b0 = seq(-40,40,.01),\n       probability = dt(b0/broom::tidy(m)$std.error, m$df.residual)) |> \n  ggplot(aes(x = b0, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = broom::tidy(m)$estimate, color = \"red\") +\n  labs(title = \"Sampling Distribution for b0\")\n```\n\n::: {.cell-output-display}\n![](03_single_mean_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Statistical Inference and Model Comparisons\n\nStatistical inference about parameters is fundamentally about model comparisons.\n  \n- You are implicitly (t-test of parameter estimate) or explicitly (F-test of model comparison) comparing two different models of your data.   \n\n- We follow Judd et al and call these two models the *compact model* and the *augmented model*.    \n\n- The compact model will represent reality as the null hypothesis predicts. The augmented model will represent reality as the alternative hypothesis predicts.  \n\n- The compact model is simpler (fewer parameters) than the augmented model. It is also nested in the augmented model (i.e. a subset of parameters).   \n\n-----\n\n## Model Comparisons: Testing Inferences about $\\beta_0$\n\n$\\hat{\\text{FPS}_i}=\\beta_0$    \n\n$H_0: \\beta_0 = 0$     \n\n$H_a: \\beta_0 \\neq 0$     \n\n\\\n\\\n\nCompact model: $\\hat{\\text{FPS}_i}=0$    \nAugmented model: $\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)$    \n\n\\\n\nWe estimate 0 parameters ($P=0$) in this compact model.   \nWe estimate 1 parameter ($P=1$) in this augmented model.   \n\n\\\n\\\n\nChoosing between these two models is equivalent to testing if $\\beta_0 = 0$ as you did with the t-test. \n\n-----\n\n## Model Comparison Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = \"\", y = fps)) +\n  geom_jitter(width = 0.1, alpha = .6, size = 2) +\n  theme(axis.line.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  xlab(NULL) +\n  geom_hline(yintercept = 0, color = \"red\", linewidth = 1) +\n  geom_hline(yintercept = broom::tidy(m)$estimate, color = \"blue\", linewidth = 1)\n```\n\n::: {.cell-output-display}\n![](03_single_mean_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n-----\n\n## Model Comparisons: Testing Inferences about $\\beta_0$ (Continued)\n\nCompact model: $\\hat{\\text{FPS}_i}=0$    \nAugmented model: $\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)$     \n\n\\\nWe can compare (and choose between) these two models by comparing their total error (SSE) in our sample.  \n\n\n$\\text{SSE}_c = \\sum(Y_i-0)^2$   \n\n$\\text{SSE}_a = \\sum(Y_i-\\hat{Y}_i)^2$   \n\n\n-----\n\nWe can calculate $\\text{SSE}_c = \\sum(Y_i-\\hat{Y}_i)^2$ as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((data$fps - 0)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 233368.3\n```\n\n\n:::\n:::\n\n\n\\\n\nWe can calcuate $\\text{SSE}_a = \\sum(Y_i- 32.2 )^2$ as follows:  \n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((data$fps - broom::tidy(m)$estimate)^2) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 133888.3\n```\n\n\n:::\n:::\n\n\n\\\n\nor\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(residuals(m)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 133888.3\n```\n\n\n:::\n:::\n\n\n-----\n\nCompact model: $\\hat{\\text{FPS}_i}=0$      \nSSE = 233368.3   \nP = 0  \n\n\\\n\nAugmented model: $\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)$     \nSSE = 133888.3    \nP = 1   \n\n\\\n\\\n\n$F(P_a-P_c, N-P_a) = \\frac{(\\text{SSE}_c-\\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$    \n\n$F(1-0, 96 -1)= \\frac{(233368.3 - 133888.3)/(1-0)}{133888.3 / (96 - 1)}$  \n\n$F(1, 95 )= 70.59, p < .0001$\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\npf(70.59, 1, 95, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0000000000004255967\n```\n\n\n:::\n:::\n\n\n-----\n\n## Sampling Distribution vs. Model Comparison\n\nThe two approaches to testing $H_0$ about parameters ($\\beta_0, \\beta_j$) are statistically equivalent.   \n\n\\\n\nThey are complementary approaches with respect to conceptual understanding of GLMs.   \n\n\\\n\n**Sampling Distribution**   \n\n- Focus on population parameters and their estimates.   \n- Tight connection to sampling and probability distributions.   \n- Understanding of SE (sampling error/power; confidence intervals, graphic displays).  \n\n\\\n\n**Model Comparison**   \n\n- Focus on models themselves.   \n- Highlights model fit (SSE) and model parsimony (P).  \n- Clearer link to PRE ($\\eta_p^2$). \n- Test comparisons that differ by >1 parameter (discouraged).\n\n-----\n\n## Effect Sizes\n\nYour parameter estimates are descriptive. They describe effects in the original units of the predictors and DV. Report them in your paper.   \n\n\\\n\nThere are many other effect size estimates available. You will learn two that we prefer.   \n\n- Partial eta squared ($\\eta_p^2$):  Judd et al call this PRE (proportional reduction in error).   \n\n- Eta squared ($\\eta^2$): This is also commonly referred to as $\\Delta R^2$ in regression. \n\n-----\n\nCompact model: $\\hat{\\text{FPS}_i}=0$      \nSSE = 233368.3   \nP = 0  \n\n\\\n\nAugmented model: $\\hat{\\text{FPS}_i}=\\beta_0 (\\approx b_0)$     \nSSE = 133888.3    \nP = 1   \n\n\\\n\nHow much was the error reduced in the augmented model relative to the compact model?   \n\n\n$\\frac{\\text{SSE}_c-\\text{SSE}_a}{\\text{SSE}_c} = \\frac{233368.3 - 133888.3}{233368.3} = 0.426$   \n\n\\\n\nOur more complex model that includes $\\beta_0$ reduces prediction error (SSE) by approximately 43%. Not bad!\n\n-----\n\n## Confidence Interval for $b_0$   \n\nA confidence interval (CI) is an interval for a parameter estimate in which you can be fairly confident that you will capture the true population parameter (in this case, $\\beta_0$).\n\n- Most commonly reported is the 95% CI. \n- Across repeated samples, 95% of the calculated CIs will include the population parameter.  \n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m) #<1>\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 24.58426 39.79742\n```\n\n\n:::\n:::\n\n\n1. Use the `confint()` function to calculate confidence intervals. The default is to provide 95% CIs, but you can change this using the `level` parameter if you wish.   \n\n-----\n\n<span style=\"color: red;\">Question: Given what you now know about confidence intervals and sampling distributions, what should the formula be?</span>\n\n-----\n\n<span style=\"color: red;\">Question: Given what you now know about confidence intervals and sampling distributions, what should the formula be?</span>\n\n\\\n\n<span style=\"color: blue;\">$\\text{CI}(b_0)= b_0 \\pm t(\\alpha;N-P) * \\text{SE}_{b_0}$</span>   \n\n\\\n\n<span style=\"color: blue;\">For the 95% confidence interval this is approximately 2 SEs around our unbiased estimate of $\\beta_0$.</span>\n \n-----\n\n<span style=\"color: red;\">Question: How can we tell if a parameter is *significant* using only the confidence interval?</span>\n\n-----\n\n<span style=\"color: red;\">Question: How can we tell if a parameter is *significant* using only the confidence interval?</span>\n\n\\\n\n<span style=\"color: blue;\">If a parameter estimate $\\neq$ 0 at $\\alpha$ = .05, then the 95% confidence interval for its parameter estimate should not include 0.</span>   \n\n\\\n\n<span style=\"color: blue;\">This is also true for testing whether the parameter estimate is equal to any other non-zero value for the population parameter.</span>\n \n-----\n\n## The one parameter (mean-only) model: Special Case  \n\n<span style=\"color: red;\">Question: What special case (specific analytic test) is statistically equivalent to the test of the null hypothesis: $\\beta_0$ = 0 in the one parameter model?</span>\n\n-----\n\n<span style=\"color: red;\">Question: What special case (specific analytic test) is statistically equivalent to the test of the null hypothesis: $\\beta_0$ = 0 in the one parameter model?</span>\n\n\\\n\n<span style=\"color: blue;\">The one sample t-test testing if a population mean = 0.</span>     \n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(data$fps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  data$fps\nt = 8.4015, df = 95, p-value = 0.0000000000004261\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 24.58426 39.79742\nsample estimates:\nmean of x \n 32.19084 \n```\n\n\n:::\n:::\n\n\n-----\n\n## Testing $\\beta_0$ = non-zero values  \n\n<span style=\"color: red;\">Question: How could you test an $H_0$ regarding $B_0$ = some value other than 0 (e.g., 10)? HINT: There are at least **three** methods.</span>\n\n-----\n\n<span style=\"color: red;\">Question: How could you test an $H_0$ regarding $B_0$ = some value other than 0 (e.g., 10)? HINT: There are at least **three** methods.</span>\n\n\\\n\n<span style=\"color: blue;\">**Option 1:**</span> Compare SSE for the augmented model ($\\hat{Y}_i= \\beta_0$) to SSE from a different compact model for this new $H_0$ ($\\hat{Y}_i= 10$).   \n\n\\\n\n<span style=\"color: blue;\">**Option 2:**</span> Recalculate t-statistic using this new $H_0$.  \n\n$t= \\frac{b_0 - 10}{\\text{SE}_{b_0}}$   \n\n\\\n\n<span style=\"color: blue;\">**Option 3:**</span> Does the confidence interval for the parameter estimate contain this other value? No p-value provided.   \n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 24.58426 39.79742\n```\n\n\n:::\n:::\n\n\n-----\n\n## Intermission...\n\nOne parameter ($\\beta_0$) *mean-only* model    \n\n- **Description:** $b_0$ describes mean of $Y$.\n- **Prediction:** $b_0$ is predicted value that minimizes sample SSE.\n- **Inference:** Use $b_0$ to test id $\\beta_0 = 0$ (default) or any other value. One sample t-test.   \n\n\\\n\\\n\nTwo parameter ($\\beta_0, \\beta_1$) model    \n\n- **Description:** $b_1$ describes how $Y$ changes as a function of $X_1$. $b_0$ describes expected value of $Y$ ar specific value (0) for $X_1$. \n- **Prediction:** $b_0$ and $b_1$ yield predicted values that vary by $X_1$ and minimize SSE in sample.\n- **Inference:** Test if $\\beta_1 = 0$. Pearson's $r$; independent sample t-test. Test if $\\beta_0=0$. Analogous to one-sample t-test controlling for $X_1$, if $X_1$ is mean-centered. Very flexible!",
    "supporting": [
      "03_single_mean_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}