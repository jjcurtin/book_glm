{
  "hash": "e65a046477a083171bdbe097e3a94ec8",
  "result": {
    "engine": "knitr",
    "markdown": "--- \noutput: html_document \neditor_options:  \n  chunk_output_type: console\n--- \n\n\n\n \n\n# Inferences about two predictors (multiple regression without interaction)\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Multiple Regression 2+ Predictors\n\n- Consider how the concepts we have discussed so far generalize to the 2 predictor (3 parameter) model.  \n- We will start with 2 quantitative predictors example. Will continue with 1 quantitative and 1 dichotomous predictor example.  \n- Learn how to quantify, test, and interpret ‘partial’ effects:\n    - $b_j$\n    - $\\Delta R^2, \\eta_p^2$\n- Multicollinearity\n- Text, table and figure descriptions of results\n- Generalization to >2 predictors is straightforward (*Unit 7*).\n\n-----\n\n## Benefits of Multiple Predictors\n\n1. **Statistical power**:  Goal is to increase power to test focal predictor’s effect on DV by adding it to model that contains additional known predictors of DV.\n\n2. **Additional explanatory power**: Goal is to demonstrate that focal predictor adds explanatory power above and beyond other predictor(s) [Unique effect controlling for other predictors].\n\n3. **Efficiency**:  Can test focal effects of two predictors in one study (each benefiting from increased power per point 1).\n\n4. **Mediation**: We have identified a known cause of a DV.  We add a new focal predictor to test if the effect of our known causal IV on the DV is mediated by our focal predictor (i.e., identify “mechanism” of IV effect).\n\n-----\n\n## Alcohol and Stress Response Dampening (SRD)\n\nTest for Alcohol *Stress response dampening*   \n\nManipulate BAC (0.00% - 0.15%)  \n\nStressor Task (threat of unpredictable shock)  \n\nMeasure Stress Response (Fear potentiated startle)  \n\n-----\n\n## Two Parameter (1 Predictor) Model\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_2 <- lm(fps ~ bac, data = data)\n\nm_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Describe the interpretation of $b_1$ (coefficient for BAC) and its significance test.</span> \n\n\n-----\n\n<span style=\"color: blue;\">$b_1$ *describes* the relationship between BAC and FPS in the units of each measure. FPS will decrease by 184 µV for every 1% increase in BAC (It will decrease by 1.84µV for every .01 increase in BAC).</span>  \n\n<span style=\"color: blue;\">The significance test for $\\beta_1$ tests the null hypothesis that the population relationship between BAC and FPS is 0 (i.e., $\\beta_1$ = 0, no relationship). We fail to reject this $H_0$. Conclude that alcohol does not affect FPS.</span>\n\n-----\n\n## Testing Inferences about $\\beta_1$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n\n$H_0: \\beta_1 = 0$  \n$H_a: \\beta_1 \\neq 0$  \n\n<span style=\"color: red;\">Question: What could we change about the sampling distribution that would make this $b_1$ be less probable given $H_0$ so that we reject the Null?</span> \n\n-----\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual)) |> \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line() +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  labs(title = \"Sampling Distribution for b1\")\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n<span style=\"color: blue;\">If the standard deviation of the sampling distribution (its standard error) was smaller so that the distribution was narrower, $b_1$ would be less probable given $H_0$.</span>\n\n-----\n\n\n## Standard Errors of GLM Coefficients\n\nThe formula for the standard error for a coefficient $b_j$ in **multiple** (more than one predictor) regression is:  \n\n$\\text{SE}_{bj} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R^2_y)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R^2_j)}}$  \n\n$R^2_j$ = variance in $X_j$ accounted for by all other predictors in the model (i.e., how redundant is $X_{a_j}$ in model?). Literally, predict $X_j$ as DV with all other predictors as IVs.  \n\n**Note: Formula for the standard error for $b_0$ is different.**   \n\nFor the one predictor model it is (need matrix notation for $k>1$):  \n\n$\\sqrt {\\frac{\\text{SSE}}{N-P}}*\\sqrt{\\frac{1}{N}+\\frac{X^2}{(N-1)s_x^2}}$\n\n-----\n\n## SE for $b_j$ and $R^2$\n\n$\\text{SE}_{b_j}= \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{(N-P)}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\nIf we increase $R_y^2$, we would decrease the SE for our regression coefficient.  \n\n-----\n\n## Model Comparison: Testing Inferences about $\\beta_1$\n\n$H_0: \\beta_1 = 0; H_a: \\beta_1 \\neq 0$  \n\n\n<span style=\"color: red;\">Question: What two models are you comparing when you test hypotheses about $\\beta_1$ for BAC? Describe the logic.</span>\n\n-----\n\n<span style=\"color: blue;\">Compact model: $\\hat{\\text{FPS}}_i = \\beta_0+0*\\text{BAC}_i$</span>    \n$P_c=1$     \n$\\text{SSE}_c=133888.3$     \n\n\n<span style=\"color: blue;\">Augmented model: $\\hat{\\text{FPS}}_i = \\beta_0+\\beta_1*\\text{BAC}_i$</span>    \n$P_a=2$   \n$\\text{SSE}_a=128837.1$  \n\\\n$F(P_a-P_c, N-P_a) = \\frac{\\text{SSE}_c-\\text{SSE}_a/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$\n\nF(1,94) = 3.685, p = 0.058\n\n-----\n\n\n$F(P_a-P_c, N-P_a) = \\frac{\\text{SSE}_c-\\text{SSE}_a/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$  \n\n\n<span style=\"color: red;\">Question: What could you change from this model comparison perspective to increase $F$ and probability to reject the $H_0$ about $\\beta_1$?</span>\n\n-----\n\n<span style=\"color: blue;\">Make $\\text{SSE}_a$ smaller by explaining more variance in $Y_i$.</span>  \n\n\n<span style=\"color: blue;\">Of course, $R^2 = \\frac{\\text{SSE}_\\text{mean-only} - \\text{SSE}_a}{\\text{SSE}_\\text{mean-only} }$</span>  \n\n\n<span style=\"color: blue;\">If you decrease $\\text{SSE}_a$ or increase model $R^2$, you will have more power to reject $H_0$ regarding parameter estimates.</span>\n\n-----\n\n## Two Parameter (1 Predictor) Model (Continued)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: What can we do analytically to decrease SSE (increase model $R^2$)?</span>\n\n-----\n\n<span style=\"color: blue;\">Include another predictor (*covariate*) in the model that accounts for additional variance in $Y$ (reduces SSE).</span>  \n\n<span style=\"color: blue;\">Ideally, this covariate should be orthogonal (uncorrelated) with the other predictor (BAC).</span>  \n\n<span style=\"color: blue;\">In this experiment, I could have measured another predictor of stress response, Trait Anxiety (TA). TA might be expected to be a robust predictor of FPS. It also should be uncorrelated with BAC because I manipulated BAC.</span>\n\n-----\n\n## Open, View, & Preliminary Checks\n\n\n\n\n::: {.cell messages='false'}\n\n```{.r .cell-code}\noptions(conflicts.policy = \"depends.ok\") \nlibrary(tidyverse)\nlibrary(patchwork)\ntheme_set(theme_classic()) \n\n\npath_data <- \"data_lecture\" \n\ndata <- read_csv(here::here(path_data, \"6_two_predictors_fps.csv\"),\n                 show_col_types = FALSE) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 96\nColumns: 4\n$ subid <chr> \"0125\", \"0013\", \"0113\", \"0116\", \"0111\", \"0014\", \"0124\", \"0022\", …\n$ bac   <dbl> 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, …\n$ ta    <dbl> 110, 120, 35, 119, 26, 103, 52, 34, 208, 34, 254, 84, 249, 163, …\n$ fps   <dbl> -98.0977778, -22.5285000, 0.4632944, 1.1943667, 2.7280444, 6.723…\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim_without_charts(data)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |data |\n|Number of rows           |96   |\n|Number of columns        |4    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |1    |\n|numeric                  |3    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|subid         |         0|             1|   4|   4|     0|       96|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|    p0|   p25|    p50|    p75|   p100|\n|:-------------|---------:|-------------:|------:|------:|-----:|-----:|------:|------:|------:|\n|bac           |         0|             1|   0.06|   0.04|   0.0|  0.02|   0.06|   0.08|   0.14|\n|ta            |         0|             1| 147.61| 105.73|  10.0| 73.75| 119.00| 208.75| 445.00|\n|fps           |         0|             1|  32.19|  37.54| -98.1|  6.79|  19.46|  50.46| 162.74|\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_fps <- data |> \n  ggplot(aes(x = fps)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  scale_x_continuous(breaks = c(-100, -50, 0, 50, 100, 150, 200)) +\n  geom_rug(color = \"red\")\n\nplot_bac <- data |> \n  ggplot(aes(x = bac)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0,\n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\")\n\nplot_ta <- data |> \n  ggplot(aes(x = ta)) +\n  geom_histogram(aes(y = after_stat(density)), boundary = 0, \n                 color = \"black\", fill = \"light grey\", bins = 10) +\n  geom_density() +\n  geom_rug(color = \"red\") \n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_fps + plot_bac + plot_ta\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  select(where(is.numeric)) |> \n           psych::corr.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:psych::corr.test(x = select(data, where(is.numeric)))\nCorrelation matrix \n      bac    ta   fps\nbac  1.00 -0.02 -0.19\nta  -0.02  1.00  0.44\nfps -0.19  0.44  1.00\nSample Size \n[1] 96\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n     bac   ta  fps\nbac 0.00 0.87 0.12\nta  0.87 0.00 0.00\nfps 0.06 0.00 0.00\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = bac, y = fps)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = ta, y = fps)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  ggplot(aes(x = bac, y = ta)) +\n  geom_point(alpha = .6)\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n## The Two Predictor and General Linear Models\n\nDATA = MODEL + ERROR\n\n**Two Predictor Model for Sample Data**  \n\n$Y_i=b_0+b_1X_1+b_2X_2+e_i$  \n\n$\\hat{Y_i}=b_0+b_1X_1+b_2X_2$  \n\n\n**$k$ Predictor Model for Sample Data**   \n\n$Y_i=b_0+b_1X_1+...+b_kX_k+e_i$  \n\n$\\hat{Y_i}=b_0+b_1X_1+...+b_kX_k$  \n\n-----\n\n## Testing BAC in a Three Parameter Model (2 Predictors)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_3 <- lm(fps ~ bac + ta, data = data)\n\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n\n$\\hat{\\text{FPS}} = 19.4 + -177 * \\text{BAC} + 0.2 * \\text{TA}$\n\n\n<span style=\"color: red;\">Question: What parameter estimate is used to test our research question about the effect of BAC? What are our $H_0$ and $H_a$ for the associated population parameter?</span>\n\n-----\n\n<span style=\"color: blue;\">We use $b_1$ (-177) to test our hypothesis about the population effect of BAC ($\\beta_1$).</span>\n\n\n<span style=\"color: blue;\">$H_0:\\beta_1=0; H_a: \\beta_1 \\neq 0$</span>\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Describe conclusion and logic of the test of $H_0:\\beta_1=0$ from sampling distribution perspective.</span>  \n\n-----\n\n<span style=\"color: blue;\">If $H_0$ is true, we expect a sampling distribution for $b_1$ to have a mean of 0 and an SE of 86.6 (red curve below).</span>  \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_3), term == \"bac\")$std.error, m_3$df.residual)) |> \n  ggplot(aes(x = b1, y = probability)) +\n  geom_line(color = \"red\") +\n  geom_vline(xintercept = subset(broom::tidy(m_3), term == \"bac\")$estimate, \n             color = \"black\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n<span style=\"color: blue;\">A sample $b_1$ = -177.0 is unlikely (about 2 standard deviations below mean; p = .0437). Therefore, we reject our $H_0$ and conclude that $\\beta_1 \\neq 0$.</span>     \n\n<span style=\"color: blue;\">Conclusion is that BAC affects FPS.</span>\n\n-----\n\n\n<span style=\"color: red;\">Describe conclusion and logic of the test of $H_0:\\beta_1=0$ from model comparison perspective.</span> \n\n-----\n\n$H_0: \\beta_1 = 0; H_a: \\beta_1 \\neq 0$\n\n<span style=\"color: blue;\">**Compact Model:** $\\hat{\\text{FPS}}=\\beta_0 + 0*\\text{BAC}+\\beta_2*\\text{TA}$</span>    \n\n$\\text{SSE}_c = 108547.9$  \n\n$P_c = 2$   \n\n\n<span style=\"color: blue;\">**Augmented Model:** $\\hat{\\text{FPS}}=\\beta_0 + \\beta_1*\\text{BAC}+\\beta_2*\\text{TA}$</span>    \n\n$\\text{SSE}_a = 103877.2$\n\n$P_c = 3$  \n\n$F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$   \n$F (1, 93) = 4.18, p = 0.0442$\n\n-----\n\n**Two parameter model test of BAC**   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 <- lm(fps ~ bac, data = data)\n\nbroom::tidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n\n$\\text{SSE} =$ 128837.1\n\n\n**Three parameter model test of BAC**   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n\n$\\text{SSE} =$ 103877.2\n\n<span style=\"color: red;\">Question: What changed about test of $\\beta_1$ (BAC effect) and why?</span> \n\n-----\n\n<span style=\"color: blue;\">SE for BAC and SSE decreased.</span>   \n\n<span style=\"color: blue;\">As a result, $R^2$ increases.</span>   \n\n**Two parameter model test of BAC**    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(m_2) |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1    0.0377\n```\n\n\n:::\n:::\n\n\n\n\n**Three parameter model test of BAC**   \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(m_3) |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.224\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Standard Error of Partial Regression Coefficient ($b_j$)\n\n$t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}$  \n\n$\\text{SE}_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n<span style=\"color: red;\">Question: What happens to $\\text{SE}_{b_j}$ as model $R^2$ ($R^2_y$) increases (holding other factors constant)?</span> \n\n-----\n\n<span style=\"color: blue;\">$\\text{SE}_{b_j}$ decreases as model $R^2$ increases. In other words, the sampling distribution get narrower.</span>\n\n-----\n\n<span style=\"color: red;\">Question: What happens to significance test of $b_j$ as $\\text{SE}_{b_j}$ decreases (holding other factors constant)?</span> \n\n-----\n\n<span style=\"color: blue;\">$t$ increases and associated p-value deceases (More Power!).</span>\n\n-----\n\n## Sampling Distributions and Power\n\n$t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}$  \n\n\n**Two parameter model test of BAC**   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n\n\n\n$t(96-2) = \\frac{-184 - 0}{95.9}$    \n\n$t(94) = -1.92$  \n\n$p = .0579$\n\n-----\n\n$t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}$  \n\n\n**Three parameter model test of BAC**   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n\n$t(96-3) = \\frac{-177 - 0}{86.6}$    \n\n$t(93) = -2.04$  \n\n$p = .0437$\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndistr_data <- tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_3), term == \"bac\")$std.error, m_3$df.residual),\n       group = \"3 parameter model\") |> \n  bind_rows(tibble(b1 = seq(-400,400,.01),\n       probability = dt(b1/subset(broom::tidy(m_2), term == \"bac\")$std.error, m_2$df.residual),\n       group = \"2 parameter model\"))\n\ndistr_data |> \n  ggplot(aes(x = b1, y = probability, color = group)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_vline(xintercept = subset(broom::tidy(m_2), term == \"bac\")$estimate, \n             color = \"red\") +\n  geom_vline(xintercept = subset(broom::tidy(m_3), term == \"bac\")$estimate, \n             color = \"blue\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n## Sampling Distributions and Precision\n\n$\\text{CI}_b=b \\pm t(\\alpha; N-P)*\\text{SE}_b$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %    97.5 %\n(Intercept)   29.45597 55.457721\nbac         -374.49261  6.308724\n```\n\n\n:::\n:::\n\n\n\n\n$\\Delta$ 380.801\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %     97.5 %\n(Intercept)    4.22130089 34.6411333\nbac         -348.98099457 -5.1177113\nta             0.08891558  0.2177329\n```\n\n\n:::\n:::\n\n\n\n\n$\\Delta$ 343.863\n\n-----\n\n\n## Standard Error of Partial Regression Coefficient ($b_j$; Continued)\n\n$t(N-P) = \\frac{b_j - 0}{\\text{SE}_{b_j}}$  \n\n$\\text{SE}_{b_j} = \\frac{s_y}{s_j}*\\frac{\\sqrt{(1-R_Y^2)}}{\\sqrt{(N-P)}}*\\frac{1}{\\sqrt{(1-R_j^2)}}$  \n\n$R^2_j$ = variance in $X_j$ accounted for by all other predictors in model (i.e., how redundant is $X_j$ in model?).  \n\n<span style=\"color: red;\">Question: What other factors affect SE for regression coefficients and how?</span>\n\n-----\n\n<span style=\"color: blue;\">Increasing $N$ decreases SE (increases power).</span>\n\n<span style=\"color: blue;\">Increasing $P$ increases SE (decreases power).</span>\n\n<span style=\"color: blue;\">Increasing $s_y$ increases SE (decreases power).</span>\n\n<span style=\"color: blue;\">Increasing $s_j$ decreases SE (increases power).</span>\n\n<span style=\"color: blue;\">Increasing $R_j^2$ increases SE (decreases power).</span>\n\n-----\n\n\n## Power and SSE in Two and Three Parameter Models\n\n**Two parameter model test of BAC**   \n\nCompact Model: $\\hat{\\text{FPS}}= 32.2 + 0*\\text{BAC}$   \n$\\text{SSE}_c = 133888.3$  \n$P_c$ = 1  \n\nAugmented Model: $\\hat{\\text{FPS}}= 42.5 + -184.1 *\\text{BAC}$   \n$\\text{SSE}_a = 128837.1$  \n$P_a$ = 2  \n\n\n**Three parameter model test of BAC**   \n\nCompact Model: $\\hat{\\text{FPS}}= 9.4 + 0*\\text{BAC} + 0.2 * \\text{TA}$   \n$\\text{SSE}_c = 108547.9$  \n$P_c$ = 2  \n\nAugmented Model: $\\hat{\\text{FPS}}= 19.4 + -177 *\\text{BAC} + 0.2 *\\text{TA}$   \n$\\text{SSE}_a = 103877.2$  \n$P_a$ = 3  \n\n\n$F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$ \n\n-----\n\n<span style=\"color: red;\">Question: How can you see the increase in power from the model comparison perspective?</span>\n\n-----\n\n**Two parameter model test of BAC** \n\n$F(2-1, 96-2) = \\frac{(133888.3 - 128837.1)/(2-1)}{128837.1 / (96-2)}$\n\n$F(1, 94) = \\frac{(5051.2)/(1)}{1370.6}$\n\n$F (1, 94) = 3.69, p = 0.0579$\n\n\n**Three parameter model test of BAC** \n\n$F(3-2, 96-3) = \\frac{(108547.9 - 103877.2)/(3-2)}{103877.2 / (99-3)}$\n\n$F(1, 93) = \\frac{(4670.7)/(1)}{1117}$\n\n$F (1, 93) = 4.18, p = 0.0442$\n\n\n<span style=\"color: blue;\">Decreased $\\text{SSE}_a$ in three parameter model. Flip side of increased model $R^2$.</span>\n\n-----\n\n$F(P_a - P_c, N - P_a) = \\frac{(\\text{SSE}_c = \\text{SSE}_a)/(P_a-P_c)}{\\text{SSE}_a/(N-P_a)}$   \n\n<span style=\"color: blue;\">Impact of $N$ on $P_a$ also clear.</span>  \n\n<span style=\"color: blue;\">Impact of $s_y$ and $s_{x_j}$ and multicollinearity less clear in formula.</span>  \n\n<span style=\"color: blue;\">Connection to precision of parameter estimation less clear in formula.</span>  \n\n-----\n\n## $R^2_j$ and Multicollinearity\n\n$t(N-P) = \\frac{b-0}{\\text{SE}_b}$   \n\n$\\text{CI}_b= b \\pm t(\\alpha; N-P)*\\text{SE}_b$   \n\nThis decrease in **power and precision** for model parameter estimates (regression coefficients) associated with redundancy among the predictors is called the *problem of Multicollinearity*.  \n\n-----\n\nIt is **not** sufficient to examine only bivariate correlations among predictors. To determine if a problem exists, calculate Variance Inflation Factors (VIF) for each predictor.  \n\n$\\text{VIF}_j = \\frac{1}{(1-R^2_j)}$   \n\nVIF tells you how much $\\text{SE}_{b_j}$ is increased because of redundancy. VIFs $\\ge$ 5 are considered problematic (SE increased by factor of 2.2).  \n\n-----\n\nWe can use `car::vif()` to calculate VIFs in R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     bac       ta \n1.000296 1.000296 \n```\n\n\n:::\n:::\n\n\n\n\nSPSS users may be more familiar with Tolerance ($X_i= 1- R^2_j$). As tolerance decreases toward 0, Multicollinearity increases.\n\n-----\n\nSolutions for Multicollinearity include:  \n\n1. Drop redundant variable.\n\n2. Factor analysis (e.g., PCA) to produce factors that reflect major sources of variance among the redundant predictors.\n\n3. This is only a problem for the variables in the model with high VIFs. If you don't care about testing them, this is not a problem. Generally, you only care about VIFs for your focal variable(s).\n\n-----\n\n## Interpretation of Multiple Regression Coefficients\n\n<span style=\"color: red;\">Question: What did the value of $b_1$ tell us in a regression model with one predictor?</span>\n\n-----\n\n<span style=\"color: blue;\">The change in $Y$ associated with a one unit increase in $X_1$.</span>    \n\n<span style=\"color: blue;\">For every 1 unit increase in $X_1$, there will be a $b_1$ unit increase in $Y$.</span>\n\n-----\n\n<span style=\"color: red;\">Question: What about $b_j$ with multiple (e.g., 2) predictors?</span>\n\n-----\n\n\n<span style=\"color: blue;\">The change in $Y$ associated with a one unit increase in $X_j$ *controlling for all other predictors in the model*. \"Controlling for\" means holding constant.</span>    \n\n<span style=\"color: blue;\">For every 1 unit increase in $X_j$, there will be a $b_j$ unit increase in $Y$ holding all other predictors in the model constant.</span>\n\n-----\n\n<span style=\"color: red;\">Question: Do hours of studying per week affect exam performance in 610?</span>   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata_exam <- read_csv(here::here(path_data, \"6_two_predictors_exam.csv\"), \n                 show_col_types = FALSE) \n\ndata_exam <- data_exam |> \n  mutate(study_hours_c = study_hours - mean(study_hours)) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 200\nColumns: 5\n$ subid         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ study_hours   <dbl> 9.197649, 10.897476, 10.508091, 12.297003, 8.789881, 11.…\n$ iq            <dbl> 106.39393, 103.02859, 98.11384, 110.42777, 93.35694, 114…\n$ exam          <dbl> 79.66300, 59.58193, 70.21451, 59.25460, 55.80582, 72.951…\n$ study_hours_c <dbl> -0.80235118, 0.89747646, 0.50809122, 2.29700259, -1.2101…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_exam_1 <- lm(exam ~ study_hours_c, data = data_exam)\n\nbroom::tidy(m_exam_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n<span style=\"color: blue;\">Yes, for every one hour of studying per week, students' exam scores increase by 2.3 points.</span>  \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_exam_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n\n\n<span style=\"color: red;\">Interpretation of $b_0$?</span>   \n\n-----\n\nMaybe study hours are related to exam scores only because intelligent students study more (learned early good study habits) and intelligent students do better on exams.   \n\n\n<span style=\"color: red;\">Question: How could you assess the unique effect of study_hours, controlling for IQ?</span>   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_exam |> \n  select(study_hours, iq, exam) |> \n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            study_hours   iq exam\nstudy_hours         1.0 0.50 0.40\niq                  0.5 1.00 0.54\nexam                0.4 0.54 1.00\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n<span style=\"color: blue;\">Model exam scores as a function of both study_hours and IQ. $b_1$ (effect of study hours) in this model is the unique effect of study_hours, controlling for IQ.</span>   \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_exam_2 <- lm(exam ~ study_hours_c + iq, data = data_exam)\n\nbroom::tidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: Do hours of studying per week affect performance in 610 after controlling for student IQ?</span>   \n\n-----\n\n<span style=\"color: blue;\">Yes, for every one hour of studying per week, students' exam scores increase by 1 point, controlling for IQ.</span>  \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n```\n\n\n:::\n:::\n\n\n\n\n\n<span style=\"color: red;\">Interpretation of $b_0$?</span>   \n\n-----\n\n<span style=\"color: red;\">Question: Why did the effect of study_hours get smaller after controlling for IQ?</span> \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_exam_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbroom::tidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n<span style=\"color: blue;\">1. When study_hours increases, IQ increases.</span>  \n\n<span style=\"color: blue;\">2. When IQ increases, exam scores increase.</span>  \n\n<span style=\"color: blue;\">3. The partial effect of study_hours on exam scores is smaller if IQ is not allowed to increase.</span>  \n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(m_exam_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      62.1      0.757     82.1  6.60e-155\n2 study_hours_c     2.34     0.379      6.18 3.65e-  9\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: blue;\">$b_1= 2.34$</span>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(m_exam_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     27.3      5.35        5.11 7.46e- 7\n2 study_hours_c    1.04     0.398       2.61 9.77e- 3\n3 iq               0.348    0.0530      6.56 4.62e-10\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: blue;\">$b_1= 1.04$</span>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(lm(iq ~ study_hours_c, data = data_exam))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)     100.       0.921    109.   2.10e-178\n2 study_hours_c     3.75     0.462      8.12 4.77e- 14\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: blue;\">$b_1= 3.75$</span>\n\n-----\n\n## Causal Models?\n\n**Total effect of study_hours**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nDiagrammeR::grViz(\"\ndigraph{\n  graph[rankdir=LR]\n  node [shape = box, fontsize = 5, width = .5, height = .2]\n    A [label = 'study_hours']\n    Y [label = 'exam']\n    A->Y [label = '2.34', fontsize = 5]\n}\n\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-e979b6f4d2806da700e8\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e979b6f4d2806da700e8\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"2.34\\\", fontsize = 5]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n------\n\n**Direct and indirect/spurious effects on study_hours**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nDiagrammeR::grViz(\"\ndigraph{\n  graph[rankdir=LR]\n  node [shape = box, fontsize = 5, width = .5, height = .2]\n    A [label = 'study_hours']\n    B [label = 'iq']\n    Y [label = 'exam']\n    A->Y [label = '1.04', fontsize = 5]\n    B->Y [label = '.35', fontsize = 5]\n    A->B [dir = both, label = '3.75', fontsize = 5]\n{ rank = same; A; B }\n}\n\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-bd70f4bd462c6c61f941\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-bd70f4bd462c6c61f941\">{\"x\":{\"diagram\":\"\\ndigraph{\\n  graph[rankdir=LR]\\n  node [shape = box, fontsize = 5, width = .5, height = .2]\\n    A [label = \\\"study_hours\\\"]\\n    B [label = \\\"iq\\\"]\\n    Y [label = \\\"exam\\\"]\\n    A->Y [label = \\\"1.04\\\", fontsize = 5]\\n    B->Y [label = \\\".35\\\", fontsize = 5]\\n    A->B [dir = both, label = \\\"3.75\\\", fontsize = 5]\\n{ rank = same; A; B }\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n**Direct + Indirect/Spurious = Total**\n\n1.04 +  (3.75 * 0.35) = 2.34\n\n-----\n\n## Interpretation of Multiple Regression Coefficients (Continued)\n\nLets return to our running example.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     42.5      6.55      6.48 0.00000000411\n2 bac           -184.      95.9      -1.92 0.0579       \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: Why did $b$ for BAC get smaller (less negative) when TA was controlled? Hint: consider the bivariate relationships between all variables.</span> \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata |> \n  select(bac, ta, fps) |> \n  cor() |> \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      bac    ta   fps\nbac  1.00 -0.02 -0.19\nta  -0.02  1.00  0.44\nfps -0.19  0.44  1.00\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n<span style=\"color: blue;\">1. When BAC increases, TA decreases.</span>  \n\n<span style=\"color: blue;\">2. When TA decreases FPS decreases. This negative *indirect/spurious effect* contributes to the total effect of BAC (negative $b$ for BAC in 1 predictor model).</span>  \n\n<span style=\"color: blue;\">3. The partial effect of BAC on FPS, if TA was not allowed to decrease (i.e., is controlled or held constant), is less negative than when it is allowed to naturally decrease as BAC increases.</span>  \n\n-----\n\n<span style=\"color: red;\">Question: In what situation would $b_j$ for a focal predictor not change when you added an additional predictor (covariate)? </span> \n\n-----\n\n<span style=\"color: blue;\">1. If the new predictor was completely uncorrelated (orthogonal) with the focal predictor in those sample data, there would be no change in the parameter estimate when you added this new predictor.</span>  \n\n<span style=\"color: blue;\">2. This is why uncorrelated predictors/covariates are considered easier to interpret when trying to increase power. If they are related to the DV, they will increase power (< SE) to test your focal variable but they will not change your estimate of the magnitude of the focal variables parameter estimate.</span>  \n\n<span style=\"color: blue;\">3. Completely orthogonal variables are typically only observed in experimental designs. However, small/trivial, nonsystematic sample $r$ will occur when population $r=0$.</span>\n\n-----\n\n**$\\text{SSR}=\\text{SSE}_c - \\text{SSE}_a$**\n\n\n**Two parameter model test of BAC** \n\n$F(2-1, 96-2) = \\frac{(133888.3 - 128837.1)/(2-1)}{128837.1 / (96-2)}$\n\n$F(1, 94) = \\frac{(5051.2)/(1)}{1370.6}$\n\n$F (1, 94) = 3.69, p = 0.0579$    \n\n**$\\text{SSR} = 5051.2$**\n\n\n**Three parameter model test of BAC** \n\n$F(3-2, 96-3) = \\frac{(108547.9 - 103877.2)/(3-2)}{103877.2 / (99-3)}$\n\n$F(1, 93) = \\frac{(4670.7)/(1)}{1117}$\n\n$F (1, 93) = 4.18, p = 0.0442$  \n\n**$\\text{SSR} = 4670.7$**\n\n-----\n\n## Model Effect Size: Coefficient of Determination ($R^2$)\n\n**Coefficient of Determination ($R^2$):**     \nProportion of variance in $Y$ explained by the set of all model predictors (i.e., proportion of variance in $Y$ predicted by all $X$s in model).    \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(m_3) |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.224\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n$R^2$ for any augmented model is:  \n\n$R^2 = \\frac{\\text{SSE}_\\text{mean-only} - \\text{SSE}_a}{\\text{SSE}_\\text{mean-only} }$   \n\n\n**Mean-Only Model: $\\hat{\\text{FPS}}_i=\\beta_0$**   \n\n$\\text{SSE}_{\\text{mean-only}} = 133888.3$\n\n**Augmented Model: $\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_a = 103877.2$\n\n$R^2 = \\frac{133888.3 - 103877.2}{133888.3} = 0.2242$\n\nIn this augmented model, $R^2$ describes the combined effect of BAC and TA. In more complex models, R2 will always be predictive strength of the set of all predictors. \n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_3 |> \n  broom::glance() |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.224\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Predictor Effect Size Options\n\n- As in the one predictor model, the parameter estimates in the two predictor model (and the $k$ predictor model) are attractive effect size estimates.   \n\n- In addition, there are variance based effect size estimates that are also attractive.\n\n- You have already learned about Partial eta-squared ($\\eta_p^2$), which Judd et al refer to as PRE.\n\n- You will now also learn about Delta $R^2$ ($\\Delta R^2$).\n\n-----\n\n## Variance Based Effect Sizes: $\\eta_p^2$ or PRE\n\n$\\eta_p^2$ or PRE describes how much SSE was reduced (proportionally) in the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.  \n\n**Compact Model: $\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_c = 108547.9$\n\n**Augmented Model: $\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_a = 103877.2$  \n\n\n<span style=\"color: red;\">Question: How much was the error reduced by estimating $\\beta_1$ for BAC?</span> \n\n-----\n\n<span style=\"color: blue;\">$\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c} = \\frac{108547.9 - 103877.2}{108547.9} = 0.043$</span>\n\n\n-----\n\n## Variance Based Effect Sizes: $\\Delta R^2$\n\n$\\Delta R^2$ is the increase in model $R^2$ for the augmented model where we estimated a specific parameter vs. the associated compact model where we fixed that parameter to 0.\n\n\n**$\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_c = 108547.9$\n\n$R^2 = 0.1893$\n\n\n**$\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_a = 103877.2$  \n\n$R^2 = 0.2242$\n\n\n$\\Delta R^2= 0.2242 -  0.1893 = 0.0349$\n\n\n-----\n\n$\\Delta R^2$ can also be defined with respect to SSE.  \n\n**Compact Model: $\\hat{\\text{FPS}}_i=\\beta_0+0*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_c = 108547.9$\n\n**Augmented Model: $\\hat{\\text{FPS}}_i=\\beta_0+\\beta_1*\\text{BAC}_i+ \\beta_2*\\text{TA}_i$**   \n\n$\\text{SSE}_a = 103877.2$  \n\n\n<span style=\"color: red;\">Question: How much was the error reduced by estimating $\\beta_1$ for BAC?</span> \n\n-----\n\n<span style=\"color: blue;\">$\\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}=\\frac{108547.9 - 103877.2}{133888.3} = 0.0349$</span>\n\n\n-----\n\n## Comparing Variance Based Effect Sizes\n\n$R^2 = \\frac{\\text{SSE}_{\\text{mean-only}} - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$\n\n\n$\\Delta R^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_{\\text{mean-only}}}$\n\n\n$\\eta_p^2 = \\frac{\\text{SSE}_c - \\text{SSE}_a}{\\text{SSE}_c}$\n\n-----\n\n$R^2$  \n\n- Describes proportion of explained variance in $Y$ explained by full model relative to total variance in $Y$.   \n- Can **not** be used for a specific predictor.   \n- Not used frequently in Psychology but useful in other fields.   \n\n\n$\\Delta R^2$  \n\n- Describes proportion of unique variance in $Y$ explained by $X_j$ relative to total variance in $Y$.  \n- If $X$s are orthogonal $\\Delta R^2$ will sum to $R^2$.   \n- Anchored to total $Y$ variance.  \n- Same denominator for all $X$s.  \n\n\n$\\eta_p^2$  \n\n- Describes proportion of reduction of unexplained variance (SSE) by adding $X_j$.  \n- $\\ge \\Delta R^2$ (stupid!).\n- SPSS reports it (stupid!).\n- Stable in experimental designs when additional IVs are added.  \n\n\n-----\n\n\n## Visualizing the Model\n\n$\\hat{\\text{FPS}}= 19.4 + -177 *\\text{BAC} + 0.2 *\\text{TA}$ \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(plot3D)\n\npreds_bac <- seq(min(data$bac), max(data$bac), length.out = 100)\npreds_ta <- seq(min(data$ta), max(data$ta), length.out = 100)\n\npreds_x <- expand.grid(bac = preds_bac, ta = preds_ta)\n\npreds_y <- matrix(predict(m_3, newdata = preds_x),\n                 nrow = 100,\n                 ncol = 100)\n\nscatter3D(data$bac, data$ta, data$fps, pch = 19, col = \"black\", ticktype = \"detailed\",\n          xlab = \"BAC\", ylab = \"TA\", zlab = \"FPS\",\n          surf = list(x = preds_bac, y = preds_ta, z = preds_y, alpha = .9, border = \"light grey\"))\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = mean(preds_ta)) \n\npreds<- preds_x |> \n  bind_cols(predict(m_3, preds_x, interval = \"confidence\", level = .95) |>\n  as_tibble()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = preds$bac, ymin = preds$lwr, ymax = preds$upr), alpha = 0.2) +\n  labs(x = \"BAC\",\n       y = \"FPS\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta = c(round(mean(preds_ta)), round(mean(preds_ta)-(sd(preds_ta)*1.5)), \n                              round(mean(preds_ta)+(sd(preds_ta)*1.5))))\n\npreds <- preds_x |> \n  bind_cols(predict(m_3, preds_x) |>\n  as_tibble()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds$bac, y = preds$value, color = as.factor(preds$ta)),\n              linewidth = 1) +\n  labs(x = \"BAC\",\n       y = \"FPS\",\n       color = \"TA\") \n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n-----\n\n\n<span style=\"color: red;\">Question: What would change if TA was mean centered?</span>   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)   19.4      7.66        2.54 0.0128    \n2 bac         -177.      86.6        -2.04 0.0437    \n3 ta             0.153    0.0324      4.73 0.00000807\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3 |> \n  broom::glance() |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.224\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata <- data |> \n  mutate(ta_c = ta - mean(ta))\n\nm_3_c <- lm(fps ~ bac + ta_c, data = data) \n\nm_3_c |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   42.1      5.91        7.12 2.28e-10\n2 bac         -177.      86.6        -2.04 4.37e- 2\n3 ta_c           0.153    0.0324      4.73 8.07e- 6\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3_c |> \n  broom::glance() |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.224\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n<span style=\"color: red;\">Question: What do you report and why?</span>   \n\n-----\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_3_c |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   42.1      5.91        7.12 2.28e-10\n2 bac         -177.      86.6        -2.04 4.37e- 2\n3 ta_c           0.153    0.0324      4.73 8.07e- 6\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n$\\eta_p^2$ effect size for BAC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_bac <- sum(residuals(lm(fps ~ ta_c, data = data))^2)\nsse_a <- sum(residuals(m_3_c)^2)\n\n(p_eta_bac <- (sse_c_bac - sse_a)/sse_c_bac)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04302933\n```\n\n\n:::\n:::\n\n\n\n\n$\\eta_p^2$ effect size for TA\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_ta <- sum(residuals(lm(fps ~ bac, data = data))^2)\n\n(p_eta_ta <- (sse_c_ta - sse_a)/sse_c_ta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.193732\n```\n\n\n:::\n:::\n\n\n\n\n$\\eta_p^2$ effect size for intercept\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_int <- sum(residuals(lm(fps ~ bac + ta - 1, data = data))^2)\n\n(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06472535\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n95% CI\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_3_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %     97.5 %\n(Intercept)   30.32490549 53.8033111\nbac         -348.98099457 -5.1177113\nta_c           0.08891558  0.2177329\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Describing Model Results\n\nWe regressed fear-potentiated startle (FPS) on Blood alcohol concentration (BAC). We included trait anxiety (mean-centered) as a covariate in this model to increase power to test substantive questions about BAC. We tested partial effects, controlling for all other predictors in the model, from the full model that included both predictors. We provide raw regression coefficients, 95% confidence intervals for these coefficients, and partial eta squared ($\\eta_p^2$) to quantify effect sizes for each predictor in Table 1.\n\nFPS was 42.1 $\\mu V$ for participants with 0.00% BAC and average trait anxiety, $t(93) = 7.12, p< .001$, indicating that our threat manipulation successfully increased FPS above zero when sober. As expected, the effect of the trait anxiety covariate was significant and reduced error variance by approximately 19%, $t(93)= 4.73, p< .001$.  FPS increased by 0.2 $\\mu V$ for every 1 unit increase in trait anxiety.\n\nAs predicted, the effect of BAC was significant and reduced error variance by approximately 4%, $t(93)= -2.04, p= .044$.  FPS decreased 1.8 $\\mu V$ for every .01% increase in BAC (see Figure 1).\n\n-----\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef <- m_3_c |> \n  broom::tidy() |> \n  select(term, estimate, statistic, p.value) |> \n  mutate(p.value = if_else(p.value < .001, \"<.001\", as.character(round(p.value, 2))))\n\nci <- confint(m_3_c) |> \n  round(2) |> \n  as_tibble() |> \n  unite(ci, `2.5 %`, `97.5 %`, sep = \", \") |> \n  mutate(ci = str_c(\"(\", ci, \")\"))\n\np_eta <- tibble (peta = c(p_eta_int, p_eta_bac, p_eta_ta)) \n\ntable <- coef |> \n  bind_cols(ci, p_eta) |> \n  mutate(` ` = factor(term, levels = c(\"(Intercept)\", \"bac\", \"ta_c\"),\n                     labels = c(\"Intercept\", \"Blood Alcohol Concentration\", \"Trait Anxiety\"))) |> \n  select(` `, \n         b = estimate,\n         `95% CI (b)` = ci,\n         `Partial eta squared` = peta,\n         t = statistic,\n         p = p.value) |> \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"))\n```\n:::\n\n\n\n\n\n-----\n\n**Table 1**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntable\n```\n\n::: {.cell-output-display}\n\n\n|                            |    b    |    95% CI (b)    | Partial eta squared |   t   |   p   |\n|:---------------------------|:-------:|:----------------:|:-------------------:|:-----:|:-----:|\n|Intercept                   |  42.06  |  (30.32, 53.8)   |        0.06         | 7.12  | <.001 |\n|Blood Alcohol Concentration | -177.05 | (-348.98, -5.12) |        0.04         | -2.04 | 0.04  |\n|Trait Anxiety               |  0.15   |   (0.09, 0.22)   |        0.19         | 4.73  | <.001 |\n\n\n:::\n:::\n\n\n\n\nNotes:   \n$R^2 = 0.224, F(2,93) = 13.44, p < .001$***    \nTrait anxiety was mean-centered   \n\n\n-----\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_x <- expand.grid(bac = seq(min(data$bac), max(data$bac), length.out = 100), \n                       ta_c = mean(data$ta_c))\n\npreds_pub <- predict(m_3_c, preds_x, se.fit = TRUE) |>\n  as_tibble() |>\n  mutate(upper = fit + se.fit,\n         lower= fit - se.fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pub <- ggplot() +\n  geom_point(aes(x = data$bac, y = data$fps), alpha = .6, size = 2) +\n  geom_line(aes(x = preds_x$bac, y = preds_pub$fit),\n              color = \"black\", linewidth = 1) +\n  geom_ribbon(aes(x = preds_x$bac, ymin = preds_pub$lower, ymax = preds_pub$upper), alpha = 0.2) +\n  labs(x = \"Blood alcohol concentration\",\n       y = \"Fear-potentiated startle\") +\n  xlim(c(0, .15)) +\n  ylim(c(-100, 200))\n```\n:::\n\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pub\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n:::\n\n\n\n\n-----\n\n\n<span style=\"color: red;\">Question: What other table should you consider in your results?</span>  \n\n-----\n\n\n<span style=\"color: blue;\">Table of simple correlations between variables. Can summarize with other important info fairly concisely. Could also include reliability, skewness, kurtosis, etc.</span>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(` ` = c(\"Trait Anxiety\", \"Blood Alcohol Concentration\", \"Mean\", \"SD\"),\n       `Fear Potentiated Startle` = c(cor(data$ta_c, data$fps), cor(data$bac, data$fps),\n                                      mean(data$fps), sd(data$fps)),\n       `Trait Anxiety` = c(NA, cor(data$bac, data$ta_c), mean(data$ta_c), sd(data$ta_c)),\n       `Blood Alcohol Concentration` = c(NA, NA, mean(data$bac), sd(data$bac))) |> \n  knitr::kable(digits = 2, align = c(\"l\", \"c\", \"c\", \"c\"))\n```\n\n::: {.cell-output-display}\n\n\n|                            | Fear Potentiated Startle | Trait Anxiety | Blood Alcohol Concentration |\n|:---------------------------|:------------------------:|:-------------:|:---------------------------:|\n|Trait Anxiety               |           0.44           |               |                             |\n|Blood Alcohol Concentration |          -0.19           |     -0.02     |                             |\n|Mean                        |          32.19           |     0.00      |            0.06             |\n|SD                          |          37.54           |    105.73     |            0.04             |\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Multiple Predictors: Dichotomous Predictor\n\n**Example:** Evaluate the effects of a new intervention for depression.   \n\n$N=200$ participants, randomly assigned to receive new invervention vs. standard of care control.  \n\nMeasure depression with CES-D pre- (baseline) and post-intervention.    \n\n<span style=\"color: red;\">Question: How do you evaluate the effect of the new intervention in a one predictor model?</span>   \n\n-----\n\n<span style=\"color: blue;\">1. Code one regressor for intervention group using dummy or zero-centered coefficients. How different?</span>\n\n<span style=\"color: blue;\">2. Regress depression scores from post-intervention on intervention group.</span>   \n\n<span style=\"color: blue;\">3. Test if $b_1$ for Intervention group is non-zero.</span>   \n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_int <- read_csv(here::here(path_data, \"6_two_predictors_intervention.csv\"),\n                     show_col_types = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_depress_1 <- lm(depress_post ~ intervention_group, data = data_int)\n\nm_depress_1 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: What do you conclude? What do you report?</span>   \n\n-----\n\n<span style=\"color: blue;\">The new intervention reduced CES-D depressions scores by approximately 3.6 units relative to the standard of care control group, $b = -3.6, t(198) = -2.40, p= .0175.$</span>\n\n-----\n\n<span style=\"color: red;\">Question: What else do you report?</span>   \n\n\n-----\n\n<span style=\"color: blue;\">PRE or $\\Delta R^2$. Means/SD of two groups or better - figure with means, standard error and raw (or residual) data points.</span>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(m_depress_1)$r.squared - broom::glance(lm(depress_post ~ 1, data = data_int))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02818089\n```\n\n\n:::\n:::\n\n\n\n\n*Note: In this case $\\Delta R^2$ is the same as model $R^2$ because there is only one predictor (i.e., $R^2$ in our compact/mean-only model is 0).*\n\n\n-----\n\n<span style=\"color: red;\">Question: What is a *better* analysis to address this question about the effect of the intervention and why is it better?</span>   \n\n-----\n\n<span style=\"color: blue;\">Control for baseline depressions scores to increase power.</span>  \n\n<span style=\"color: blue;\">Baseline depression should be uncorrelated (in the population) with intervention group because participants were randomly assigned to group. Therefore, including baseline depression should not systematically change $b_1$.</span>  \n\n<span style=\"color: blue;\">However, baseline scores are likely strong predictors of post-intervention scores. This will increase model $R^2$, reduce SSE, and therefore reduce the SE for intervention group. More power!</span>\n\n-----\n\n**Previous one predictor model**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_depress_1 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)           35.7       1.07     33.3  4.96e-83\n2 intervention_group    -3.64      1.52     -2.40 1.75e- 2\n```\n\n\n:::\n:::\n\n\n\n\n\n**Two predictor model**\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata_int <- data_int |> \n  mutate(depress_base_c = depress_base - mean(depress_base))\n\nm_depress_2 <- lm(depress_post ~ intervention_group + depress_base_c, data = data_int)\n\nm_depress_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n$\\eta_p^2$ effect size for intercept\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_int <- sum(residuals(lm(depress_post ~ intervention_group + depress_base_c - 1,\n                              data = data_int))^2)\nsse_a <- sum(residuals(m_depress_2)^2)\n\n(p_eta_int <- (sse_c_int - sse_a)/sse_c_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8893157\n```\n\n\n:::\n:::\n\n\n\n\n$\\eta_p^2$ effect size for intervention group\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_group <- sum(residuals(lm(depress_post ~ depress_base_c,\n                              data = data_int))^2)\n\n(p_eta_group <- (sse_c_group - sse_a)/sse_c_group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05251917\n```\n\n\n:::\n:::\n\n\n\n\n$\\eta_p^2$ effect size for baseline depression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse_c_base <- sum(residuals(lm(depress_post ~ intervention_group, data = data_int))^2)\n\n(p_eta_base <- (sse_c_base - sse_a)/sse_c_base)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2930961\n```\n\n\n:::\n:::\n\n\n\n\nModel $R^2$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_depress_2 |> \n  broom::glance() |> \n  select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.313\n```\n\n\n:::\n:::\n\n\n\n\n-----\n\n## Plot Option 1: Raw Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_int <- tibble(intervention_group = c(0,1),\n                depress_base_c = 0) \n\npreds_int <- preds_int|> \n  bind_cols(predict(m_depress_2, preds_int, se.fit = TRUE) |>\n  as_tibble() |>\n  mutate(upper = fit + se.fit,\n         lower= fit - se.fit))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_raw <- ggplot() +\n  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, \n               fill = as.factor(preds_int$intervention_group)),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(data_int$intervention_group), y = data_int$depress_post), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, \n                    ymax = preds_int$fit+preds_int$se.fit, \n                    x = as.factor(preds_int$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_raw\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Plot Option 2: Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_res <- data_int |> \n  bind_cols(tibble(resids = residuals(m_depress_2))) |> \n  mutate(depress_post_res = depress_post + resids)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_res <- ggplot() +\n  geom_col(aes(x = as.factor(preds_int$intervention_group), y = preds_int$fit, \n               fill = as.factor(preds_int$intervention_group)),\n           alpha = .4, color = \"black\") +\n  geom_jitter(aes(x = as.factor(preds_res$intervention_group), y = preds_res$depress_post_res), \n              width = .02, height = NULL) +\n  geom_errorbar(aes(ymin = preds_int$fit-preds_int$se.fit, \n                    ymax = preds_int$fit+preds_int$se.fit, \n                    x = as.factor(preds_int$intervention_group)), width = .4) +\n  scale_fill_manual(values = c(\"black\", \"light grey\")) +\n  scale_x_discrete(breaks = c(0, 1),\n                   labels = c(\"Standard of Care\", \"New Intervention\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Intervention Group\",\n       y = \"CES-D Score\") \n```\n:::\n\n\n\n\n-----\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_res\n```\n\n::: {.cell-output-display}\n![](06_two_predictors_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Adding a Third Predictor\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nm_depress_2 |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          36.0      0.905      39.8  4.19e-96\n2 intervention_group   -4.23     1.28       -3.30 1.13e- 3\n3 depress_base_c        0.575    0.0637      9.04 1.50e-16\n```\n\n\n:::\n:::\n\n\n\n\n<span style=\"color: red;\">Question: What would you do and why if you had also measured sex (unbalanced or balanced) across intervention groups? What would change?</span>   \n\n\n-----\n\n## Summary: New and Familiar Concepts\n\n- Interpretation of $b_0$, $b_1$, and $b_2$, in 2+ predictor model with continuous and dichotomous focal predictor.  \n\n- Impact of centering $X_1$ or $X_2$ on $b_0$, $b_1$, and $b_2$.  \n\n- What affects standard errors of $b_j$.  \n\n- What is Multicollinearity, how to detect, what are implications if high, and what are solutions.  \n\n- Model effect size ($R^2$).  \n\n- Effect sizes for $X$s ($b_j$s, $\\Delta R^2$, $\\eta_p^2$).  \n\n- Text, table, and figure descriptions of results.  \n\n\n\n\n\n\n",
    "supporting": [
      "06_two_predictors_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/grViz-binding-1.0.11/grViz.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}