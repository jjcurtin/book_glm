--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 9: Dealing with Messy Data III - Transformations

```{r}
#| echo: false

library(tidyverse) |> 
  suppressPackageStartupMessages()
library(patchwork)

theme_set(theme_classic()) 

options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```


## Transformations: The Family of Power and Roots

- The GLM makes **strong assumptions** about the structure of data, assumptions which often fail to hold in practice.

- One solution is to abandon the GLM for more complicated models (generalized linear models; weighted least squares; robust regression).

- Another solution is to transform the data (either the $X$s or $Y$) so that they conform more closely to the assumptions.  

- A particularly useful group of transformations is the *family* of powers and roots: 

    **$X$ → $X^p$**

    - If p is negative, then the transformation is an inverse power: $X^{-1} = \frac{1}{X}$, and $X^{−2} = \frac{1}{X^2}$
    - If p is a fraction, then the transformation represents a root: $X^{\frac{1}{2}} = \sqrt{X}$, and $X^{-\frac{1}{2}} = \frac{1}{\sqrt{X}}$
    
    
-----

- The Box-Cox family of transformations provides a comparable but more convenient form (in some cases*).

    **$X$ → $X^{(p)} \equiv \frac{X^p- 1}{p}$**  
    
- Since $X^{(p)}$ is a linear function of $X^p$, the 2 transformations have the same essential effect on the data.

    - Dividing by p preserves the direction of $X$, which otherwise would be reversed when p is negative.  
    - This function also matches level and slope of curve at $X=1$.
    
-----
    
```{r}
#| echo: false
#| fig-cap: The Box-Cox family of modified power transformations, $X^{(p)}= (X^p - 1)/p$, for values of $p = -1, 0, 1, 2, 3$. When $p = 0, X^{(p)} = log_e X$.

transform <- expand.grid(p = c(-1, 0, 1, 2, 3),
                    X = c(0, 1, 2, 3, 4)) |> 
  mutate(`X'` = if_else(p == 0, 
                        log(X),
                        (X^p)/p),
         p = factor(p))

transform |> 
  ggplot() +
  geom_line(aes(x = X, y = `X'`, color = p), linewidth = 1)
```

-----

- The power transformation $X^{(0)}$ is useless, but the very useful log transformation is a kind of *zeroth* power:
                   
    lim
    $p$ → $0$   $\frac{X^p - 1}{p} = \text{log}_eX$
                   
    where $e \approx 2.718$ is the base of the natural logarithms. Thus, we will take $X^{(0)} \equiv log(X)$.

- It is generally more convenient to use logs to the base 10 or base 2, which are more easily interpreted than logs to the base e.

- Changing bases is equivalent to multiplying your variable by a constant. No effect on significance tests.


-----

- Descending the *ladder* of powers and roots from $p = 1$ (i.e., no transformation) towards $p = -2$ **compresses the large values of X and spreads out the small ones**.

- Ascending the ladder of powers from $p = 1$ towards $p = 3$ has the **opposite effect**.

-----

- Power transformations are sensible only when all of the values of $X$ are positive:

    - Some of the transformations, such as log ($p= 0$) and square root ($p= .5$), are undefined for negative or zero values of $X$.

    - Power transformations are not monotone (i.e., they change to order of scores) when there are both positive and negative values among the data.

    - We can add a positive constant (called a *start*) to each data value to make all of the values positive: 

      $X$ → $(X + s)^{(p)}$
    
-----

- Power transformations are effective only when the ratio of the biggest data values to the smallest ones is sufficiently large; if this ratio is close to 1, then power transformations are nearly linear.  For example:  

    - Power transformations will work well if range of $X = 1 – 100$.
  
    - Power transformations will have little effect if range of $X = 1000 – 1100$
    
- Using a negative start can often increase the ratio of highest/lowest score.

- Using reasonable starts, if necessary, an adequate power transformation can usually be found in the range $−2 \le p \le 3$.

-----

- Power transformations of $Y$ (or sometimes $X$) can correct problems with normality of errors.

- Power transformations of $Y$ (or sometimes $X$) can stabilize the variance of the errors.

- Power transformations of $X$ (or sometimes $Y$) can make many nonlinear relationships more nearly linear.

- You can experiment with Box-Cox transformations of $X$ or $Y$ in R using `bcPower()` in the `car` package.

- In many fields, $X^p$ rather than $X^{(p)}$ may be preferred. Particularly, if $p \ge 0$. This is simply done algebraically.

-----

## Transformations: Dealing with Skew

- Transforming $Y$ *down the ladder* can correct positive skew in the errors (most common problem).  

- Transforming $Y$ *up the ladder* corrects negative skew in the errors.

-----

```{r}
#| code-fold: true

set.seed(102030)
y <- tibble(y_raw = rchisq(n=500, df=1),
            y_.5 = car::bcPower(y_raw, .5),
            y_.25 = car::bcPower(y_raw, .25),
            y_0 = car::bcPower(y_raw, 0))    

plot_raw <- y |> 
  ggplot(aes(x = y_raw)) +
  geom_density() + 
  scale_x_continuous(limits = c(-.5, 12), breaks = c(0, 2, 4, 6, 8, 10)) +
  labs(title = "Raw Y",
       x = NULL,
       caption = str_c("N = 500; bandwidth = ", round(density(y$y_raw)$bw, 4)))

plot_.5 <- y |> 
  ggplot(aes(x = y_.5)) +
  geom_density() + 
  scale_x_continuous(limits = c(-3, 5), breaks = c(-2, 0, 2, 4)) +
  labs(title = "p = .5; sqrt(Y)",
       x = NULL,
       caption = str_c("N = 500; bandwidth = ", round(density(y$y_.5)$bw, 4)))

plot_.25 <- y |> 
  ggplot(aes(x = y_.25)) +
  geom_density() + 
  scale_x_continuous(limits = c(-4.5, 4), breaks = c(-4, -2, 0, 2, 4)) +
  labs(title = "p = .25",
       x = NULL,
       caption = str_c("N = 500; bandwidth = ", round(density(y$y_.25)$bw, 4)))

plot_0 <- y |> 
  ggplot(aes(x = y_0)) +
  geom_density() + 
  scale_x_continuous(limits = c(-15, 4), breaks = c(-15, -10. -5, 0)) +
  labs(title = "p = 0; log(Y)",
       x = NULL,
       caption = str_c("N = 500; bandwidth = ", round(density(y$y_0)$bw, 4)))


(plot_raw + plot_.5) / (plot_.25 + plot_0)
```

-----

## Transformations: Dealing with Heteroscedasticity

- Transforming $Y$ *down the ladder* can correct problems with increasing spread of errors as $Y$ increases (most common problem).  

- Transforming $Y$ *up the ladder* corrects decreasing spread. 

- The problems of unequal spread and skewness commonly occur together and can be corrected together. Therefore, transforming $Y$ down the ladder can correct both issues simultaneously.

-----

```{r}
#| code-fold: true
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(skimr)


path_data <- "data_lecture"

data <- read_csv(here::here(path_data, "09_transformations_fps.csv"), 
                 show_col_types = FALSE) |> 
   mutate(sex_c = if_else(sex == "female", -.5, .5)) |> 
   # remove outliers to fit same model as last two units 
   filter(!subid %in% c("0125", "2112"))
```

```{r}
m <- lm(fps ~ bac + ta + sex_c, data = data)
```

```{r}
#| code-fold: true

car::qqPlot(m, id = FALSE, simulate = TRUE,
            main = "Quantile-Comparison Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```

```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),
                                      y = dnorm(seq(-4, 4, length.out = 100), 
                                      mean=0, sd=sd(rstudent(m)))), 
            linetype = "dashed", color = "blue")
```

----

```{r}
#| code-fold: true

tibble(x = fitted.values(m),
       y = rstudent(m)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Studentized Residuals")
```

-----

```{r}
#| echo: false
#| warning: false
#| code-fold: true

car::spreadLevelPlot(m)
```

```{r}
car::ncvTest(m)
```

-----


```{r}
gvlma::gvlma(m)
```

-----


## Box Cox Transformation Function

```{r}
data |> 
  select(bac, ta, sex_c, fps) |> 
  skim() |> 
  focus(n_missing, numeric.mean, numeric.sd, min = numeric.p0, max = numeric.p100) |> 
  yank("numeric")
```

We are going to first add a constant so all response values are > 0
```{r}
data <- data |> 
  mutate(fps_27 = fps + 27)

m_2 <- lm(fps_27 ~ bac + ta + sex_c, data = data)
```

-----

Next we will pull the best lambda value from a plot of log-likelihood values by lambda power transformations of response variable.
```{r}
car::boxCox(m_2)

round(car::boxCox(m_2)$x[which.max(car::boxCox(m_2)$y)],2)
```


-----

Lastly, we will use the best lambda value to conduct our power transformation and re-evaluate our model assumptions. 

```{r}
data <- data |> 
  mutate(fps_bc = car::bcPower(fps_27, .55))

m_3 <- lm(fps_bc ~ bac + ta + sex_c, data = data)
```

-----

```{r}
#| code-fold: true

car::qqPlot(m_3, id = FALSE, simulate = TRUE, 
            main = "Quantile-Comparison Plot to Assess Normality",  
            ylab = "Studentized Residuals")
```


```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_3), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),
                                      y = dnorm(seq(-4, 4, length.out = 100), 
                                      mean=0, sd=sd(rstudent(m_3)))), 
            linetype = "dashed", color = "blue")
```

-----

```{r}
#| code-fold: true

tibble(x = fitted.values(m_3),
       y = rstudent(m_3)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Studentized Residuals")
```


```{r}
#| warning: false

car::spreadLevelPlot(m_3)
```

```{r}
#| code-fold: true

car::ncvTest(m_3)
```

-----


```{r}
#| code-fold: true

gvlma::gvlma(m_3)
```

**Transformations often don't help!**

-----

```{r}
m_3 |> 
  broom::tidy()
```

-----

White (1980) Heteroscedasticity-corrected SEs and Tests

```{r}
corrected_ses <- sqrt(diag(car::hccm(m)))

broom::tidy(m) |> 
  select(term, estimate) |> 
  add_column(std.error = corrected_ses) |> 
  mutate(statistic = estimate/std.error,
         p.value = 2*(pt(abs(statistic), df=m_2$df.residual, lower.tail=FALSE)))
```

*Maybe* better choicer was untransformed Y (original model) with corrected SEs?    
 
*Maybe* the problem wasn’t bad enough to do anything?

-----

## Transformations: Dealing with Non-linearity

Power transformations can make simple monotone relationships more linear (Fig A).  Polynomial regression (or other transformations, e.g. logit) is often needed for more complex relationships (Figs, B & C).


```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/monotone_relationship.png"))
```


## Transformations: Mosteller and Tukey’s bulging rule

Simple monotone relationships can be corrected by transforming $X$ or $Y$.     

- $X$ is typical if only one $XY$ relationship is problematic.    

- If all $X$s have similar non-linear relationship, transform $Y$.    

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/mosteller_tukey.png"))
```

-----

::: {.callout-important}
# Question

What 2 transformations would make this relationship linear?
:::


```{r}
#| echo: false

data_nonlinear <- tibble(X=1:100, Y = sqrt(1:100))

data_nonlinear  |> 
  ggplot(aes(x = X, y = Y)) +
  geom_point(alpha = .4)
```

:::{.fragment}
[Move $X$ down the ladder (e.g., $X^.5$).]{style="color:blue;"}    

```{r}
data_nonlinear  |> 
  ggplot(aes(x = sqrt(X), y = Y)) +
  geom_point(alpha = .4)
```

:::


:::{.fragment}
[Move $Y$ up the ladder (e.g., $Y^2$).]{style="color:blue;"}  

```{r}
data_nonlinear  |> 
  ggplot(aes(x = X, y = Y^2)) +
  geom_point(alpha = .4)
```

:::

-----

## General Transformations

- In some fields, certain power transformations (sqrt, log, inverse) are common.

- If we have a choice between transformations that perform roughly equally well, we may prefer one transformation to another because of interpretability:

    - The log transformation has a convenient multiplicative interpretation (e.g., increasing  log2 (X) by 1 doubles X; increasing log10 (X) by 1 multiples X by 10).

- Transformations are a big source of research dfs. Should be pre-registered.  You may know what your DV typically *needs* from prior data. Worst case, report with and without transformations and justify.

-----

## 5K Race Times


```{r}
data_5k <- read_csv(here::here(path_data, "09_transformations_5k.csv"),
                    show_col_types = FALSE) |> 
  glimpse()
```

```{r}
m_5k <- lm(time ~ age + miles, data = data_5k)

broom::tidy(m_5k)
```

-----

```{r}
#| code-fold: true

car::qqPlot(m_5k, id = FALSE, simulate = TRUE,
            main = "Quantile-Comparison Plot to Assess Normality", 
            ylab = "Studentized Residuals")
```


```{r}
#| code-fold: true

ggplot() +
  geom_density(aes(x = value), data = enframe(rstudent(m_5k), name = NULL)) +
  labs(title = "Density Plot to Assess Normality of Residuals",
       x = "Studentized Residual") +
  geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 100),
                                      y = dnorm(seq(-4, 4, length.out = 100), 
                                      mean=0, sd=sd(rstudent(m_5k)))), 
            linetype = "dashed", color = "blue")
```

-----

```{r}
#| code-fold: true

tibble(x = fitted.values(m_5k),
       y = rstudent(m_5k)) |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue", linewidth = 1) +
  labs(title = "Studentized Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Studentized Residuals")
```

-----

```{r}
car::spreadLevelPlot(m_5k)
```

```{r}
car::ncvTest(m_5k)
```

-----

```{r}
car::crPlots(m_5k, ask = FALSE)
```


-----


```{r}
gvlma::gvlma(m_5k)
```

-----


Let's try transforming miles using `log2()`, a binary logarithm (base 2).
```{r}
data_5k <- data_5k |> 
  mutate(log_miles = log2(miles))

m_5k_tran <- lm(time ~ age + log_miles, data = data_5k)

broom::tidy(m_5k_tran)
```

-----


```{r}
car::crPlots(m_5k_tran, ask = FALSE)
```

-----

```{r}
gvlma::gvlma(m_5k_tran)
```

-----

## Displaying Transformed Results With Fake Data

```{r}
data_fake <- tibble(x = 3 * rchisq(200, df=3),
                    x_sr = sqrt(x),
                    y = 3 * x_sr + rnorm(200,mean=0,sd = 1))

m_raw = lm(y ~ x, data = data_fake)
broom::tidy(m_raw)
```

```{r}
data_fake |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .4) +
  geom_abline(intercept = coef(m_raw)[1], slope = coef(m_raw)[2], color = "red")
```

-----


```{r}
m_sr = lm(y ~ x_sr, data = data_fake)
broom::tidy(m_sr)
```

```{r}
data_fake |> 
  ggplot(aes(x = x_sr, y = y)) +
  geom_point(alpha = .4) +
  geom_abline(intercept = coef(m_sr)[1], slope = coef(m_sr)[2], color = "red")
```

-----

You can display results from linear model but provide scale for raw $X$ instead of Sqrt($X$) or in addition to Sqrt($X$) on another axis.


```{r}
preds <- tibble(x_sr = seq(min(data_fake$x_sr),max(data_fake$x_sr), by=.01))
y_pred = predict(m_sr, newdata = preds)

ggplot() +
  geom_point(aes(x = data_fake$x_sr, y = data_fake$y), alpha = .4) +
  geom_line(aes(x = preds$x_sr, y = y_pred)) +
  ylab("Y") +
  scale_x_continuous(name = "SQRT(X)", sec.axis = sec_axis(~.^2, name = "Raw X"))
```



-----

You can display results from linear model in Raw $X$ Units and use `predict()` to get the $Y$s based on transformed $X$.

```{r}
preds <- tibble(x_sr = sqrt(seq(min(data_fake$x),max(data_fake$x), by=.1)))
y_pred = predict(m_sr, newdata = preds)

ggplot() +
  geom_point(aes(x = data_fake$x, y = data_fake$y), alpha = .4) +
  geom_line(aes(x = seq(min(data_fake$x),max(data_fake$x), by=.1), y = y_pred)) +
  ylab("Y") +
  xlab("X")

```

