--- 
output: html_document 
editor_options:  
  chunk_output_type: console
--- 
 

# Unit 14: Power Analysis and Statistical Validity

```{r}
#| echo: false
options(scipen = 999) # turns off scientfic notation
options(knitr.kable.NA = '')
```


```{r}
#| echo: false
#| message: false

library(tidyverse)

tibble("Research Concludes" = c("FAIL TO REJECT NULL; NO EFFECT", 
                                "REJECT NULL; EFFECT EXISTS"),
       "NO EFFECT" = c("CORRECT FTR; True Negative (TN)", 
                                 "TYPE I ERROR; False Positive (FP)"),
       "EFFECT EXISTS" = c("TYPE II ERROR; False Negative (FN)", 
                                     "CORRECT R; True Positive (TP)")) |> 
  kableExtra::kbl() |> 
  kableExtra::column_spec(1, bold = TRUE) |>
  kableExtra::add_header_above(c(" ", "Reality" = 2)) 
```


----

## Power

Power: The probability of rejecting the null hypothesis when it is false.    

Symbol is 1 - $\beta$, where $\beta$ is the probability of making a Type II error - failing to reject the null hypothesis when you should.    

In everyday language, power is the probability of concluding that variables are related (or groups differ) based on inferential statistics calculated in your sample, when the variables are related (or groups differ) in the population.

----

## What Determines Power?

Some statistical tests are more powerful (i.e., better at detecting real/non-zero population effects) than others.     

Parametric tests often are more powerful than non-parametric, because they work with more information from the data.  

GLM is MVUE when assumptions are met.   


----

## What Factors Affect Power in GLM?

1.  Alpha ($\alpha$; significance level): As alpha increases (e.g., goes from .01 to .05, power increases - with less stringent alpha, easier to reject the null hypothesis of no relation/difference.   

2. Sample size: As $N$ increases, power increases - with larger $N$, standard error of parameter estimates are reduced, so small, non-zero effects are less likely to be due to sampling error.  

3. Magnitude of effect in the population: As magnitude of the effect in the population goes up,  power increases – easier to determine that population effect $\ne$ 0 if it is bigger because sample $b$s will be bigger on average.

4. Number of parameters in augmented Model (and effect): As either the model gets more complex with more parameters (> $P_a$) or the effect gets more complex (> $P_a - P_c$), power goes down because SE gets larger. Critical value also gets larger.   

5. Error (unexplained  variance in $Y$): As $SSE_a$ goes up (and model $R_2$ goes down) power is lower – larger error means larger standard error of parameter estimates. More difficult to tell if effects $\ne$ 0 because $b$s vary more greatly from sample to sample (and from zero) by chance.

----

## Power Conventions

Desired level of power: depends on purpose, but usually the more the better. The value of .80 has become a minimum threshold standard (much like alpha = .05 for significance testing.   

Higher power also means more precision in estimating the magnitude of the effect (tighter CI).   

Two strategies:

1. Determine number of subjects needed ($N$) for given level of power (e.g. .80).   

2. Determine power for a given design (e.g., completed experiment with fixed $N$).   


----

**Power can be calculated for tests of:**    

- Effect of single regressor.
- Effect of subset of regressors controlling for other regressors in the model.  
- Effect of all regressors in the model.

**A priori power analysis (for sample size planning):**   

1. Must set alpha level, $P_a$, $P_c$, and desired power.   
2. Specify (calculate) effect size (e.g., $\eta_p^2$ or $\Delta R^2$).
3. $N$ will be a function of the above factors.   


**Post hoc power analysis ([When?]{style="color: red;"}):**   

1. Must set alpha level, $P_a$, $P_c$, and $N$.
2. Specify minimum effect size of interest. Be very conservative.
3. Power will be a function of above factors.

----

## Effect Size: $\eta_p^2$

You can calculate $\eta_p^2$ for any reported effect.   

$\eta_p^2 = \frac{F*\text{df}_n}{F*\text{df}_n + \text{df}_d}$    

Where $\text{df}_n$ and $\text{df}_d$ are numerator and denominator degrees of freedom from the F statistic.
<!--John: the spacing for df looked weird without specifying it as text so I am using \text here-->

----


## Cohen (1992) Rules of Thumb for $\eta_p^2$

The values of $\eta_p^2$ that Cohen implicitly defines as small, medium, and large vary across research context/statistical test.  

```{r}
#| echo: false

tibble("Effect" = c("Small", "Medium", "Large"),
       "MR" = c(".02 (R<sup>2</sup>  = .02)", ".13 (R<sup>2</sup>  = .13)", ".26 (R<sup>2</sup>  = .26)"),
       "ANOVA" = c(".01 (F = .10)", ".06 (F = .25", ".14 (F = .40"),
       "t-test" = c(".01 (d = .20)",".06 (d = .50)", ".14 (d = .80)"),
       "r" = c(".01 (r = .10)", ".09 (r = .30)", ".25 (r = .50)")) |> 
  kableExtra::kbl(escape = FALSE) 
```

- Use MR effect sizes if you are testing model $R^2$ or a set (>1) of quantitative regressors.   
- Use ANOVA/t effect sizes if you are testing one categorical variable.
- Use r effect sizes if you are testing one quantitative variable.

----

## Power Analysis in Quarto

You can use the `pwr.f2.test()` function in the `pwr` package to calculate power.   

It takes the following parameters:   

- `u` = Numerator degrees of freedom for F statistic ($P_a - P_c$).   
- `v` = Denominator degrees of freedom for F statistic ($N - P_a$). 
- `f2` = Effect size ($\frac{\eta_p^2}{1-\eta_p^2}$ or $\frac{\Delta R^2}{(1 - R^2)}$).	
- `sig.level` = Significance level (Type I error probability; $\alpha$).
- `power`	= Power of test (1 minus Type II error probability)

----

## A Priori Power Analysis for One Parameter in MR

::: {.callout-important}
# Question

How many subjects are needed for 80% power to detect a $\eta_p^2$ of .09 for one predictor in a model with 5 predictors at an alpha of 0.05?
:::


:::{.fragment}
```{r}
pwr::pwr.f2.test(u = 1, 
                 v = NULL, #<1>
                 f2 = .09/(1-.09), 
                 sig.level = .05, 
                 power = .80)
```

1. Since we do not yet have a sample size (this is what we are trying to determine!), we set `v` to `NULL`.   

[$N = v + P_a = 79.327 + 6 = 85.327$]{style="color: blue;"}
:::


----

::: {.callout-important}
# Question

How many subjects are needed for 85% power to detect a $\Delta R^2$ of .05 for one predictor in a model with 3 predictors and an $R^2$ of .30 at an alpha of 0.05?
:::


:::{.fragment}
```{r}
pwr::pwr.f2.test(u = 1, 
                 v = NULL,
                 f2 = .05/(1-.30), 
                 sig.level = .05, 
                 power = .85)
```

  

[$N = v + P_a = 125.648 + 4 = 129.648$]{style="color: blue;"}
:::

----

## Hefner et al. (2013): Duration Task

**Certain Versus Uncertain Duration Cues**    

"We analyzed startle potentiation at 4.5 s postcue onset in a General Linear Model (GLM) with a between-subjects regressor for beverage group (alcohol vs. control) and repeated measures on cue type (certain vs. uncertain). We included two covariates, baseline startle response and sex, as additive regressors to increase power. [**As predicted, we observed a significant interaction between beverage group and cue type, $b = 22.2, \eta_p^2 = 0.16, t(64) = 3.45, p = .001$.**]{style="color: green;"}"

----

::: {.callout-important}
# Question

How many subjects are needed to detect the beverage group x cue type interaction based on the effect size observed in Hefner et al. with 90% power and an alpha of .05?
:::


:::{.fragment}

$\eta_p^2 = \frac{F * \text{df}_n}{F*\text{df}_n + \text{df}_d} = \frac{3.45^2*1}{3.45^2*1+64} = .157$

```{r}
pwr::pwr.f2.test(u = 1, 
                 v = NULL, 
                 f2 = .157/(1-.157), 
                 sig.level = .05, 
                 power = .90)
```

[$N = v + P_a = 56.408 + 4 = 60.408$]{style="color: blue;"}

:::

----

## Post Hoc Power Analysis for One Parameter in MR

::: {.callout-important}
# Question

How much power did Hefner et al. have to detect a moderate effect size ($\eta_p^2 = .06$), with alpha of .05?
:::

:::{.fragment}

```{r}
pwr::pwr.f2.test(u = 1, 
                 v = 64, 
                 f2 = .06/(1-.06), 
                 sig.level = .05, 
                 power = NULL)
```

[Power = 0.525]{style="color: blue;"}

:::

----

## Problems from Low Power

1. Low probability of finding true effects.   
2. Low positive predictive value (PPV). 
3. An exaggerated estimate of the magnitude of the effect when a true effect is discovered. 

----

## Missed True Effects

Low power, by definition, means that the chance of discovering effects that are genuinely true is low.   

If a study has 20% power and there really is an effect, you only have a 20% chance of finding it.  

Low-powered studies prodice more false negatives (misses) than high-powered studies.   

We tend to focus on false alarms, but misses can be equally costly (e.g., new treatments).   

You waste resources (time, money) with low-powered studies. One high-powered study (N) > Two low-powered studies (N/2).

----

## Low Positive Predictive Value (PPV)

The lower the power of a study, the lower the probability that an observed *significant* effect (among the set of all significant effects) reflects a true non-zero effect in the population (vs. a false alarm).     

Called the Positive Predictive Value (PPV) of a claimed discovery.    

::: {.callout-important}
# Question

If alpha = .05, what is PPV?
:::

:::{.fragment}
[You need more information. Probably not 95%!]{style="color: blue;"}
:::

----

$\text{PPV} = \frac{(1 - \beta) * \text{OR}}{(1 - \beta) * \text{OR} + \alpha}$   

Where:    

- $(1 - \beta)$ is the power.   
- $\beta$ is the type II error rate.   
- $\alpha$ is the type I error rate.
- $\text{OR}$ is the pre-study odds ratio (that is, the odds that an effect is indeed non-null among the effects being tested in a field or other set).

Formula can be rewritten as:   
$\text{PPV} = \frac{\text{Power * OR}}{\text{Power * OR} + \alpha}$

----

A priori odds ratio that effect exists: 1 (1:1 or 1/2)  
Power: 80%   
Simulate: 200 studies   

$\text{PPV} = \frac{(1 - \beta) * \text{OR}}{(1 - \beta) * \text{OR} + \alpha}$    
$\text{PPV} = \frac{(1 - .20) *1}{(1 - .20) * 1 + .05} = \frac{.80}{.85} = .94$   

```{r}
#| echo: false

tibble("Research Concludes" = c("FAIL TO REJECT NULL; NO EFFECT", 
                                "REJECT NULL; EFFECT EXISTS", " "),
       "NO EFFECT" = c("CORRECT FTR; <b>95</b>", 
                       "TYPE I ERROR; <b>5</b>",
                       "<b>100</b>"),
       "EFFECT EXISTS" = c("TYPE II ERROR; <b>20</b>",
                           "CORRECT R; <b>80</b>",
                           "<b>100</b>")) |> 
  kableExtra::kbl(escape = FALSE) |> 
  kableExtra::column_spec(1, bold = TRUE) |>
  kableExtra::add_header_above(c(" ", "Reality" = 2)) |> 
  kableExtra::row_spec(2, background = "yellow")
```

----

A priori odds ratio that effect exists: 1 (1:1 or 1/2)  
Power: 20%   
Simulate: 200 studies   

$\text{PPV} = \frac{(1 - \beta) * \text{OR}}{(1 - \beta) * \text{OR} + \alpha}$    
$\text{PPV} = \frac{(1 - .80) *1}{(1 - .80) * 1 + .05} = \frac{.20}{.25} = .80$   

```{r}
#| echo: false

tibble("Research Concludes" = c("FAIL TO REJECT NULL; NO EFFECT", 
                                "REJECT NULL; EFFECT EXISTS", " "),
       "NO EFFECT" = c("CORRECT FTR; <b>95</b>", 
                       "TYPE I ERROR; <b>5</b>",
                       "<b>100</b>"),
       "EFFECT EXISTS" = c("TYPE II ERROR; <b>80</b>",
                           "CORRECT R; <b>20</b>",
                           "<b>100</b>")) |> 
  kableExtra::kbl(escape = FALSE) |> 
  kableExtra::column_spec(1, bold = TRUE) |>
  kableExtra::add_header_above(c(" ", "Reality" = 2)) |> 
  kableExtra::row_spec(2, background = "yellow")
```

----

A priori odds ratio that effect exists: .25 (1:4 or 1/5)  
Power: 20%   
Simulate: 200 studies   

$\text{PPV} = \frac{(1 - \beta) * \text{OR}}{(1 - \beta) * \text{OR} + \alpha}$    
$\text{PPV} = \frac{(1 - .80) *.25}{(1 - .80) * .25 + .05} = \frac{.05}{.10} = .50$   

```{r}
#| echo: false

tibble("Research Concludes" = c("FAIL TO REJECT NULL; NO EFFECT", 
                                "REJECT NULL; EFFECT EXISTS", " "),
       "NO EFFECT" = c("CORRECT FTR; <b>152</b>", 
                       "TYPE I ERROR; <b>8</b>",
                       "<b>160</b>"),
       "EFFECT EXISTS" = c("TYPE II ERROR; <b>32</b>",
                           "CORRECT R; <b>8</b>",
                           "<b>40</b>")) |> 
  kableExtra::kbl(escape = FALSE) |> 
  kableExtra::column_spec(1, bold = TRUE) |>
  kableExtra::add_header_above(c(" ", "Reality" = 2)) |> 
  kableExtra::row_spec(2, background = "yellow")
```

----

## What does this imply about papers in Science?

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/power_science.png"))
```


----
  
## Sample Estimate of Effect is Too Large

When an under-powered study discovers a true effect, it is likely that the estimate of the magnitude of that effect provided by that study will be exaggerated.    

Effect inflation is worst for small, low-powered studies, which can only detect sample parameter estimates effects that happen to be large.    

::: {.callout-important}
# Question

Why does this make sense?
:::


::: {.fragment}
Consider an example where $\beta = 5, SE = 4$. Draw sampling distribution. Indicate region where sample $b$s would be significant. What can we conclude about estimates ($b$) for $\beta$ in any one study or across a bunch of studies?
:::

----

## Other Biases That Often Come With Low Power

1. Vibration of effects.
2. Publication bias, selective data analysis, and selective reporting of outcomes.
3. May be of lower quality in other aspects of their design.

----

## Vibration of Effects

**Vibration of effects** refers to the situation in which a study obtains different estimates of the magnitude of
the effect depending on the analytical options it implements.   

These options could include the statistical model, the definition of the variables of interest, the use (or not) of adjustments for certain potential confounders but not others, the use of filters to include or exclude specific observations and so on.    

This is more often the case for small studies.    

Results can vary markedly depending on the analysis strategy when power is low because of small N.

----

## Publication Bias and Selective Reporting

Publication bias and selective reporting of outcomes and analyses are more likely to affect smaller, under-powered studies.   

Smaller studies “disappear” into a file drawer (larger studies are known and anticipated).     

Larger null results may be published because low power isn't a viable explanation for null effect.     

The protocols of large (clinical) studies are more likely to have been registered or otherwise made publicly available. Deviations in the analysis plan and choice of outcomes may be more obvious.   

Smaller studies may have a worse design quality than larger studies.    

Small studies may be opportunistic, quick and dirty experiments. Data collection and analysis may have been conducted with little planning.      

Large studies often require more funding and personnel resources. Designs are examined more carefully before data collection, and analysis and reporting may be more structured.    

----

##### Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366.


```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/Simmons_fig_1.png"), dpi = 100)
```


----

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/Simmons_fig_2.png"))
```


----

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/Simmons_fig_3.png"), dpi = 125)
```


----

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/Simmons_fig_4.png"), dpi = 100)
```

----

## Commitment to Research Transparency and Open Science

[http://www.researchtransparency.org/](http://www.researchtransparency.org/)

```{r}
#| echo: false

knitr::include_graphics(path = here::here("figures/research_transparency_logo.png"))
```


----

## Method Transparency and Sharing

**The 21 Word Solution**   

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2012). A 21 word solution. Retrieved from: [http://dx.doi.org/10.2139/ssrn.2160588](http://dx.doi.org/10.2139/ssrn.2160588)   

We have adopted the recommendation to report 1) how we determined our sample size, 2) all data exclusions, 3) all manipulations, and 4) all study measures. We will include the following brief (21 word) statement in all papers we submit for publication:      

"We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.”

----

<!--John, I am finding this section a little confusing. Are you quoting the paper or are these requests you are making for the students to follow? Also, request one is essentially the 21 word statement from the previous slide so it feels weird to say this is an additional request.-->

I have three additional requests that follow from the increased efforts directed at transparency and open science more generally in our field.    


1. **Add a statement to the paper confirming that you have reported all measures, conditions, data exclusions, and how you determined the sample size.** You should, of course, add any additional text to ensure this statement is accurate. This is the standard disclosure request endorsed by the Center for Open Science (see [http://osf.io/hadz3](http://osf.io/hadz3)).

2. **Add a statement to the manuscript to confirm that the study was or was not pre-registered. If it was not, provide the rationale for why.** 

<!--John I just extracted the request from this paragraph, this was original text:

Second, as you likely know, substantial concerns have been raised in recent years about the impact of researcher degrees of freedom in data processing, analysis and selective reporting of outcomes on the validity of our statistic inference based on p-values.  I saw nothing to indicate that study hypotheses, data processing decisions, and proposed analysis were pre-registered to increase confidence that researcher degrees of freedom were reduced or removed.  I would ask that you add a statement to the manuscript to confirm that the study was not pre-registered and provide the rationale for why it was not.  
-->

3. **Make data, measures, and analysis code publicly available, online, hosted by a reliable third party (e.g., [https://osf.io/](https://osf.io/)), and provide a persistent link to these materials in the paper.**

<!--Original text:
Third, the ability to evaluate (and associated confidence in) the integrity of the study design and analyses for any manuscript is substantially enhanced when authors make data, measures, and analysis code publicly available, online, hosted by a reliable third party (e.g., https://osf.io/), and provide a persistent link to these materials in the paper. Data and materials sharing also increases the impact of any study beyond its current findings because such sharing can stimulate and assist further research on the topic. The data and study materials from this study should be shared unless clear reasons (e.g., legal, ethical constraints, or severe impracticality) that prevent sharing are provided.
-->

----

## Important Research Degrees of Freedom

1. IVs and how they are “scored” (levels, processing, etc)   
2. DVs and how they are scored  
3. Analytic model (additive, interactive, other model features)  
4. Covariates and how they are selected   
5. Transformations  
6. Outliers and influence  
7. N

----

## Resources

**Pre-registration FAQ:**     
[https://www.psychologicalscience.org/observer/research-preregistration-101](https://www.psychologicalscience.org/observer/research-preregistration-101)   


**Pre-registration examples:**    
[https://osf.io/ukhcf/register/564d31db8c5e4a7c9694b2be](https://osf.io/ukhcf/register/564d31db8c5e4a7c9694b2be)    

[https://osf.io/m8jmp/register/564d31db8c5e4a7c9694b2c0](https://osf.io/m8jmp/register/564d31db8c5e4a7c9694b2c0)