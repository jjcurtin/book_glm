--- 
editor_options:  
  chunk_output_type: console
--- 

# Unit 5: Inferences about a Single Dichotomous Predictor

```{r}
#| echo: false

options(scipen = 999) # turns off scientfic notation

```

## Announcements

- Application Exam assigned this Friday (10/04).  Due at 8:30 am on Friday, 10/11
- Concepts exam on Wednesday, 10/23 in class
- Progress on video recordings
  - Lab recorded this week
  - Captioning in progress
  - Trying to record lecture today!
- Class notetaker needed




## Categorical Variables in GLM

1. One dichotomous predictor coded with one regressor (*this unit*).   

2. Learn models with multiple regressors in later units:

  - One-way between subject ANOVA (One predictor, multiple regressors) 
  - Multiple predictors and regressors
	  - *Additive:*  ANCOVA, General Additive Models
	  - *Interactive:* Factorial ANOVA, Attitude Treatment Interactions, General Interactive Models    

3. Repeated measures & mixed model designs
  
-----

## Dichotomous Predictor
  
::: {.callout-important}
# Question

Consider a regression model with one dichotomous predictor and the typical continuous DV. What other statistical test is this regression equivalent to?
:::

:::{.fragment}
[An independent samples (between subjects) t-test comparing the two groups that are represented by the two levels of the predictor on the mean of the DV.]{style="color:blue;"}
:::


::::{.fragment}
::: {.callout-important}
# Question

Question: How do we handle analysis of dichotomous predictors in regression?
:::
::::

:::{.fragment}
[Very simple! We will assign two “arbitrary” values to a regressor $X$ to represent the two levels (groups) in the dichotomous predictor variable.]{style="color:blue;"}
:::

## Coding Dichotomous Predictors

There are 2 coding schemes that we will need to learn:    

1. Dummy coding (reference/control group)
2. Contrast coding (for now, unit weighted, centered coefficients).   

\

We also need to learn distinction between weighted vs. unweighted means (advanced topic).   

-----

## Dummy Coding

Dummy coding involves assigning 1’s and 0’s to the regressor in a specific pattern.  

- For the two level simple case, assign a 0 to the control/reference group and a 1 to the target or experimental group.  
- The signficance of the statistical test isn't affected by which group you assign to be the reference.

::: {.callout-important}
# Question

What would change if you reverse the 0/1 coding such that you assign 1 to the reference group?
:::

:::{.fragment}
[The sign of the parameter estimate for $b_1$ will reverse!  But its t statistic and p-value will NOT change.  What if we used -1 vs. 1?]{style="color:blue;"}
:::

-----

## Our Running Example

```{r}
#| code-fold: true
#| message: false

options(conflicts.policy = "depends.ok") 
library(tidyverse)
library(broom)  # for tidy() and glance()
library(patchwork)

theme_set(theme_classic()) 

path_data <- "data_lecture" 
```

```{r}
data <- read_csv(here::here(path_data, "05_single_dichotomous_bg_fps.csv"),
                 show_col_types = FALSE)
data |> head()
```

-----

:::{.callout-tip}
# Coding Tip
We can customize skimr output to exclude certain statistics. 

- For example, we can exclude the histogram, 25th percentile, and 100th percentile for numeric variables. 
- Below is an example of how to customize skimr output.
- Save this is a script that you source to have access to it without having to rewrite it each time (DRY!!)

```{r}
library(skimr)
my_skim <- skim_with(base = sfl(n_complete = ~ sum(!is.na(.), na.rm = TRUE),
                                n_missing = ~sum(is.na(.), na.rm = TRUE)),
                     numeric = sfl(p25 = NULL,
                                   p75 = NULL,
                                   hist = NULL),
                     character = sfl(min = NULL, max = NULL),
                     factor = sfl(ordered = NULL))
```
:::

-----

Skim the data

\

```{r}
data |> my_skim()
```

-----

Get counts for `bg`

\

```{r}
data |> 
  group_by(bg) |> 
  count()
```

-----

## Dummy Codes for Beverage Group

::: {.callout-important}
# Question

Why can't we use `bg` directly in our glm in `lm()`
:::


:::{.fragment}
[It is not numeric.  What is a 1 unit change on `bg` when it is coded as "placebo" and "alcohol?"]{style="color:blue;"}

[We need to make a regressor (an `X` in our model) to represent `bg`]{style="color:blue;"}
:::
-----

```{r}
data <- data |> 
  mutate(bg_d = if_else(bg == "alcohol", 1, 0)) 

data  |> slice_sample(n = 20)
```

-----

Now we can estimate $b_0$ and $b_1$

\

```{r}
m_d <- lm(fps ~ 1 + bg_d, data = data)

m_d |> 
  tidy()
```

-----

And lets compare this to our previous analysis using `bac` coded quantitatively

... But first lets write a quick function to calculate SSE for any model since we will do this frequently.

```{r}
sse <- function(model) {
  sum(residuals(model)^2)
}
```

-----

And now lets look at SSE for these models

```{r}
m_d |> tidy()
sse(m_d)
```

\

```{r}
#| code-fold: true

data_q <- read_csv(here::here(path_data, "04_single_quantitative_bac_fps.csv"),
                 show_col_types = FALSE) 

m_q <- lm(fps ~ 1 + bac, data = data_q)
```

```{r}
m_q |> tidy()
sse(m_q)
```

-----

::: {.callout-important}
# Question

What is different about the NHST of the **alcohol** effect with `bg_d` vs `bac` and why?   
:::

:::{.fragment}
[p-value is worse (bigger) for bg_d than BAC. Taking a quantitative variable and dichotomizing (or trichotomizing, etc.) throws away potentially valuable information in  your predictor variable. The dichotomous variable will generally be a worse predictor. This will produce a model with more error (> SSE) and less power (> p-values) to test hypotheses.]{style="color:blue;"}

\

[Don’t dichotomize unless there is a **REALLY** good reason. GLM can accommodate quantitative variables anywhere you would have previously used a categorical variable.]{style="color:blue;"}

\

$SE_{b_0}=\sqrt{\frac{SSE}{N-P}}*\sqrt{\frac{1}{N}+\frac{(\overline{X})^2}{(N-1)*S_x^2}}$  

$SE_{b_1}=\frac{s_y}{s_x}*\sqrt{\frac{1-R^2}{N-P}}$
:::

-----

## Dummy Codes for Beverage Group (cont.)

```{r}
#| code-fold: true

m_d |> tidy()
```

\

::: {.callout-important}
# Question

What is the interpretation of $b_1$ (parameter estimate for `bg_d`) in this model?
:::

:::{.fragment}
[Still change in $Y$ for one unit change in $X$.]{style="color:blue;"}

[However, a one unit change in $X$ moves from the placebo group (coded 0) to alcohol group (coded 1).  Therefore $b_1$ is now the predicted change in $Y$ between placebo and alcohol groups. This is simply the difference in the mean of the two groups!]{style="color:blue;"}
:::

-----

::: {.callout-important}
# Question

Where is $b_1$ ($bg_d$ effect) on this graph?

\

```{r}
#| code-fold: true
#| fig-height: 4

plot_d <- data |> 
  ggplot(aes(x = bg_d, y = fps)) +
  geom_point(alpha = .6, size = 2) +
  geom_abline(aes(intercept = coef(m_d)[1],
                  slope = coef(m_d)[2]),
                  linewidth = 1) 

plot_d
```
:::

-----

[It is the slope of the line (as before). The line goes through the mean of FPS scores for each of the two groups.]{style="color:blue;"}

\

```{r}
#| code-fold: true
labels <- tibble(means = c(round(mean(subset(data, bg_d == 0)$fps), 2), 
                           round(mean(subset(data, bg_d == 1)$fps), 2)),
                 vars = c("bg_1", "bg_d"))
plot_d +
  annotate("text", x = -.15, y = labels$means[1], label = labels$means[1], 
           size = 6, color = "blue") +
  annotate("text", x = 1.1, y = labels$means[2], label = labels$means[2], 
           size = 6, color = "blue") +
  annotate("text", x = 0, y = -140, label = "Placebo", 
           size = 5, color = "blue") +
   annotate("text", x = 1, y = -140, label = "Alcohol", 
           size = 5, color = "blue") +
  coord_cartesian(xlim = c(0, 1),
                  (ylim = c(-100, 175)),
                  clip = "off") +
  theme(plot.margin = unit(c(0, .75, .5, .75), "inches"))
```

-----

::: {.callout-important}
# Question

What is the interpretation of $b_0$ in this model?

\

```{r}
#| code-fold: true

m_d |> tidy()
```
:::

:::{.fragment}

[Still predicted value for $Y$ when $X$ = 0.]{style="color:blue;"}

[In this context, that is the predicted FPS score for the placebo group (coded 0).]{style="color:blue;"}
:::

-----

::: {.callout-important}
# Question

Where is $b_0$ on this graph?

\

```{r}
#| code-fold: true
#| fig-height: 4

plot_d
```
:::

-----

[It is the $Y$ intercept as before. The value of FPS when $X$ = 0]{style="color:blue;"}

[And clearly, this is still just the predicted value for the placebo group - which is just the mean of the placebo group in the sample]{style="color:blue;"}

```{r}
#| code-fold: true

plot_d +
  annotate("text", x = -.15, y = labels$means[1], label = labels$means[1], 
           size = 6, color = "blue") +
  annotate("text", x = 0, y = -140, label = "Placebo", 
           size = 5, color = "blue") +
  coord_cartesian(xlim = c(0, 1),
                  (ylim = c(-100, 175)),
                  clip = "off") +
  theme(plot.margin = unit(c(0, 0, .5, .75), "inches"))
```

-----

::: {.callout-important}
# Question

Does alcohol affect FPS?
:::

:::{.fragment}

```{r}
#| code-fold: true

m_d |> 
  tidy()
```


$H_0: \beta_1 = 0$   
$H_a: \beta_1 \neq 0$   

[No. The probability of getting a $b_1$ of `r round(coef(m_d)[2], 3)` or more extreme is `r round(tidy(m_d)$p.value[2], 3)` if the null hypothesis is true.  This is not sufficient evidence to reject the null.]{style="color:blue;"}
:::

-----

## Model Comparisons ($\beta_1$)

::: {.callout-important}
# Question

What two models are you comparing to test your hypotheses about alcohol’s effect on FPS?
:::

:::{.fragment}
$H_0: \beta_1 = 0$  
$H_a: \beta_1 \neq 0$   

[Compact model:]{style="color: blue;"}

```{r}
m_com <- lm(fps ~ 1, data = data) 
```

- $\hat{FPS}_i=\beta_0+0*bg\_d_i$
- $P_c = 1$  
- $SSE_c =$ `r round(sse(m_com), 2)`

\

[Augmented model:]{style="color: blue;"}

```{r}
m_aug <- lm(fps ~ 1 + bg_d, data = data)
```

- $\hat{FPS}_i=\beta_0+\beta_1*bg\_d_i$
- $P_a=2$  
- $SSE_a = $ `r round(sse(m_aug), 2)`
:::

-----

::: {.callout-important}
# Question

How do you use this information to calculate $\eta_p^2$ (PRE) for the `bg_d` effect?
:::

:::{.fragment}

[PRE or $\eta_p^2$ is the proportional reduction in error due to the effect (the parameter). Use the same models that you would use to test the hypotheses about that parameter.]{style="color:blue;"}

$\frac{SSE_c - SSE_a}{SSE_c}$ = $\frac{`r round(sse(m_com), 2)` - `r round(sse(m_aug), 2)`}{`r round(sse(m_com), 2)`}$ = `r round((sse(m_com) - sse(m_aug)) / sse(m_com), 4)`
:::

-----

## Testing $\beta_0$

::: {.callout-important}
# Question

Does our task produce non-zero fear potentiated startle in sober people?
:::

:::{.fragment}

```{r}
#| code-fold: true

m_d |> tidy()
```

$H_0: \beta_0 = 0$  
$H_a: \beta_0 \neq 0$   

[Yes. The probability of getting a $b_0$ of `r round(coef(m_d)[1], 3)` or more extreme is `r round(tidy(m_d)$p.value[1], 8)` if the null hypothesis is true.  Very unlikely so reject null. Conclude $B_0 > 0$.]{style="color:blue;"}
:::

-----

## Model Comparisons ($\beta_0$)

::: {.callout-important}
# Question

What two models are you comparing to test your hypotheses about mean FPS for sober people?
:::

:::{.fragment}

$H_0: \beta_0 = 0$  
$H_a: \beta_0 \neq 0$   

[Compact model:]{style="color:blue;"}

```{r}
m_com <- lm(fps ~ 0 + bg_d, data = data) 
```

- $\hat{FPS}_i=0+\beta_1*bg\_d_i$
- $P_c = 1$  
- $SSE_c$ = `r round(sse(m_com), 2)`  


[Augmented model:]{style="color:blue;"}

```{r}
m_aug <- lm(fps ~ 1 + bg_d, data = data) 
```

- $\hat{FPS}_i=\beta_0+\beta_1*bg\_d_i$
- $P_a=2$  
- $SSE_a$ = `r round(sse(m_aug), 2)`
:::

-----

::: {.callout-important}
# Question

How do you use this information to calculate $\eta_p^2$ (PRE) for $\beta_0$?
:::

:::{.fragment}
[Compact model:]{style="color:blue;"}

```{r}
m_com <- lm(fps ~ 0 + bg_d, data = data) 
```

- $\hat{FPS}_i=0+\beta_1*bg\_d_i$  
- $P_c = 1$  
- $SSE_c$ = `r round(sse(m_com), 2)`  


[Augmented model:]{style="color:blue;"}

```{r}
m_aug <- lm(fps ~ 1 + bg_d, data = data) 
```

- $\hat{FPS}_i=\beta_0+\beta_1*bg\_d_i$   
- $P_a=2$  
- $SSE_a$ = `r round(sse(m_aug), 2)`

[PRE or $\eta_p^2$ is the proportion reduction in error due to the effect (the parameter). Use the same models that you would use to test the hypotheses about that parameter.]{style="color:blue;"}

\

$\frac{SSE_c - SSE_a}{SSE_c}$ = $\frac{`r round(sse(m_com), 2)` - `r round(sse(m_aug), 2)`}{`r round(sse(m_com), 2)`}$ = `r round((sse(m_com) - sse(m_aug)) / sse(m_com),  4)`
:::

-----

::: {.callout-important}
# Question

How do you use this information to calculate $R^2$ for the augmented model?
:::

:::{.fragment}
[Mean Only model:]{style="color:blue;"}

- $\hat{FPS_i}=\beta_0$  
- $P_c = 1$  
- $SSE_c =$ `r round(sse(m_com), 2)`  


[Augmented model:]{style="color:blue;"}

- $\hat{FPS}=\beta_0+\beta_1*bg\_d_i$
- $P_a=2$  
- $SSE_a =$ `r round(sse(m_aug), 2)`  

[$R^2$ is the proportion of explained variance over total variance in $Y$. The total variance is equivalent to the SSE of the Mean Only model]{style="color:blue;"}
:::

-----

::: {.callout-important}
# Question

Why?
:::

:::{.fragment}
[At this point it equals PRE b/c $SSE_c$ is the mean only model!]{style="color:blue;"}

```{r}
#| echo: false
m_mo <- lm(fps ~ 0 + bg_d, data = data) 
```

$\frac{SSE_{mean}-SSE_a}{SSE_{mean}}$ = $\frac{`r round(sse(m_mo), 2)` - `r round(sse(m_aug), 2)`}{`r round(sse(m_mo), 2)`}$ = `r round((sse(m_mo) - sse(m_aug)) / sse(m_mo), 4)`
:::

## Effect Sizes in R

```{r}
m_mean <- lm(fps ~ 1, data = data) 
m_d <- lm(fps ~ 1 + bg_d, data = data)
```

$\eta_p^2$
```{r}
(sse(m_mean) - sse(m_d)) / sse(m_mean) 
```

\

$R^2$
```{r}
glance(m_d)$r.squared 
```

-----

## Unit Weighted, Centered Coefficients

In some situations, we will prefer to use centered coefficients.

- These coefficients are "centered" around zero
- They are typcially unit weighted as well (i.e., the difference between the low value and high value is 1)

-----

```{r}
data <- data |> 
  mutate(bg_c = if_else(bg == "placebo", -.5, .5))

data |> slice_sample(n = 20)
```

-----

Lets look at the model that we get using these centered coefficients

\

```{r}
m_c <- lm(fps ~ 1 + bg_c, data = data)

m_c |> 
  tidy()
```

## Visual Displays of Dummy vs. Centered

```{r}
#| code-fold: true

plot_d <- plot_d +
  labs(title = "Dummy")

plot_c <- data |> 
  ggplot(aes(x = bg_c, y = fps)) +
  geom_point(alpha = .6, size = 2) +
  geom_abline(aes(intercept = coef(m_c)[1],
                  slope = coef(m_c)[2]),
                  linewidth = 1) +
  labs(title = "Zero-centered")
  
plot_d + plot_c
```

::: {.callout-important}
# Question

$b_0, b_1$ in each?
:::

## $b_0$ with (Zero-)Centered Coefficients

::: {.callout-important}
# Question

Is $b_0$ (`r round(coef(m_c)[1], 2)`) the mean of FPS in the sample?
:::

:::{.fragment}
[It is the unweighted mean.]{style="color:blue;"}

[Weighted and unweighted means emerge as concepts with grouped data.   An unweighted mean is the mean of the group means, ignoring the $N$ in each group.]{style="color:blue;"}
:::

-----

```{r}
data |> 
  summarise(mean = mean(fps),
            sd = sd(fps))
```

```{r}
data |> 
  group_by(bg) |> 
  summarise(mean = mean(fps),
            sd = sd(fps))
```

-----

## $b_0$ as Weighted Mean

A weighted mean is the mean of the groups *weighted* or proportional to their sample sizes. It is also equal to the grand mean of the DV ignoring group.  

::: {.callout-important}
# Question

How can you get $b_0$ to reflect the weighted mean?
:::

:::{.fragment}
[Mean center $X$. This is the same as we did with a quantitative variable to make $b_0$ equal the grand mean.]{style="color:blue;"}
:::

-----

```{r}
data <- data |> 
  mutate(bg_mc = bg_d - mean(bg_d))
```

\

```{r}
data |> 
  pull(bg_mc)
```

\

```{r}
mean(data$bg_mc)
```

-----

## $b_0$ as Weighted Mean

```{r}
m_mc <- lm(fps ~ 1 + bg_mc, data = data)

m_mc |> 
  tidy()
```

\

NOTE:  Mean centered vs. centered coefficients are less commonly used with categorical variables.  But it is important to understand because it deepens your intuition about how these models work.   

You can use transformations of your predictors (into regressors) in many different ways depending on your goals and questions in many different ways depending on your goals and questions.

-----

## Comparing $b_0$ Across Models

```{r}
#| code-fold: true

plot_d <- plot_d +
  labs(title = "Dummy") +
  annotate("text", x = -.1, y = round(coef(m_d)[1],2) + 10, label = round(coef(m_d)[1],2), 
           size = 5, color = "blue") +
  coord_cartesian(clip = "off") 

plot_c <- data |> 
  ggplot(aes(x = bg_c, y = fps)) +
  geom_point(alpha = .6, size = 2) +
  geom_abline(aes(intercept = coef(m_c)[1],
                  slope = coef(m_c)[2]),
                  linewidth = 1) +
  labs(title = "Zero-centered")  +
  annotate("text", x = -.1, y = round(coef(m_c)[1],2) + 10, label = round(coef(m_c)[1],2), 
           size = 5, color = "blue") +
  coord_cartesian(clip = "off") 

plot_mc <- data |> 
  ggplot(aes(x = bg_mc, y = fps)) +
  geom_point(alpha = .6, size = 2) +
  geom_abline(aes(intercept = coef(m_mc)[1],
                  slope = coef(m_mc)[2]),
                  linewidth = 1) +
  labs(title = "Mean-centered") +
  annotate("text", x = -.1, y = round(coef(m_mc)[1],2) + 10, label = round(coef(m_mc)[1],2), 
           size = 5, color = "blue") +
  coord_cartesian(clip = "off") 
  
plot_d + plot_c + plot_mc
```

-----

## Other (not useful) Coefficients

::: {.callout-important}
# Question

What would be the interpretation of $b_0$ and $b_1$ if I used coefficients of -1 and 1 for $bg$?
:::

:::{.fragment}
[$b_0$ will be the unweighted mean of FPS because 0 is halfway between the two groups.]{style="color:blue;"}

[$b_1$ will be half what it was in all previous models b/c a one unit change on $X$ will only go halfway between placebo and alcohol groups.]{style="color:blue;"}
:::

-----

```{r}
data <- data |> 
  mutate(bg_c2 = if_else(bg == "placebo", -1, 1))
```

```{r}
m_c2 <- lm(fps ~ 1 + bg_c2, data = data)

m_c2 |> 
  tidy()
```

-----

```{r}
data |> 
  ggplot(aes(x = bg_c2, y = fps)) +
  geom_point(alpha = .6, size = 2) +
  geom_abline(aes(intercept = coef(m_c2)[1],
                  slope = coef(m_c2)[2]),
                  linewidth = 1) 
```

-----

## Publication Quality Figure

Here is an example of a publication quality figure using +/- 1 SE

- First we need predicted values and SEs
```{r}
x <- tibble(bg_d = c(0, 1))

preds_pub <- x |> 
  bind_cols(predict(m_d, x, se.fit = TRUE) |> 
  as_tibble()) |> 
  mutate(lwr = fit - se.fit,
         upr = fit + se.fit) |>
  select(-df, -residual.scale)  # removed unnecessary columns

print(preds_pub)
```

-----

- Then we plot them using ggplot

```{r}
plot_pub <- ggplot() +
  geom_col(aes(x = preds_pub$bg_d, y = preds_pub$fit),
           alpha = .4, color = "black") +
  geom_jitter(aes(x = data$bg_d, y = data$fps), width = .02, height = NULL, 
              color = "blue") +
  geom_errorbar(aes(ymin = preds_pub$lwr, 
                    ymax = preds_pub$upr, 
                    x = preds_pub$bg_d), width = .2) +
  scale_x_continuous(breaks = c(0, 1),
                   labels = c("Placebo", "Alcohol")) +
  xlab("Group") +
  ylab("Fear-potentiated startle") 
```

-----

- I like to display raw data though it often does require extending the Y-axis scale substantially which can diminish the visual impact of the group differences.  This is a trade-off you will need to consider in your own work.

```{r}
#| echo: false
#| fig-cap: "Bars display point estimates for fear potentiated startle by group from the general linear model. Confidence intervals display +/- 1 standard error for these point estimates."

plot_pub
```


